### A Pluto.jl notebook ###
# v0.19.45

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local iv = try Base.loaded_modules[Base.PkgId(Base.UUID("6e696c72-6542-2067-7265-42206c756150"), "AbstractPlutoDingetjes")].Bonds.initial_value catch; b -> missing; end
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : iv(el)
        el
    end
end

# ╔═╡ 0fff8e1b-d0c2-49b8-93b4-8d1615c26690
begin
	using Statistics, PlutoPlotly, Random, StatsBase, PlutoUI, HypertextLiteral, DataStructures, StaticArrays, Transducers, Serialization, Arrow
	TableOfContents()
end

# ╔═╡ 516234a8-2748-11ed-35df-432eebaa5162
md"""
# Chapter 8 - Planning and Learning with Tabular Methods

So far we have seen example problems where we have a full model of the environment in the form of the probability transition function as well as environments where we can only sample trajectories.  In this chapter we will integrate the techniques that are *model-based* and *model-free* and show how they can be intermixed.

## 8.1 Models and Planning

A *model* is anything that an agent can use to predict the environment.  If the model provides a full description of all possible transitions it is called a *distribution model* vs a *sample model* that can only generate one of those possibilities according to the correct probability distribution.  In dynamic programming, we used a distribution model while for certain example problems such as blackjack we only had a sample model.

A model can be used the create a *simulated experience* of the environment such as a trajectory.  The common thread across all the techniques is the computation of the value function to improve a policy and using some update process to compute the value function for example from the data collected in simulated experience.  For the learning methods considered so far, we have assumed that the data collected from trajectories is generated by the environment itself while in planning methods this experience would come instead from a model.  However the learning techniques largely still apply to planning techniques as well since the nature of the data is the same.  Consider a planning method analogous to Q-learning called *random-sample one-step tabular Q-planning*.  This technique applies the Q-learning update to a transition sapmled from a model.  Instead of interacting with the environment in an episode or continuing task, this technique simply selects a state action pair at random and observes the transition.  Just like with Q-learning, this method converges to the optimal policy under the assumption that all state-action pairs are visited an infinite number of times but the policy will only be optimal for the model of the environment.

Performing updates on single transitions highlights another theme of planning methods which don't necessarily involve exaustive solutions to the whole environment.  We can direct the method to specific states of interest which may be important for problems with very large state spaces.

## 8.2 Dyna: Integrated Planning, Acting, and Learning

A planning agent can use real experience in at least two ways: 1) it can be used to improve the model to make it a better match for the real environment (*model-learning*) and 2) it can be used directly to improve the value function using the previous learning methods (*direct reinforcement learning*).  If a better model is then used to improve the value function this is also called *indirect reinforcement learning*.  

Indirect methods can make better use of a limited amount of experience, but direct methods are much simpler and are not affected by the biases in the design of the model.  Dyna-Q includes all the processes of planning, acting, model-learning, and direct RL.  The planning method is the random-sample one-step tabular Q-planning described above.  The direct RL method is one-step tabular Q-learning.  The model-learning mdethod is also table-based and assumes the environment is deterministic.  After each transition $S_t,A_t \longrightarrow R_{t+1},S_{t+1}$, the model records in its table entry for $S_t,A_t$ the prediction that $R_{t+1},S_{t+1}$ will deterministically follow.  Thus if the model is queried with a state-action pair that has been experienced before, it simply returns the last-observed next state and next reward as its prediction.

During planning, the Q-planning algorithm randomly samples only from state-action pairs that have previously been experienced, so the model is never queried with a pair about which it has no information.  The learning and planning portions of the algorithm are connected in that they use the same type of update.  The only difference is the source of the experience used.

The collection of real experience and planning could occur simultaneously in these agents, but for a serial implementation it is assumed that the acting, model-learning, and direct RL processes are very fast while the planning process is the model computation-intensive.  Let us assume that after each step of acting, model-learning, and direct RL there is time for $n$ iterations of the Q-planning algorithm.  Without the model update and the $n$ loop planning step, this algorithm is identical to one-step tabular Q-learning.  An example implementation is below along with an example applying it to a maze environment.
"""

# ╔═╡ 65818e67-c146-4686-a9aa-d0859ef662fb
md"""
### Example 8.1: Dyna Maze
"""

# ╔═╡ d5ac7c6f-9636-46d9-806f-34d6c8e4d4d5
md"""
Consider the simple maze shown above where the agent can take four actions which take it deterministically into the neighboring square unless it is blocked by a wall represented by a W in the diagram.  In this case the agent remains in its original state.  Reward is zero on all transitions except those into the goal state, on which it is +1.  After reaching the goal the agent returns to the start to begin a new episode.  This is a discounted episodic task with $\gamma = 0.95$
"""

# ╔═╡ 76489f21-677e-4c25-beaa-afaf2244cd94
md"""
#### Figure 8.2

Learning curves for a Dyna-Q agent in the maze
"""

# ╔═╡ 27d12c1c-ddb0-4bc1-af51-3388ff806705
md"""
#### Figure 8.3:

Policies found by planning and nonplanning Dyna-Q agents after episode $(@bind num_episodes_8_3 NumberField(1:100, default = 2)). The arrows indicate the greedy action with respect to the learned state-action value function.  Without planning, the only states with a policy update are those within n episodes of the goal where n is the number of episodes experienced so far in training.  For the planning agent after 1 episode, at least one of the values will be updated so during future planning steps, that information can propagate through the other states through bootstrapping since we are free to sample transitions from states into the ones that already have a value update.
"""

# ╔═╡ cd79ce14-14a1-43c6-93e0-b4a786f7f9fb
md"""
Because planning proceeds incrementally, it is trivial to intermix planning and acting.  So the learning from the model can inform the interaction with the environment in parallel and then any information from the environment can be used to update the model whenever it is received.
"""

# ╔═╡ e0cc1ca1-595d-44e2-8612-261df9e2d327
md"""
> ### *Exercise 8.1* 
> The nonplanning method looks particularly poor in Figure 8.3 because it is a one-step method; a method using multi-step bootstrapping would do better. Do you think one of the multi-step bootstrapping methods from Chapter 7 could do as well as the Dyna method? Explain why or why not.

For the n = 50 agent, it can learn a policy that covers nearly the entire maze during the second episode.  In the extreme case of a multistep method we would attempt to solve the maze using monte carlo sampling in which case after a single episode we would have action/value updates for every state visited along the random trajectory.  However, these action/value estimates would be high variance estimates of the randomly initialized policy.  In contrast, the Dyna method after one random episode has observed most or all of the transitions in the maze.  During the long planning phase its Q updates would actually be able to converge to the optimal policy given a large enough n using bootstrapping and the single reward from reaching the goal.  As long as something is sampled close to the goal that information will propagate through to the rest of the states and each update is simultaneously improving the ϵ-greedy policy.  With multi-step bootstrapping we can extend the updates back along the trajectory a certain distance, but in the extreme case we just sample values from the random policy without any bootstrapping or we bootstrap to a limited degree close to the goal and still have the lack of information further away from it.  Since this environment is deterministic, having the sample transitions is equivalent to having a full model of the environment.  This could be used explicitely with value iteration to obtain the Q function as well.  The planning step is effectively performing this computation but only using the known transitions and over time is focused only on the states visited by the optimal or nearly optimal policy.  No n-step method can take advantage of previously observed transitions this well, but it should be noted that Dyna-Q only works this well if the environment is deterministic and is taking advantage of the fact that we know our model is correct for the transitions already observed.  
"""

# ╔═╡ 4f4551fe-54a9-4186-ab8f-3535dc2bf4c5
md"""
## 8.3 When the Model Is Wrong

In the maze example the model started out empty and then was only filled in with correct information.  In general, we cannot expect to be so fortunate.  Models may be incorrect because the environment is stochastic and only a limited number of samples have been observed, or simply because the environment has changed.  When the model is incorrect, the planning process will likely compute a suboptimal policy.

In some cases, the suboptimal poicy computed by planning quickly leads to the discovery and correction of the modeling error.  This tends to happen when the model is optimistic in the sense of predicting greater reward or better state transitions than are actually possible.  The planned policy attempts to exploit these opportunities and in doing so it discovers that they do not exist.

### Example 8.2: Blocking Maze

Consider a maze with a barrier that separates the start from the goal except for an opening on one end.  After a set number of time steps this path is blocked and replaced with a longer path open at the other end.  Below is an example of the environment with a random trajectory before and after the blocking.
"""

# ╔═╡ 69ff1b72-cb1b-4724-a445-38e4c9846964
md"""
#### Figure 8.4:

Average performance of Dyna-Q agent on a blocking task.  After 1000 episodes of training the environment is switched to the second version.  The flat portion of the graph shows the agent getting stuck following the old optimal policy and needing many time steps to learn the new environment
"""

# ╔═╡ 8987052b-0828-43a5-982e-5f3d6209f2aa
@bind fig_8_4_params PlutoUI.combine() do Child
	md"""
	Planning Steps: $(Child(:n, NumberField(1:100, default = 5)))
	"""
end |> confirm

# ╔═╡ dab6eac0-7958-4d3a-a1af-781fb57e7adb
md"""
### Example 8.3: Shortcut Maze

The problem of an incorrect model can be even worse in the case of an improvement to the environment.  The following example illustrates the problem in which a maze is at first blocked forcing a longer path.  After a certain number of timesteps a shorter path is opened up without disturbing the original path.  The Dyna-Q agent however never learns to switch to the shortcut because the model never reveals it or provides incentive to explore paths that it already thinks are suboptimal.  Even with an ϵ-greedy policy, it is very unlikely that hte agent will take so many exploratory actions as to discover the shortcut.  
"""

# ╔═╡ b0df4dad-74c7-4469-a13f-5ef6bb81199f
md"""
#### Figure 8.5:

Average performance of Dyna-Q on shortcut task.  The constant slope after learning the optimal policy indicates the shortcut is not exploited.
"""

# ╔═╡ 976ff0c3-54aa-41f0-b963-baf77bea8cb8
md"""
The general problem here is the conflict between exploration and exploitation.  In a planning context, exploration means trying actions that improve the model, whereas exploitation means behaving in the optimal way given the current model.  We want the agent to explore to find changes in the environment, but not so much that performance is greatly degraded.  As in the earlier exploration/exploitation conflict, there probably is no solution that is both perfect and practical, but simple heuristics are often effective.

An example of such a heuristic is that used by the Dyna-Q+ agent which keeps track of how many time steps have elapsed since the state-action pair was last tried in a real interaction with the environment.  The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model is incorrect.  To encourage behavior that tests long-untried actions, a special "bonus reward" is given on simulated experiences involving these actions.  In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\tau$ time steps, then planning updates are done as if that transition produced a reward of $r + \kappa \sqrt{\rho}$, for some small $\kappa$.  This encourages the agent to keep testing all accessible state transitions and even to find long sequences of actions in order to carry out such tests.  Of course all this testing has its cost, but in many cases, as in the shortcut maze, this kind of computational curiosity is well worth the extra exploration.
"""

# ╔═╡ 24efe9b4-9308-4ad1-8ef0-69f6f93407c0
md"""
> ### *Exercise 8.2* 
> Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?

For the second phase, the maze changed in both cases so the exploration reward bonus in Dyna-Q+ encourages the policy to attempt different actions that have not been visited recently which would result in model updates that reflect the new environment.  For the first phase where the model is accurate, this task may benefit from larger initial exploration than the ϵ of 0.1 provides.  In that case the Dyna-Q+ reward simply acts like if we had a larger ϵ in the first place which may result in faster learning.
"""

# ╔═╡ 26fe0c28-8f0f-4cff-87fb-76f04fce1be1
md"""
> ### *Exercise 8.3* 
> Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?

As long as the environment isn't changing, no matter how small ϵ is both algorithms will converge to the optimal policy as long as $\alpha$, $\kappa$, and $\tau$ converge to 0.  For a given value of $\epsilon$, the Dyna-Q+ agent will take more exploratory actions due to the modified rewards updating the Q function.  If both agents have converged to close to the optimal policy at this point, then the agent which follows a more expoitative policy will produce higher rewards.  In this case, the extra exploration of Dyna-Q+ is not beneficial and its relative performance to Dyna-Q degrades.
	"""

# ╔═╡ 340ba72b-172a-4d92-99b2-17687ab511c7
md"""
> ### *Exercise 8.4 (programming)* 
> The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Suppose the bonus $\kappa \sqrt{\tau}$ was used not in updates, but solely in action selection. That is, suppose the action selected was always that for which $Q(S_t,a) + \kappa \sqrt{\tau(S_t, a)}$ was maximal. Carry out a gridworld experiment that tests and illustrates the strengths and weaknesses of this alternate approach.
"""

# ╔═╡ caca8d95-e40e-4592-b29e-a7e8b19faeb5
md"""
Modifying Dyna-Q+ with this method of action selection no longer uses the ϵ-greedy policy for exploration as before and instead directly encourages exploration in the policy itself which no longer follows the value estimates directly.  Now, during the planning steps, we still update the value function using existing data.  The bonus reward for old experiences will only have an impact on the data collection by encouraging visits to states that have not been seen for a while.  The original method instead operates under the assumption that the value estimate updates during planning may be underestimating the value of states visited a long time ago and will boost their value increasing the chance of action selection indirectly.  

One problem with the method that modifies the action selection directly is that once the agent has found an optimal policy, along the trajectory of that path many state action pairs will be frequently visited, so an agent may deviate from that path for one action which has not been attempted in a while, but then by the same logic, the transition immediately back to the optimal state will be favored over all others since that transition hasn't been visited for a long time as well.  The agent may deviate slightly from the optimal path but never for long.  With DynaQ+, however, eventually, unvisited states no matter how far away they are from the optimal path will eventually be visited since during planning those value bonuses will propagate throughout the whole environment.  That may be one reason why in the shortcut maze task, the modified DynaQ+ agent shown above never finds the shortcut and falls into the same trap as unmodified DynaQ.  If the goal is to find cases where the model is wrong even if it is far away in state space from the previously optimal policy, then DynaQ+ is a much more effective method than the one acting directly on action selection.

Similar to the original DynaQ, if the model is correct, then this algorithm will follow the optimal policy more often than DynaQ+ because very few deviations from optimal are tollerated.  You can see in both figures, once the environment is stable, the modified algorithm starts accumulating reward faster.  For the bloacking maze task, this method might perform the best over a certain time horizon by finding the new path faster than DynaQ but also consistently sticking new to the optimum more than DyanQ+.
"""

# ╔═╡ 1127c36f-9bc0-49b1-9481-8a5861bdf6ca
md"""
> ### *Exercise 8.5*
> How might the tabular Dyna-Q algorithm shown above be modified to handle stochastic environments?  How might this modification perform poorly on changing environments such as considered in this section?  How could the algorithm be modified to handle stochastic environments *and* changing environments?

Instead of storing the most recent transition for a state-action pair, we could store the fraction of the time different transition states and rewards are observed.  That way our model becomes a distribution model using the previously experienced samples as estimates of the true transition probabilities.  Then for the planning updates, we would sample the transition from the model stochastically and perform the same Q update.

If the environment is changing, then the model will take much longer to modify compared to the original version where as soon as a new transition is observed it replaces the previous value.  If we averaging the previously observed transitions and there are many actions, then a new experience would only slightly modify the model even if we use constant step size averaging.  We would therefore expect such a model to react much more slowly to a changing environment.

One solution to speed up model updates could be to use a modified reward such as what is used for Dyna-Q+ in which the time since a state-action pair was selected increases the planning reward update for that transition, but that method assumed that a single visit to the state is adequate to update the model.  In a stochastic environment if we are counting transition to have an estimate of the probabilities for each one, our method would need a way of forgetting transitions that no longer happen.  We could simply keep only data from the most recent n transitions or average with exponential decaying weights such that we give higher weight to more cently observed transitions in the probability distribution.  We may also have to keep track of the time since a specific transitions was seen rather than simply the time a state action pair was attempted, so we'd have $\tau(s, a, s′, r)$ and then the planning Q update could add up or take the maximum of all of the observed transitions from that state.  A state action pair would then be boosted if any one of the possible transitions has not been seen recently.  
"""

# ╔═╡ a3243bc4-7ae7-418a-9881-a265ca95f5ef
md"""
## 8.4 Prioritized Sweeping
"""

# ╔═╡ 08ff749b-f4ff-4639-99f5-48262aa4643e
md"""
In the Dyna agents simulated transitions are started in the state-actino pairs selected uniformly at random from all previously experienced pairs, but a uniform selection is usually not the best.  Planning can be much more efficient if updates are focused on particular state-action pairs.  In particular if an environment has goal states such as the maze, then one-step TD updates will only be relevant along the trajectory leading to that goal.  Not all environments have goal states, but the idea still applies to cases where only certain states are expected to be updated given a certain amount of data from environment interactions.  We can therefore work *backward* from states we know will be updated to consider all of the relevant predecessor states.  Using *prioritized sweeping*, we maintain a queue of every state-action pair whose estimated value would change nontrivially if updated, prioritized by the size of the change.  When the top pair in the queue is updated, the effect on each of its predecessor pairs is computed.  If the effect is greater than some small threshold, then the pair is inserted int the queue with a new priority.  In this way the effects of changes are efficiently backpropagated backward until quienscence.  A full implementation of priorized sweeping can be found below.
"""

# ╔═╡ dff6f326-ab7e-45e5-8c5e-28bfbb1d99bc
md"""
### Example 8.4: Prioritized Sweeping on Mazes

Consider different mazes with the same wall structure as the Dyna Maze example but scaled up to different resolutions.  Some example mazes are shown below.
"""

# ╔═╡ 466268c4-664d-42dd-84c1-7b8ade49936f
md"""
For each maze there is an optimal path that scales up with the size.  The plot below compares prioritized sweeping with Dyna-Q where each method can make at most 5 planning updates.  The number of updates until an optimal path is found is computed for different size mazes.  The number of updates required for prioritied sweeping is dramatically lower by tens or hundreds of times depending on the size of the maze.
"""

# ╔═╡ 08e81e30-119d-4f4e-a865-f1f85cbffd31
md"""
For many tasks the state space is too large to be solved effectively without prioritization.  That being said, there are multiple methods of prioritizing states such as looking forward from states that are being visited frequently by the policy.  

Prioritized sweeping also makes use of expected updates to update the priority queue (sweeping through model state action pairs) which could be very costly in environments with large numbers of actions.  Also in stochastic environments, our model may not even be accurate for some low probability transitions which would be a waste to consider.  An alternative is to use sample updates which are higher variance individually, but may win out in terms of rate of convergence per computation in environments with high stochasticity and large branching.  In the next sections, we more closely examine the value of expected and sample updates.
"""

# ╔═╡ 63bf9d16-4516-4cec-895f-f010275bca16
md"""
## 8.5 Expected vs Sample Updates
"""

# ╔═╡ 31072f9b-de1b-42cf-a187-cbff99b49b50
md"""
Throughout the book we have considered a variety of value-function updates for the state value $v(s)$ and the state-action value $q(s, a)$.  Broadly, they can be separated into expected updates and sample-updates.  The first expected update considered was for dynamic programming in which we make use of the full probability transition function $p(s^\prime, r \vert s, a)$.  When used with a policy $\pi$ that provides a probability distribution over actions for each state, this technique is called *policy evaluation* and when a maximization is used over actions to find optimal value function and policy policy, this corresponds to *value iteration*.  Both of these techniques could be used with either the state or state-action value function.

The expected update for a state-action pair, $s,a$, is:

$Q(s, a) \leftarrow \sum_{s^\prime,r}\hat p (s^\prime,r \vert s, a) \left [ \gamma \max_{a^\prime} Q(s^\prime, a^\prime) \right ] \tag{8.1}$

Considering just one-step sample updates, we use an environment to sample a transition: $(s, a) \rightarrow (s^\prime, r)$.  Since we do not use the full probability transition function, the updates make use of a learning rate and accumulate samples over time either with Sarsa or Q-learning.  

The corresponding sample update for $s,a$ given a sample next state and reward, $S^\prime$ and $R$ (from the model), is the Q-learning-like update:

$Q(s, a) \leftarrow Q(s, a) + \alpha \left [ R + \gamma \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right ] \tag{8.2}$

where $\alpha$ is the usual positive step-size parameter.

The difference between expected and sample updates is significant to the extent that the environment is stochastic, specifically, to the extent that, given a state and action, many possible next states may occur with various probabilities.  If only one next state is possible, then the expected and sample updates given above are identical (taking $\alpha=1$).  If there are many possible next states, then tehre may be significant differences.  In favor  of the expected update is that it is an exact computation, resulting in a new $Q(s, a)$ whose correctness is limited only by the correctness of the $Q(s^\prime, a^\prime)$ at successor states.  The sample update is in addition affected by sampling error.  On the other hand, the sample update is cheaper computationally because it considers only one next state, not all possible next states.  In practice, the computation required by update operations is usually dominated by the number of state-action pairs at which $Q$ is evaluated.  For a particular starting pair, $s, a$, let $b$ be the *branching factor* (i.e. the number or possible next states, $s^\prime$, for which $\hat p (s^\prime, s, a) \gt 0$).  Then an expected update of this pair requires roughly $b$ times as much computation as a sample update.

If there is enough time to complete an expected update, then the resulting estimate is generally better than that of $b$ sample updates because of the absense of sampling error.  But if there is insufficient time to complete an expected update, then sample updates are always preferable because they at least make some improvement int he value estimate with fewer than $b$ updates.  In a large problem with many state-action pairs, we are often in the latter situation.  With so many state-action pairs, expected updates of all of them would take a very long time.  Before that we may be much better off with a few sample updates at many state-action pairs than with expected updates at a few pairs.  Given a unit of computation effort, is it better devoted to a few expected updates or to $b$ times as many sample updates?

Figure 8.7 shows the results of any analysis that suggests and answer ot this question.  It shows the estimation error as a function of computation time for expected and sample updates for a variety of branching factors $b$.  The case considered is that in which all $b$ successor states are equally likely and in which the error in the initial estimate is 1.  The values at the next states are assumed correct, sot he expected update redues the error to zero upon its completion.  In this case, sample updates reduce the error according to $\sqrt{\frac{b-1}{bt}}$ where $t$ is the number of sample updates that have been performed (assuming same averages, i.e. $\alpha = 1/t$).  The key observation is that for moderately large $b$ the error falls dramatically with a tiny fraction of $b$ updates.  For these cases, many state-action pairs could have their values improved dramatically, to within a few percent of the effect of an expected update, in teh same time that a single state-action pair could undergo an expected update.

The advantage of sample updates shown in Figure 8.7 is probably an underestimate of the real effect.  In a real problem, the values of successor states would be estimates that are themselves updated.  By causing estimates to be more accurate sooner, sample updates will have a second advantage in that the values backed up from the successor states will be more accurate.  These results suggest that sample updates are likely to be superior to expected updates on problems with large stochastic branching factors and too many states to be solved exactly.
"""

# ╔═╡ df62fd47-6627-4931-b429-964c65960446
function figure_8_7(;bvec = [2, 10, 100, 1000, 10_000])
	traces = [begin
		t = 1:2*b
		error_reduction = sqrt.((b-1) ./ (b.*t))
		scatter(x = [0; t ./ b], y = [1; error_reduction], name = "b = $b")
	end
	for b in bvec]
	ex_update_trace = scatter(x = [0, 1, 1, 2], y = [1, 1, 0, 0], name = "expected updates", mode = "lines", line_color = "gray")
	md"""
	### Figure 8.7: 
	Comparison of efficiency of expected and sample updates
	$(plot([traces; ex_update_trace], Layout(yaxis_title = "RMS error in value estimates", xaxis=attr(title = "Number of max Q computations", tickvals = [0, 1, 2], ticktext = [0, "1b", "2b"]))))
	"""
end

# ╔═╡ cfdaa9c2-265f-4540-9d04-d1b7a72aee3e
figure_8_7()

# ╔═╡ 3b0d2c55-2123-4b51-b946-6bc352e3d00a
md"""
> ### *Exercise 8.6* 
> The analysis above assumed that all of the $b$ possible next states were equally likely to occur.  Suppose instead that the distribution was highly skewed, that some of the $b$ states were much more likely to occur than most.  Would this strengthen or weaken the case for sample updates over expected updates?  Support your answer.

For the expected updates, all transitions no matter how small the probability are given the same amount of computation time.  If certain transitions are much more likely than others, than the resulting state value computation is dominated by those transitions.  If we perform sample updates, those transitions will appear more frequently than the less likely ones and will contribute earlier to the error reduction.  In the extreme case if just one transition is close to probability 1, then that will be sampled almost every time and after just a single sample the error could be reduced close to 0.  An expected update, on the other hand, would have to consider every state action pair to just perform a single update.  Therefore the efficiency of sample updates would become exaggerated in all real world problems where the subsequent state transition probabilities deviate from a uniform distribution.
"""

# ╔═╡ aab6ca56-57ca-421e-9b71-e3e96681c4c5
md"""
## 8.6 Trajectory Sampling
"""

# ╔═╡ 72b40384-9ca1-4bc1-8e1a-8b639d39e215
md"""
In this section we compare two ways of distributing updates.  The classical approach, from dynamic programming, is to perform sweeps through the entire state (or state-action) space, updating each state (or state-action pair) once per sweep.  This is problematic on large tasks because there may not be time to complete even one sweep.  In many tasks, the vast majority or the states are irrelevant because they are visited only under very poor policies or with very low probability.  Exhaustive sweeps implicitely devote equal time to all parts of the state space rather than focusing where it is needed.  As we discussed in Chapter 4, exhaustive sweeps and the equal treatment of all states that they imply are not necessary properties of dynamic programming.  In principle, updates can be distributed in any way one likes (to assure convergence, all states or state-action pairs must be visited in the limit an infinite number of times; although an exception to this is discussed in Section 8.7 below), but in practice exhaustive sweeps are often used.

The second approach is to sample from the state or state-action space according to some distribution.  One could sample uniformly, as in the Dyna-Q agent, but this would suffer from some of the same problems as exhaustive sweeps.  More appealing is to distribute updates according to the on-poliy distribution, that is according to the distribution observed when following the current policy.  One advantage of this distribution is that it is easily generated; one simply interacts with the model, following the current policy.  In an episodic task, one starts in a start state (or according to the starting-state distribution) and simulates until the terminal state.  In a continuing task, one starts anywhere and just keeps simulating.  In either case, sample state transitions and rewards are given by the model, and sample actions are given by the current policy.  In other words, one simulates explicit individual trajectories and performs updates at the state or state-action pairs encountered along the way.  We call this way of generating experience and updates *trajectory sampling*.

Is the on-policy distribution of updates a good one?  Intuitively it seems like a good choice, at least better than the uniform distribution.  For example, if you are learning to play chess, you study positions that might arise in real games, not random positions of chess pieces.  The latter may be valid states, but to be able to accurately value them is a different skill from evaluating positions in real games.  We will also see in Part II that the on policy distribution has significant advantages when function approximation is used.  Whether or not function approximation is used, one might expect on-policy focusing to significantly improve the speed of planning.

The following experiment attempts to assess empirically the value of sampling from the on policy distribution by comparing it directly to the alternative of *uniform* sampling.  In the *uniform* case, all state action pairs are updated once in place and in the *on-policy* case an episode is imulated starting from the same state and updating each state-action pair that occured under the current $\epsilon$-greedy policy $(\epsilon = 0.1)$.  The tasks were undiscounted episodic tasks, generated randomly as follows.  From each of the $\vert \mathcal{S} \vert$ states, two actions were possible, each of which results in one of $b$ next states, all equally likely, with a different random selection of $b$ states for each state-action pair.  The branching factor, $b$, was the same for all state-action pairs.  In addition, on all transitions there was a 0.1 probability of transition to the terminal state, ending the episode.  The expected reward on each transition was selected from a Gaussian distribution with mean 0 and variance 1.  At any point in the planning process one can stop and exhaustively compute $v_{\tilde \pi}(s_0)$, the true value of the start state under the greedy policy, $\tilde \pi$, given the current action-value function $Q$, as in indication of how well the agent would do on a new episode on which it acted greedily (all while assuming the model is correct).
"""

# ╔═╡ e8a6e672-b860-404f-83c1-62a080f23112
md"""
### Figure 8.8
"""

# ╔═╡ 04ea981e-337e-4324-a5cc-178eb3c7605b
md"""
## 8.11 Monte Carlo Tree Search
"""

# ╔═╡ ee0f55c6-e9c6-4199-9f27-5706f3c84863
md"""
### MCTS Implementation
"""

# ╔═╡ f9cdd5a8-3a9b-4be4-9a33-bb0047ac4a96
#create a function that performs a rollout given an mdp, starting state, and discount rate
rollout(;max_steps = Inf) = (mdp, s, γ) -> rollout(mdp, s, γ; max_steps = max_steps)

# ╔═╡ d4e1e807-7e87-4b43-9c36-f59999dfcd2d
#get the value of a dictionary d at key k, defaulting to some value when the key is absent
get_dict_value(d::Dict{K, V}, k::K; default = zero(V)) where {K, V<:Real} = haskey(d, k) ? d[k] : default 

# ╔═╡ 308fd488-a009-4de0-8b27-c1f6b0677fed
uct(counts::Dict{S, Dict{Int64, T}}, s::S, i_a::Int64, ntot::T) where {S, T<:Real} = sqrt(log(ntot)/counts[s][i_a])

# ╔═╡ cffc9f11-77d7-4076-aa3b-821f1c741f58
uct(tree_values::Dict{S, Tuple{T, Dict{Int64, Tuple{T, T}}}}, s::S, i_a::Int64, ntot::T) where {S, T<:Real} = sqrt(log(ntot)/tree_values[s][2][i_a][1])

# ╔═╡ f369a092-420d-4660-b802-93f05d5e7972
function apply_uct!(v_hold::Vector{T}, tree_values::Dict{S, Tuple{T, Dict{Int64, Tuple{T, T}}}}, s::S, c::T) where {S, T<:Real}
	#for normal UCB selection, unvisited states have an infinite bonus
	v_hold .= T(Inf)

	d = tree_values[s][2]
	isempty(d) && return v_hold
	ntot = sum(t[1] for t in values(d))
	@inbounds @fastmath for i in keys(d)
		#note that the only bonus values computed here are for actions that have been visited
		v_hold[i] = (d[i][2] / d[i][1]) + c * uct(tree_values, s, i, ntot)
	end
	return v_hold
end

# ╔═╡ 82e5719c-bbdb-4a18-b2f0-ad746b6acd41
function make_puct(update_prior!::Function) 
	function f(v_hold::Vector{T}, q_ests::Dict{S, Dict{Int64, T}}, counts::Dict{S, Dict{Int64, T}}, s::S, c::T) where {S, T<:Real}
		update_prior!(v_hold, s)
		if !isempty(q_ests[s])
			ntot = zero(T)
			qsum = zero(T)
			@inbounds @fastmath for i in keys(q_ests[s])
				ntot += counts[s][i]
				qsum += counts[s][i] * q_ests[s][i]
			end
			qavg = qsum / ntot
			v_hold .= qavg .+ ((c * sqrt(ntot)) .* v_hold)
			@inbounds @fastmath for i in keys(q_ests[s])
				#note that the only bonus values computed here are for actions that have been visited
				v_hold[i] = q_ests[s][i] + ((v_hold[i] - qavg) / (one(T) + counts[s][i]))
			end
		end
		return v_hold
	end
end

# ╔═╡ f6486854-4892-4fb6-a805-de56b19b3571
function make_uct_plus(update_prior!::Function) 
	function f(v_hold::Vector{T}, q_ests::Dict{S, Dict{Int64, T}}, counts::Dict{S, Dict{Int64, T}}, s::S, c::T) where {S, T<:Real}
		update_prior!(v_hold, s)
		if !isempty(q_ests[s])
			ntot = zero(T)
			qsum = zero(T)
			visited_weight = zero(T)
			@inbounds @fastmath for i in keys(q_ests[s])
				ntot += counts[s][i]
				qsum += counts[s][i] * q_ests[s][i]
				visited_weight += v_hold[i]
			end

			#sample from unvisited weights proportional to the probability that one of those is the best option given the prior
			unvisited_weight = one(T) - visited_weight

			#sample from the unvisited actions weighted by their prior probability
			if rand() < unvisited_weight
				v_hold ./= unvisited_weight
				@inbounds @fastmath for i in keys(q_ests[s])
					v_hold[i] = zero(T)
				end
			else #sample from visited actions weighted by UCT
				v_hold .= zero(T)
				@inbounds @fastmath for i in keys(q_ests[s])
					#note that the only bonus values computed here are for actions that have been visited
					v_hold[i] = q_ests[s][i] + c*uct(counts, s, i, ntot)
				end
			end
		end
		return v_hold
	end
end

# ╔═╡ aa898360-f802-438a-9081-a2e517230db2
function apply_uct!(v_hold::Vector{T}, q_ests::Dict{S, Dict{Int64, T}}, counts::Dict{S, Dict{Int64, T}}, s::S, c::T) where {S, T<:Real}
	ntot = sum(values(counts[s]))
	#for normal UCB selection, unvisited states have an infinite bonus
	v_hold .= T(Inf)
	@inbounds @fastmath for i in keys(q_ests[s])
		#note that the only bonus values computed here are for actions that have been visited
		v_hold[i] = q_ests[s][i] + c * uct(counts, s, i, ntot)
	end
	return v_hold
end

# ╔═╡ bc295bb5-addb-4bcf-a3e3-c839ccc346bd
md"""
### Maze Example
"""

# ╔═╡ 245d9616-e0d2-497e-bfdd-4729a7215bfd
md"""
The maze task is a horrible candidate for MCTS because it is solved so effectively with the other techniques that build accurate action value estimates for the entire maze.  It is very likely that states will be revisited in this task as well and that information is useful to solve the problem.  MCTS would be most useful for problems where we cannot even know the full state space and states are never seen again in the course of a single episode as well as for tasks whose episodes don't last very long.  Blackjack might be a good example of this since the number of steps is usually one or two.  Another example could be tic-tac-toe.
"""

# ╔═╡ 321bdf5a-bff7-4181-986f-d3884ea96d27
md"""
# Dependencies
"""

# ╔═╡ e689df6b-d6f0-4928-9212-a940aa00b0ef
html"""
	<style>
		main {
			margin: 0 auto;
			max-width: min(1200px, 90%);
	    	padding-left: max(10px, 5%);
	    	padding-right: max(10px, 5%);
			font-size: max(10px, min(24px, 2vw));
		}
	</style>
	"""

# ╔═╡ b03087e9-e15d-4563-bdae-4d9ba7d2cec6
md"""
## MDP Types and Functions
"""

# ╔═╡ c1ff1bea-649c-4483-b4be-55134f0e8cb7
function sample_action(π::Matrix{T}, i_s::Integer) where T<:AbstractFloat
	(n, m) = size(π)
	sample(1:n, weights(π[:, i_s]))
end

# ╔═╡ 6858ef8b-1ca7-4e96-b57e-26553423cc13
sample_action(v::Vector{T}) where T<:AbstractFloat = sample(1:length(v), weights(v))

# ╔═╡ c04c91be-de42-4dfc-bd0d-b9fbdde0c9cf
function simulate!(visit_counts, Q, mdp, γ::T, v_est, s, depth, c::T, v_hold, update_tree_policy!, updateQ!, updateV!, q_hold, apply_bonus!, step_kwargs, est_kwargs) where T<:Real
	#if the state is terminal, produce a value of 0
	mdp.isterm(s) && return zero(T)
	
	depth ≤ 0 && return v_est(mdp, s, γ; est_kwargs...)
	
	#for a state where no actions have been attempted, expand a new node
	if !haskey(visit_counts, s)
		# Q[s] = sparse(q_hold)
		# visit_counts[s] = sparse(q_hold)
		Q[s] = Dict{Int64, T}()
		visit_counts[s] = Dict{Int64, T}()
		return v_est(mdp, s, γ; est_kwargs...)
	end

	apply_bonus!(v_hold, Q, visit_counts, s, c)
	
	update_tree_policy!(v_hold, s)
	i_a = sample_action(v_hold)
	a = mdp.actions[i_a]
	r, s′ = mdp.step(s, a; step_kwargs...)
	q = r + γ*simulate!(visit_counts, Q, mdp, γ, v_est, s′, depth - 1, c, v_hold, update_tree_policy!, updateQ!, updateV!, q_hold, apply_bonus!, step_kwargs, est_kwargs)
	
	updateV!(visit_counts, one(T), s, i_a)

	# δq = (q - Q[s][i_a]) / visit_counts[s][i_a]
	δq = (q - get_dict_value(Q[s], i_a)) / visit_counts[s][i_a]
	updateQ!(Q, δq, s, i_a)
	return q
end

# ╔═╡ 037f1804-b24e-46e7-b2a8-6747e669db66
makelookup(v::Vector) = Dict(x => i for (i, x) in enumerate(v))

# ╔═╡ e25ec0d5-f70f-4269-b2a1-efa194936f72
abstract type MDP{S, A, F, G, H} end

# ╔═╡ f9e4baec-c988-4abd-9bb0-c618c0ec07b9
#perform a rollout with an mdp from state s using a policy function π that produces an action selection given a state input. return value is an unbiased estimate of the value of this state under the policy
function rollout(mdp::MDP{S, A, F, G, H}, s::S, π::Function, γ::T; max_steps = Inf) where {S, A, F, G, H, T<:Real}
	step = 0
	#note that the terminal state will not be added to the state list
	r = zero(T)
	while !mdp.isterm(s) && (step <= max_steps)
		a = π(s)
		r, s′ = mdp.step(s, a)
		r += γ^step * r
		s = s′
		step += 1
	end
	return r
end

# ╔═╡ fad6e3b9-6d6e-4ea0-ac3d-5e57374c7056
#if no policy is provided then the rollout will use a uniformly random policy
rollout(mdp::MDP{S, A, F, G, H}, s::S, γ::T; max_steps = Inf) where {S, A, F, G, H, T<:Real} = rollout(mdp, s, s -> rand(mdp.actions), γ; max_steps = max_steps)

# ╔═╡ 1eb9a2ad-4584-4d32-8abb-e0e0bc0a771b
#if no value estimation function is provided, use a random rollout
monte_carlo_tree_search(mdp::MDP{S, A, F, G, H}, γ::T, s::S; kwargs...) where {S, A, F, G, H, T<:Real} = monte_carlo_tree_search(mdp, γ, rollout, s; kwargs...)

# ╔═╡ 47ce2eda-b2c4-4f81-8d91-955bc35bab49
begin
	struct AfterstateMDP_MC{S, AS, A, F<:Function, G<:Function, H<:Function, I<:Function}
		init_states::Vector{S}
		init_afterstates::Dict{Tuple{S, A}, AS}
		actions::Vector{A}
		afterstate_step::F #deterministic function that produces a reward and afterstate given a state action pair
		afterstate_transition::G #stochastic function that preduces a probability distribution of reward, state pairs given an afterstate
		state_init::H #function that produces an initial state for an episode
		isterm::I #function that returns true if the state input is terminal
		action_index::Dict{A, Int64}
		function AfterstateMDP_MC(init_states::Vector{S}, actions::Vector{A}, afterstate_step::F, afterstate_transition::G, isterm::I) where {S, A, F<:Function, G<:Function, I<:Function}
			state_init() = rand(init_states)
			init_afterstates = Dict((s, a) => afterstate_step(s, a)[2] for a in actions for s in init_states)
			action_index = makelookup(actions)
			new{S, typeof(init_afterstates[first(init_states), first(actions)]), A, F, G, typeof(state_init), I}(init_states, init_afterstates, actions, afterstate_step, afterstate_transition, state_init, isterm, action_index)
		end	
	end
end

# ╔═╡ f143446c-e44b-4d75-baa7-3b24eafad003
#need to decide which tree statistics to collect like state values or afterstate values and what expansion means vs normal mcts.  I know that when I visit a new afterstate which is the same as a new action selection, I want to estimate it with a weighted sum of the value estimates of all the sucessor states but I don't necessarily want the tree search to continue down all those paths and split although it could so a single simulation would split into all the successor states avoiding the need to make a selection.  For doing sample updates though, I want to just pick one of those branches to go down by sampling from the distribution so then the simulation function itself should handle the case of an unvisited state which would look at the afterstate values that lead from that state if any exist and well this is the problem is which values should be saved and what does it mean to estimate the value of something for one of the unvisited states
function simulate!(s::S, visit::Bool, tree_values::Dict{S, Tuple{T, Dict{Int64, Tuple{T, T}}}}, mdp::AfterstateMDP_MC{S, AS, A, F, G, H, I}, γ::T, v_est::Function, depth::Integer, c::T, v_hold, update_tree_policy!, update_tree!, q_hold, apply_bonus!, step_kwargs, transition_kwargs, est_kwargs) where {T<:Real, S, AS, A, F<:Function, G<:Function, H<:Function, I<:Function}
	#if the state is terminal, produce a value of 0
	mdp.isterm(s) && return zero(T)

	depth ≤ 0 && return v_est(mdp, s, γ; est_kwargs...)
	
	#for a state where no actions have been attempted, expand a new node
	if !haskey(tree_values, s)
		v = v_est(mdp, s, γ; est_kwargs...)
		tree_values[s] = (v, Dict{Int64, Tuple{T, T}}()) 
		return v
	end

	!visit && return max(tree_values[s][1], maximum(t[2]/t[1] for t in values(tree_values[s][2]); init = zero(T))) #if not visiting this state then just return the best value estimate and do not update the tree values

	#compute value estimates and bonus applies to each potential action
	apply_bonus!(v_hold, tree_values, s, c)
	update_tree_policy!(v_hold, s)

	#select an action from the tree policy
	i_a = sample_action(v_hold)
	a = mdp.actions[i_a]
	r1, w = mdp.afterstate_step(s, a; step_kwargs...) #take a step with the action and get the afterstate
	v_w = simulate!(w, tree_values, mdp, γ, v_est, depth, c, v_hold, update_tree_policy!, update_tree!, q_hold, apply_bonus!, step_kwargs, transition_kwargs, est_kwargs)
	v_a = r1 + v_w #value for the visited action
	update_tree!(tree_values, v_a, s, i_a)
	return max(tree_values[s][1], maximum(t[2]/t[1] for t in values(tree_values[s][2]); init = zero(T))) #the value that was just updated will be included in this maximum
end

# ╔═╡ 13f08473-f0d8-47d1-aa48-de3e4a083dbf
function simulate!(w::AS, tree_values::Dict{S, Tuple{T, Dict{Int64, Tuple{T, T}}}}, mdp::AfterstateMDP_MC{S, AS, A, F, G, H, I}, γ::T, v_est::Function, depth::Integer, c::T, v_hold, update_tree_policy!, update_tree!, q_hold, apply_bonus!, step_kwargs, transition_kwargs, est_kwargs) where {T<:Real, S, AS, A, F<:Function, G<:Function, H<:Function, I<:Function}
	dist = mdp.afterstate_transition(w; transition_kwargs...) #get the distribution of states following the transition
	k_sample = sample(collect(keys(dist)), weights(collect(values(dist)))) #sample one of the transition states to visit in the tree
	sum(begin
		(r, s) = k
		p = dist[k]
		v′ = simulate!(s, k == k_sample, tree_values, mdp, γ, v_est, depth - 1, c, v_hold, update_tree_policy!, update_tree!, q_hold, apply_bonus!, step_kwargs, transition_kwargs, est_kwargs)
		p * (r + γ * v′) 
	end
	for k in keys(dist))
end

# ╔═╡ 304e6afd-11e0-4011-9929-85889b988400
struct MDP_MC{S, A, F<:Function, G<:Function, H<:Function} <: MDP{S, A, F, G, H}
	init_states::Vector{S}
	actions::Vector{A}
	step::F #function that produces reward and updated state given a state action pair
	state_init::G #function that produces an initial state for an episode
	isterm::H #function that returns true if the state input is terminal
	function MDP_MC(init_states::Vector{S}, actions::Vector{A}, step::F, isterm::H) where {S, A, F<:Function, H<:Function}
		state_init() = rand(init_states)
		new{S, A, F, typeof(state_init), H}(init_states, actions, step, state_init, isterm)
	end
end

# ╔═╡ c62cc32c-0d29-4ea2-8284-ac4c883df6db
struct MDP_TD{S, A, F<:Function, G<:Function, H<:Function} <: MDP{S, A, F, G, H}
	states::Vector{S}
	statelookup::Dict{S, Int64}
	actions::Vector{A}
	actionlookup::Dict{A, Int64}
	state_init::G #function that produces an initial state for an episode
	step::F #function that produces reward and updated state given a state action pair
	isterm::H #function that returns true if the state input is terminal
	function MDP_TD(states::Vector{S}, actions::Vector{A}, state_init::G, step::F, isterm::H) where {S, A, F<:Function, G<:Function, H<:Function}
		statelookup = makelookup(states)
		actionlookup = makelookup(actions)
		new{S, A, F, G, H}(states, statelookup, actions, actionlookup, state_init, step, isterm)
	end
end

# ╔═╡ fb5601b0-06d4-43c4-81a6-23a4a8f29f00
function make_random_policy(mdp::MDP_TD; init::T = 1.0f0) where T <: AbstractFloat
	ones(T, length(mdp.actions), length(mdp.states)) ./ length(mdp.actions)
end

# ╔═╡ 1d97325a-8b9a-438d-a5f9-e17638e64627
initialize_state_value(mdp::MDP_TD; vinit::T = 0.0f0) where T<:AbstractFloat = ones(T, length(mdp.states)) .* vinit

# ╔═╡ 7ae23e8e-d554-4d26-a08a-83dab507af13
initialize_state_action_value(mdp::MDP_TD; qinit::T = 0.0f0) where T<:AbstractFloat = ones(T, length(mdp.actions), length(mdp.states)) .* qinit

# ╔═╡ eb9ba23d-bee5-4bb1-b3e1-fe40d9f681dc
function check_policy(π::Matrix{T}, mdp::MDP_TD) where {T <: AbstractFloat}
#checks to make sure that a policy is defined over the same space as an MDP
	(n, m) = size(π)
	num_actions = length(mdp.actions)
	num_states = length(mdp.states)
	@assert n == num_actions "The policy distribution length $n does not match the number of actions in the mdp of $(num_actions)"
	@assert m == num_states "The policy is defined over $m states which does not match the mdp state count of $num_states"
	return nothing
end

# ╔═╡ 618b5f35-2df5-4ffb-a34f-add542691080
#take a step in the environment from state s using policy π represented by a distribution over state-action pairs
function takestep(mdp, π::Matrix{T}, s) where T<:Real
	i_s = mdp.statelookup[s]
	i_a = sample_action(π, i_s)
	a = mdp.actions[i_a]
	(r, s′) = mdp.step(s, a)
	i_s′ = mdp.statelookup[s′]
	return (i_s, i_s′, r, s′, a, i_a)
end

# ╔═╡ 3edc09ab-8fa1-440f-8a45-546898a2b2a3
#take a step in the environment from state s using policy π represented by a function that returns an action given a state
function takestep(mdp, π::Function, s)
	a = π(s)
	(r, s′) = mdp.step(s, a)
	return (0, 0, r, s′, a, 0)
end

# ╔═╡ 6612f482-2f10-43fa-9b7b-2f0c6a94b8e8
runepisode(mdp::MDP_TD; kwargs...) = runepisode(mdp, make_random_policy(mdp); kwargs...)

# ╔═╡ e43513e8-2517-43b7-9a16-e57d4125edc4
runepisode(mdp::MDP_MC; kwargs...) = runepisode(mdp, s -> rand(mdp.actions); kwargs...)

# ╔═╡ 94b339bb-6e2d-422f-8043-615e8be9a217
begin
	abstract type CompleteMDP{T<:Real, S, A} end
	struct FiniteMDP{T<:Real, S, A} <: CompleteMDP{T, S, A} 
		states::Vector{S}
		actions::Vector{A}
		rewards::Vector{T}
		# ptf::Dict{Tuple{S, A}, Matrix{T}}
		ptf::Array{T, 4}
		action_scratch::Vector{T}
		state_scratch::Vector{T}
		reward_scratch::Vector{T}
		state_index::Dict{S, Int64}
		action_index::Dict{A, Int64}
		function FiniteMDP{T, S, A}(states::Vector{S}, actions::Vector{A}, rewards::Vector{T}, ptf::Array{T, 4}) where {T <: Real, S, A}
			new(states, actions, rewards, ptf, Vector{T}(undef, length(actions)), Vector{T}(undef, length(states)+1), Vector{T}(undef, length(rewards)), Dict(zip(states, eachindex(states))), Dict(zip(actions, eachindex(actions))))
		end	
	end
	FiniteMDP(states::Vector{S}, actions::Vector{A}, rewards::Vector{T}, ptf::Array{T, 4}) where {T <: Real, S, A} = FiniteMDP{T, S, A}(states, actions, rewards, ptf)
	
	struct FiniteDeterministicMDP{T<:Real, S, A} <: CompleteMDP{T, S, A}
		states::Vector{S}
		actions::Vector{A}
		state_index::Dict{S, Int64}
		action_index::Dict{A, Int64}
		state_transition_map::Matrix{Int64} #index of state reached from the state corresponding to the column when taking action corresponding to the row
		reward_transition_map::Matrix{T} #reward received for the transition from the state corresponding to the column when taking action corresponding to the row
	end
	FiniteDeterministicMDP(states::Vector{S}, actions::Vector{A}, state_transition_map::Matrix{Int64}, reward_transition_map::Matrix{T}) where {T<:Real, S, A} = FiniteDeterministicMDP{T, S, A}(states, actions, makelookup(states), makelookup(actions), state_transition_map, reward_transition_map)
	
	struct FiniteStochasticMDP{T<:Real, S, A} <: CompleteMDP{T, S, A}
		states::Vector{S}
		actions::Vector{A}
		state_index::Dict{S, Int64}
		action_index::Dict{A, Int64}
		ptf::Dict{Tuple{Int64, Int64}, Dict{Int64, Tuple{T, T}}} #for each state action pair index there is a corresponding dictionary mapping each transition state index to the probability of that transition and the average reward received
	end
	FiniteStochasticMDP(states::Vector{S}, actions::Vector{A}, ptf::Dict{Tuple{Int64, Int64}, Dict{Int64, Tuple{T, T}}}) where {T<:Real, S, A} = FiniteStochasticMDP{T, S, A}(states, actions, makelookup(states), makelookup(actions), ptf)
end

# ╔═╡ 81f2f335-6606-4506-bfb3-d0d95e651f24
function bellman_optimal_update!(Q::Matrix{T}, i_s::Int64, i_a::Int64, mdp::FiniteStochasticMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	#perform a bellman optimal update for a given state action pair index and return the percentage change in value
	q_avg = zero(T)
	r_avg = zero(T)
	ptf = mdp.ptf[(i_a, i_s)]
	x = zero(T)
	for i_s′ in keys(ptf)
		(p, r) = ptf[i_s′]
		maxvalue = typemin(T)
		@inbounds @fastmath @simd for i_a′ in eachindex(mdp.actions)
			maxvalue = max(maxvalue, Q[i_a′, i_s′])
		end
		x += p*(r + γ * maxvalue)
	end
	delt = abs(x - Q[i_a, i_s]) / (eps(abs(Q[i_a, i_s])) + abs(Q[i_a, i_s]))
	Q[i_a, i_s] = x
	return delt
end

# ╔═╡ a8ec05ad-8333-4423-ab42-883ab806ebd7
function uniform_bellman_optimal_value!(Q::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	delt = zero(T)
	num_updates = 0
	for i_s in eachindex(mdp.states)
		for i_a in eachindex(mdp.actions)
			delt = max(delt, bellman_optimal_update!(Q, i_s, i_a, mdp, γ))
			num_updates += 1
		end
	end
	return delt, num_updates
end

# ╔═╡ 094321bc-2d44-4e67-9ac6-5216a42e0cd3
function bellman_policy_update!(Q::Matrix{T}, π::Matrix{T}, i_s::Int64, i_a::Int64, mdp::FiniteStochasticMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	#perform a bellman optimal update for a given state action pair index and return the percentage change in value
	q_avg = zero(T)
	r_avg = zero(T)
	ptf = mdp.ptf[(i_a, i_s)]
	x = zero(T)
	for i_s′ in keys(ptf)
		(p, r) = ptf[i_s′]
		v = zero(T)
		@inbounds @fastmath @simd for i_a′ in eachindex(mdp.actions)
			v += π[i_a′, i_s′] * Q[i_a′, i_s′]
		end
		x += p*(r + γ * v)
	end
	delt = abs(x - Q[i_a, i_s]) / (eps(abs(Q[i_a, i_s])) + abs(Q[i_a, i_s]))
	Q[i_a, i_s] = x
	return delt
end

# ╔═╡ 6e273f2b-a1af-421f-aca7-772a836b89ef
function uniform_bellman_policy_value!(Q::Matrix{T}, π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	delt = zero(T)
	num_updates = 0
	for i_s in eachindex(mdp.states)
		for i_a in eachindex(mdp.actions)
			delt = max(delt, bellman_policy_update!(Q, π, i_s, i_a, mdp, γ))
			num_updates += 1
		end
	end
	return delt, num_updates
end

# ╔═╡ ffdd925e-b2b4-4cb1-9d6f-b8c9397729f6
function q_policy_evaluation!(Q::Matrix{T}, π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, γ::T; max_updates = typemax(Int64), θ = eps(zero(T))) where {T <: Real, S, A}
	delt, num_updates = uniform_bellman_policy_value!(Q, π, mdp, γ)
	total_updates = num_updates
	while (delt > θ) && (total_updates <= max_updates)
		delt, num_updates = uniform_bellman_policy_value!(Q, π, mdp, γ)
		total_updates += num_updates
	end
	return Q
end

# ╔═╡ 6605b946-3010-47ed-8d88-3c4dca993cf8
begin_value_iteration_q(mdp::CompleteMDP{T,S,A}, γ::T; Qinit::T = zero(T), kwargs...) where {T<:Real,S,A} = begin_value_iteration_q(mdp, γ, Qinit .* ones(T, length(mdp.actions), length(mdp.states)); kwargs...)

# ╔═╡ 636d768c-670d-4485-a1dd-2bab6cf086d0
function make_random_mdp(num_states, b)
 	states = collect(1:num_states)
	actions = [1, 2]
	next_states = [rand(states, b) for _ in 1:num_states*2]

	rterm = randn()
	ptf = Dict((i_a, i_s) => Dict(zip([num_states+1; next_states[i_s*i_a]], zip([0.1; fill((1-0.1)/b, b)], [rterm; randn(b)]))) for i_s in eachindex(states) for i_a in eachindex(actions))

	#all transitions from terminal state lead to terminal state
	ptf[(1, num_states + 1)] = Dict([num_states+1 => (1., 0.0)])
	ptf[(2, num_states + 1)] = Dict([num_states+1 => (1., 0.0)])
	
	# function step(s, a)
	# 	r = randn()
	# 	rand() <= 0.1 && return (r, 0)
		
	# 	#each action result in one of b next states all equally likely with a different random selection of b states for each state-action pair
	# 	s′ = rand(next_states[s*a])
	# 	return (r, s′)
	# end
 
	# MDP_TD([states; 0], actions, () -> 1, step, s -> s == 0)
	FiniteStochasticMDP([states; 0], actions, ptf)
end

# ╔═╡ f8bf29fe-568f-437f-ba82-6b861988a18e
function make_greedy_policy!(π::Matrix{T}, mdp::FiniteDeterministicMDP{T, S, A}, V::Vector{T}, γ::T) where {T<:Real,S,A}
	for i_s in eachindex(mdp.states)
		maxv = -Inf
		for i_a in eachindex(mdp.actions)
			i_s′ = mdp.state_transition_map[i_a, i_s]
			r = mdp.reward_transition_map[i_a, i_s]
			x = r + γ*V[i_s′]
			maxv = max(maxv, x)
			π[i_a, i_s] = x
		end
		π[:, i_s] .= (π[:, i_s] .≈ maxv)
		π[:, i_s] ./= sum(π[:, i_s])
	end
	return π
end

# ╔═╡ 82e1ceb8-b1bb-4dea-b041-bf462041793f
function make_greedy_policy!(π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, V::Vector{T}, γ::T) where {T<:Real,S,A}
	for i_s in eachindex(mdp.states)
		maxv = -Inf
		for i_a in eachindex(mdp.actions)
			r_avg = zero(T)
			v_avg = zero(T)
			ptf = mdp.ptf[(i_a, i_s)]
			for i_s′ in keys(ptf)
				p = ptf[i_s′][1]
				v_avg += p*V[i_s′]
				r_avg += p*ptf[i_s′][2]
			end
			x = r_avg + γ*v_avg
			maxv = max(maxv, x)
			π[i_a, i_s] = x
		end
		π[:, i_s] .= (π[:, i_s] .≈ maxv)
		π[:, i_s] ./= sum(π[:, i_s])
	end
	return π
end

# ╔═╡ e4f73889-af82-4304-89d5-ee50172eb3da
function make_greedy_policy!(π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, Q::Matrix{T}) where {T<:Real,S,A}
	for i_s in eachindex(mdp.states)
		maxq = -Inf
		for i_a in eachindex(mdp.actions)
			maxq = max(maxq, Q[i_a, i_s])
		end
		π[:, i_s] .= (Q[:, i_s] .≈ maxq)
		π[:, i_s] ./= sum(π[:, i_s])
	end
	return π
end

# ╔═╡ ecd8742c-2e10-4814-b477-7024e85b7fa6
function make_ϵ_greedy_policy!(π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, Q::Matrix{T}, ϵ::T) where {T<:Real,S,A}
	n = length(mdp.actions)
	for i_s in eachindex(mdp.states)
		maxq = -Inf
		for i_a in eachindex(mdp.actions)
			maxq = max(maxq, Q[i_a, i_s])
		end
		π[:, i_s] .= (Q[:, i_s] .≈ maxq)
		π[:, i_s] ./= (sum(π[:, i_s]) / (one(T) - ϵ))
		π[:, i_s] .+= ϵ / actions
	end
	return π
end

# ╔═╡ fc137613-7b4b-414c-93af-eeb2ace5d67f
function make_ϵ_greedy_policy!(π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, Q::Matrix{T}, ϵ::T, i_s::Int64) where {T<:Real,S,A}
	n = length(mdp.actions)
	maxq = -Inf
	for i_a in eachindex(mdp.actions)
		π[i_a, i_s] = Q[i_a, i_s]
		maxq = max(maxq, Q[i_a, i_s])
	end
	π[:, i_s] .= (π[:, i_s] .≈ maxq)
	π[:, i_s] ./= (sum(π[:, i_s]) / (one(T) - ϵ))
	π[:, i_s] .+= ϵ / n
	if isnan(π[1, i_s])
		@info "Q = $(Q[:, i_s]), maxq = $maxq"
	end
	return π
end

# ╔═╡ 6c8cd429-2c2e-4515-98b2-d0394962e479
function takestep(mdp::FiniteDeterministicMDP{T, S, A}, π::Matrix{T}, s) where {T<:Real, S, A}
	i_s = mdp.state_index[s]
	i_a = sample_action(π, i_s)
	i_s′ = mdp.state_transition_map[i_a, i_s]
	r = mdp.reward_transition_map[i_a, i_s]
	s′ = mdp.states[i_s′]
	a = mdp.actions[i_a]
	return (i_s, i_s′, r, s′, a, i_a)
end

# ╔═╡ 3b4e27e7-8065-44b3-bc2a-e540913aa540
function takestep(mdp::FiniteStochasticMDP{T, S, A}, π::Matrix{T}, s) where {T<:Real, S, A}
	i_s = mdp.state_index[s]
	i_a = sample_action(π, i_s)
	ptf = mdp.ptf[(i_a, i_s)]
	probabilities = [ptf[i_s′][1] for i_s′ in keys(ptf)]
	i_s′ = sample(collect(keys(ptf)), weights(probabilities))
	s′ = mdp.states[i_s′]
	r = ptf[i_s′][2]
	a = mdp.actions[i_a]
	return (i_s, i_s′, r, s′, a, i_a)
end

# ╔═╡ b15f1262-1acf-40e5-87a7-bc4b1b437a42
function runepisode(mdp::MDP{S, A, F, G, H}, π; max_steps = Inf) where {S, A, F, G, H}
	s = mdp.state_init()
	states = Vector{S}()
	actions = Vector{A}()
	push!(states, s)
	(_, _, r, s′, a, _) = takestep(mdp, π, s)
	push!(actions, a)
	rewards = [r]
	step = 2
	sterm = s
	if mdp.isterm(s′)
		sterm = s′
	else
		sterm = s
	end
	s = s′

	#note that the terminal state will not be added to the state list
	while !mdp.isterm(s) && (step <= max_steps)
		push!(states, s)
		(_, _, r, s′, a, _) = takestep(mdp, π, s)
		push!(actions, a)
		push!(rewards, r)
		s = s′
		step += 1
		if mdp.isterm(s′)
			sterm = s′
		end
	end
	return states, actions, rewards, sterm
end

# ╔═╡ 250ea9da-dea3-4bf3-932d-cdda6756ae33
function runepisode(mdp::CompleteMDP{T, S, A}, s0::S, isterm::Function, π::Matrix{T}; max_steps = Inf) where {T<:Real, S, A}
	s = s0
	states = Vector{S}()
	actions = Vector{A}()
	push!(states, s)
	(_, _, r, s′, a, _) = takestep(mdp, π, s)
	push!(actions, a)
	rewards = [r]
	step = 2
	sterm = s
	if isterm(s′)
		sterm = s′
	else
		sterm = s
	end
	s = s′

	#note that the terminal state will not be added to the state list
	while !isterm(s) && (step <= max_steps)
		push!(states, s)
		(_, _, r, s′, a, _) = takestep(mdp, π, s)
		push!(actions, a)
		push!(rewards, r)
		s = s′
		step += 1
		if isterm(s′)
			sterm = s′
		end
	end
	return states, actions, rewards, sterm
end

# ╔═╡ 726af565-8905-4409-864f-a5c1b5767e09
#forms a random policy for a generic finite state mdp.  The policy is a matrix where the rows represent actions and the columns represent states.  Each column is a probability distribution of actions over that state.
form_random_policy(mdp::CompleteMDP{T, S, A}) where {T, S, A} = ones(T, length(mdp.actions), length(mdp.states)) ./ length(mdp.actions)

# ╔═╡ 33f66659-1a87-4890-9137-dbc7776a19d8
function make_greedy_policy!(π::Matrix{T}, mdp::FiniteMDP{T, S, A}, V::Vector{T}, γ::T) where {T<:Real,S,A}
	for i_s in eachindex(mdp.states)
		maxv = -Inf
		for i_a in eachindex(mdp.actions)
			x = zero(T)
			for i_r in eachindex(mdp.rewards)
				for i_s′ in eachindex(V)
					x += mdp.ptf[i_s′, i_r, i_a, i_s] * (mdp.rewards[i_r] + γ * V[i_s′])
				end
			end
			maxv = max(maxv, x)
			π[i_a, i_s] = x
		end
		π[:, i_s] .= (π[:, i_s] .≈ maxv)
		π[:, i_s] ./= sum(π[:, i_s])
	end
	return π
end

# ╔═╡ 195d2a34-c44c-4088-8ec4-dece3107f16d
function bellman_optimal_value!(V::Vector{T}, mdp::FiniteMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	delt = zero(T)
	@inbounds @fastmath @simd for i_s in eachindex(mdp.states)
		maxvalue = typemin(T)
		@inbounds @fastmath @simd for i_a in eachindex(mdp.actions)
			x = zero(T)
			for (i_r, r) in enumerate(mdp.rewards)
				@inbounds @fastmath @simd for i_s′ in eachindex(V)
					x += mdp.ptf[i_s′, i_r, i_a, i_s] * (r + γ * V[i_s′])
				end
			end
			maxvalue = max(maxvalue, x)
		end
		delt = max(delt, abs(maxvalue - V[i_s]) / (eps(abs(V[i_s])) + abs(V[i_s])))
		V[i_s] = maxvalue
	end
	return delt
end

# ╔═╡ 16c68a13-c295-4a64-bc2b-2ae8451f332f
function bellman_optimal_value!(V::Vector{T}, mdp::FiniteDeterministicMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	delt = zero(T)
	@inbounds @fastmath @simd for i_s in eachindex(mdp.states)
		maxvalue = typemin(T)
		@inbounds @fastmath @simd for i_a in eachindex(mdp.actions)
			i_s′ = mdp.state_transition_map[i_a, i_s]
			r = mdp.reward_transition_map[i_a, i_s]
			x = r + γ*V[i_s′]
			maxvalue = max(maxvalue, x)
		end
		delt = max(delt, abs(maxvalue - V[i_s]) / (eps(abs(V[i_s])) + abs(V[i_s])))
		V[i_s] = maxvalue
	end
	return delt
end

# ╔═╡ 44364e7f-1910-421a-b961-63fbbaac8230
function bellman_optimal_value!(V::Vector{T}, mdp::FiniteStochasticMDP{T, S, A}, γ::T) where {T <: Real, S, A}
	delt = zero(T)
	@inbounds @fastmath @simd for i_s in eachindex(mdp.states)
		maxvalue = typemin(T)
		@inbounds @fastmath @simd for i_a in eachindex(mdp.actions)
			r_avg = zero(T)
			v_avg = zero(T)
			ptf = mdp.ptf[(i_a, i_s)]
			for i_s′ in keys(ptf)
				p = ptf[i_s′][1]
				r = ptf[i_s′][2]
				v_avg += p*V[i_s′]
				r_avg += p*r
			end
			x = r_avg + γ*v_avg
			maxvalue = max(maxvalue, x)
		end
		delt = max(delt, abs(maxvalue - V[i_s]) / (eps(abs(V[i_s])) + abs(V[i_s])))
		V[i_s] = maxvalue
	end
	return delt
end

# ╔═╡ 4b2a4fb1-7395-4293-9ff9-e2f9da50f56b
function value_iteration_q!(Q, θ, mdp, γ, nmax)
	nmax <= 0 && return Q
	
	#update value function
	delt = bellman_optimal_value!(Q, mdp, γ)

	#halt when value function is no longer changing
	delt <= θ && return Q
	
	value_iteration_q!(Q, θ, mdp, γ, nmax - 1)	
end

# ╔═╡ 6778296c-ab05-47e7-86d2-e98c075a8a0c
function begin_value_iteration_q(mdp::M, γ::T, Q::Matrix{T}; θ = eps(zero(T)), nmax=typemax(Int64)) where {T<:Real, M <: CompleteMDP{T, S, A} where {S, A}}
	value_iteration_q!(Q, θ, mdp, γ, nmax)
end

# ╔═╡ 80affd41-b5e6-4b9c-b827-4e3b39bd7767
function value_iteration_v!(V, θ, mdp, γ, nmax, valuelist)
	nmax <= 0 && return valuelist
	
	#update value function
	delt = bellman_optimal_value!(V, mdp, γ)
	
	#add copy of value function to results list
	push!(valuelist, copy(V))

	#halt when value function is no longer changing
	delt <= θ && return valuelist
	
	value_iteration_v!(V, θ, mdp, γ, nmax - 1, valuelist)	
end

# ╔═╡ 2fa207dd-749f-4dc0-b4ab-159edf1d9bce
begin_value_iteration_v(mdp::CompleteMDP{T,S,A}, γ::T; Vinit::T = zero(T), kwargs...) where {T<:Real,S,A} = begin_value_iteration_v(mdp, γ, Vinit .* ones(T, length(mdp.states)); kwargs...)

# ╔═╡ 6cf35193-dba5-4f78-a4ac-245dda7a0846
begin_value_iteration_v(mdp::FiniteMDP{T,S,A}, γ::T; Vinit::T = zero(T), kwargs...) where {T<:Real,S,A} = begin_value_iteration_v(mdp, γ, Vinit .* ones(T, size(mdp.ptf, 1)); kwargs...)

# ╔═╡ 96bd8d33-d4e8-45bf-9b75-43e8bda6fa07
function make_ϵ_greedy_policy!(v::AbstractVector{T}, ϵ::T; valid_inds = eachindex(v)) where T <: Real
	vmax = maximum(view(v, valid_inds))
	v .= T.(isapprox.(v, vmax))
	s = sum(v)
	c = ϵ / length(valid_inds)
	d = c + (one(T) - ϵ)/s #value to add to actions that are maximizing
	v .= c .+ (v .* d)
	# for i in valid_inds
	# 	if v[i] == 1
	# 		v[i] = d + c
	# 	else
	# 		v[i] = c
	# 	end
	# end
	return v
end

# ╔═╡ d0b18699-7d3a-418d-9d15-be41f1643f09
function trajectory_bellman_optimal_value!(Q::Matrix{T}, π::Matrix{T}, mdp::FiniteStochasticMDP{T, S, A}, γ::T, ϵ::T, s0::S, isterm::Function) where {T <: Real, S, A}
	delt = zero(T)
	num_updates = 0
	s = s0
	i_s = mdp.state_index[s]
	make_ϵ_greedy_policy!(π, mdp, Q, ϵ, i_s)
	while !isterm(s)
		(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
		delt = max(delt, bellman_optimal_update!(Q, i_s, i_a, mdp, γ))
		make_ϵ_greedy_policy!(π, mdp, Q, ϵ, i_s′)
		num_updates += 1
		s = s′
	end
	return delt, num_updates
end

# ╔═╡ 00c8f62f-dee2-476c-b896-68d3ab57a168
function figure_8_8(num_states; expected_updates_max = 20_000, num_samples = 200, blist = [1, 3, 10])
	function get_uniform_value(num_states, b)
		Q = zeros(2, num_states + 1)
		Qref = copy(Q)
		π = copy(Q)
		mdp = make_random_mdp(num_states, b)
		total_updates = 0
		x = Vector{Int64}()
		y = Vector{Float64}()
		while total_updates <= expected_updates_max
			(delt, num_updates) = uniform_bellman_optimal_value!(Q, mdp, 1.0)
			# make_greedy_policy!(π, mdp, Q)
			# q_policy_evaluation!(Qref, π, mdp, 1.0)
			# qval = maximum(Qref[:, 1])
			qval = maximum(Q[:, 1])
			total_updates += num_updates
			push!(x, num_updates)
			push!(y, qval)
		end
		return x, y
	end

	function get_on_policy_value(num_states, b)
		Q = zeros(2, num_states + 1)
		π = copy(Q)
		mdp = make_random_mdp(num_states, b)
		total_updates = 0
		x = Vector{Int64}()
		y = Vector{Float64}()
		while total_updates <= expected_updates_max
			(delt, num_updates) = trajectory_bellman_optimal_value!(Q, π, mdp, 1.0, 0.1, 1, s -> s == 0)
			qval = maximum(Q[:, 1])
			total_updates += num_updates
			push!(x, total_updates)
			push!(y, qval)
		end
		updatecounts = collect(1:num_states*2:expected_updates_max)
		values = [y[min(searchsortedfirst(x, c), length(y))] for c in updatecounts]
		return updatecounts, values
	end
	
	uniform_traces = [begin
		out = 1:num_samples |> Map(_ -> get_uniform_value(num_states, b)) |> foldxt((x, y) -> ((x[1] .+ y[1]), (x[2] .+ y[2])))
		trace = scatter(x = [0; cumsum(out[1] ./ num_samples)], y = [0; out[2] ./ num_samples], name = "b = $b, uniform")
	end
	for b in blist]

	on_policy_traces = [begin
		out = 1:num_samples |> Map(_ -> get_on_policy_value(num_states, b)) |> foldxt((x, y) -> ((x[1] .+ y[1]), (x[2] .+ y[2])))
		trace = scatter(x = [0; out[1] ./ num_samples], y = [0; out[2] ./ num_samples], name = "b = $b, on-policy")
	end
	for b in blist]
	
	plot([uniform_traces; on_policy_traces], Layout(xaxis_title = "Computation time, in expected updates", yaxis_title = "Value of start state under greedy policy"))
end

# ╔═╡ 8586d633-7c50-49ba-9b74-b5bdad27c317
figure_8_8(1000; num_samples = 200)

# ╔═╡ fcaddc19-c0c9-4e8e-8f84-adc7a02cc1f5
figure_8_8(10_000; blist = [1, 3], num_samples = 10, expected_updates_max = 200_000)

# ╔═╡ 5e49504e-9623-48f9-aeb5-360906b92a09
vholdtest = [[1f0, 1f0, 1f0]; fill(Inf32, length(nyt_valid_words) - 3)]

# ╔═╡ 679b6096-b3ca-422d-9d9a-225832510ab1
make_ϵ_greedy_policy!(vholdtest, 0.1f0)

# ╔═╡ 0adcbce8-2be5-48ef-af43-04815e10dc5c
function make_greedy_policy!(v::AbstractVector{T}; c = 1000) where T<:Real
	(vmin, vmax) = extrema(v)
	isnan(vmin) && error("NaN values in vector")
	if isinf(vmax)
		v .= T.(isinf.(v))
		v ./= sum(v)
	elseif vmin == vmax
		v .= zero(T)
		v .= one(T) / length(v)
	else
		v .= (v .- vmax) ./ abs(vmin - vmax)
		v .= exp.(c .* v)
		v ./= sum(v)
	end
	return v
end

# ╔═╡ 466b1cbf-586f-4b53-8b4b-2dc32e1c8b0a
#perform action selection within an mdp for a given state s, discount factor γ, and state value estimation function v_est.  v_est must be a function that takes the arguments (mdp, s, γ) and produces a reward of the same type as γ
function monte_carlo_tree_search(mdp::MDP{S, A, F, G, H}, γ::T, v_est::Function, s::S; 
	depth = 10, 
	nsims = 100, 
	c = one(T), 
	# visit_counts = Dict{S, SparseVector{T, Int64}}(), 
	visit_counts = Dict{S, Dict{Int64, T}}(),
	# Q = Dict{S, SparseVector{T, Int64}}(),
	Q = Dict{S, Dict{Int64, T}}(),
	update_tree_policy! = (v, s) -> make_greedy_policy!(v), 
	v_hold = zeros(T, length(mdp.actions)),
	updateQ! = function(Q, x, s, i_a)
		# Q[s][i_a] += x
		Q[s][i_a] = get_dict_value(Q[s], i_a) + x
	end,
	updateV! = function(V, x, s, i_a)
		# V[s][i_a] += x
		V[s][i_a] = get_dict_value(V[s], i_a) + x
	end,
	apply_bonus! = apply_uct!,
	make_step_kwargs = k -> NamedTuple(), #option to create mdp step arguments that depend on the simulation number, 
	make_est_kwargs = k -> NamedTuple(), #option to create state estimation arguments that depend on the simulation number
	sim_message = false
	) where {S, A, F, G, H, T<:Real}

	q_hold = zeros(T, length(mdp.actions))
	#I want to have a way of possible a kwargs such as the answer index to the simulator that can change with each simulation
	t = time()
	last_time = t
	for k in 1:nsims
		seed = rand(UInt64)
		if sim_message
			elapsed = time() - last_time
			if elapsed > 5
				last_time = time()
				pct_done = k/nsims
				total_time = time() - t
				ett = total_time / pct_done
				eta = ett - total_time
				@info """Completed simulation $k of $nsims after $(round(Int64, total_time/60)) minutes
				ETA: $(round(Int64, eta/60)) minutes"""
			end
		end
		simulate!(visit_counts, Q, mdp, γ, v_est, s, depth, c, v_hold, update_tree_policy!, updateQ!, updateV!, q_hold, apply_bonus!, make_step_kwargs(seed), make_est_kwargs(seed))
	end

	minv = minimum(Q[s][k] for k in keys(Q[s]))
	# v_hold .= Q[s]
	for i in eachindex(v_hold)
		v_hold[i] = get_dict_value(Q[s], i; default = minv)
	end
	make_greedy_policy!(v_hold)
	if sim_message
		@info "Finished MCTS evaluation of state $s"
	end
	return mdp.actions[sample_action(v_hold)], visit_counts, Q
end

# ╔═╡ bf7950f2-05fa-4455-ad08-27735148d95c
#perform action selection within an mdp for a given state s, discount factor γ, and state value estimation function v_est.  v_est must be a function that takes the arguments (mdp, s, γ) and produces a reward of the same type as γ
function monte_carlo_tree_search(mdp::AfterstateMDP_MC{S, AS, A, F, G, H, I}, γ::T, v_est::Function, s::S; 
	depth = 10, 
	nsims = 100, 
	c = one(T), 
	tree_values = Dict{S, Tuple{T, Dict{Int64, Tuple{T, T}}}}(),
	update_tree_policy! = (v, s) -> make_greedy_policy!(v), 
	v_hold = zeros(T, length(mdp.actions)),
	update_tree! = function(tree_values, v::T, s::S, i_a::Integer)
		d = tree_values[s][2]
		new_value = if haskey(d, i_a)
			(d[i_a][1]+1, d[i_a][2]+v)
		else
			(1f0, v)
		end
		tree_values[s][2][i_a] = new_value
	end,
	apply_bonus! = apply_uct!,
	make_step_kwargs = k -> NamedTuple(), #option to create mdp afterstate step arguments that depend on the simulation number
	make_transition_kwargs = k -> NamedTuple(), #option to create mdp afterstate transition arguments that depend on the simulation number
	make_est_kwargs = k -> NamedTuple(), #option to create state estimation arguments that depend on the simulation number
	sim_message = false
	) where {S, AS, A, F, G, H, I, T<:Real}

	q_hold = zeros(T, length(mdp.actions))
	#I want to have a way of possible a kwargs such as the answer index to the simulator that can change with each simulation
	t = time()
	last_time = t
	for k in 1:nsims
		seed = rand(UInt64)
		if sim_message
			elapsed = time() - last_time
			if elapsed > 5
				last_time = time()
				pct_done = k/nsims
				total_time = time() - t
				ett = total_time / pct_done
				eta = ett - total_time
				@info """Completed simulation $k of $nsims after $(round(Int64, total_time/60)) minutes
				ETA: $(round(Int64, eta/60)) minutes"""
			end
		end
		simulate!(s, true, tree_values, mdp, γ, v_est, depth, c, v_hold, update_tree_policy!, update_tree!, q_hold, apply_bonus!, make_step_kwargs(seed), make_transition_kwargs(seed), make_est_kwargs(seed))
	end

	minv = minimum(t[2]/t[1] for t in values(tree_values[s][2]))
	for i in eachindex(v_hold)
		if haskey(tree_values[s][2], i)
			v_hold[i] = tree_values[s][2][i][2] / tree_values[s][2][i][1]
		else
			v_hold[i] = minv
		end
	end
	make_greedy_policy!(v_hold)
	if sim_message
		@info "Finished MCTS evaluation of state $s"
	end
	return mdp.actions[sample_action(v_hold)], tree_values
end

# ╔═╡ 0899f37c-5def-4d15-8ca3-ebdec8e96b43
function begin_value_iteration_v(mdp::M, γ::T, V::Vector{T}; θ = eps(zero(T)), nmax=typemax(Int64)) where {T<:Real, M <: CompleteMDP{T, S, A} where {S, A}}
	valuelist = [copy(V)]
	value_iteration_v!(V, θ, mdp, γ, nmax, valuelist)

	π = form_random_policy(mdp)
	make_greedy_policy!(π, mdp, V, γ)
	return (valuelist, π)
end

# ╔═╡ 9a1b250f-b404-4db3-a4b7-4cd33b79d921
function create_greedy_policy(Q::Matrix{T}; c = 1000, π = copy(Q)) where T<:Real
	vhold = zeros(T, size(Q, 1))
	for j in 1:size(Q, 2)
		vhold .= Q[:, j]
		make_greedy_policy!(vhold; c = c)
		π[:, j] .= vhold
	end
	return π
end

# ╔═╡ 324181f0-b890-4198-9b4b-c36547e6629a
function create_ϵ_greedy_policy(Q::Matrix{T}, ϵ::T; π = copy(Q), get_valid_inds = j -> 1:size(Q, 1)) where T<:Real
	vhold = zeros(T, size(Q, 1))
	for j in 1:size(Q, 2)
		vhold .= Q[:, j]
		make_ϵ_greedy_policy!(vhold, ϵ; valid_inds = get_valid_inds(j))
		π[:, j] .= vhold
	end
	return π
end

# ╔═╡ 5afad18b-1d87-450e-a0ff-8c1249d663ed
function tabular_dynaQ(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000,
	max_steps = 10000, n = 100, qinit = zero(T), ϵinit = one(T)/10, 
	Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), 
	decay_ϵ = false, history_state::S = first(mdp.states), 
	save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), 
	history = Dict{Int64, Set{Int64}}(),
	#each column contains the index of the state reached from the state represented by the column index while taking the action represented by the row index.  the values are initialized at 0 which represents a transition that has not occured
	state_transition_map = zeros(Int64, length(mdp.actions), length(mdp.states)),
	#each column contains the reward received from the state represented by the column index while taking the action represented by the row index.  the state_transition_map can be used to determine if any of these values have been updated from the zero initialization
	reward_transition_map = zeros(T, length(mdp.actions), length(mdp.states)),
	init_step = 0,
	isoptimal = l -> false) where {S, A, F, G, H, T<:AbstractFloat}

	π = copy(πinit)
	Q = copy(Qinit)
	ϵ = ϵinit
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q[:, terminds] .= zero(T)
	num_updates = 0
	
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards per step and steps per episode
	rewards = zeros(T, max_steps)
	steps = zeros(Int64, num_episodes)

	function updateQ!(Q, i_s, i_a, i_s′, r)
		qmax = maximum(Q[i, i_s′] for i in eachindex(mdp.actions))
		Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
		num_updates += 1
	end

	function q_planning!(Q, history)
		for _ in 1:n
			i_s = rand(keys(history))
			i_a = rand(history[i_s])
			i_s′ = state_transition_map[i_a, i_s]
			r = reward_transition_map[i_a, i_s]
			updateQ!(Q, i_s, i_a, i_s′, r)
		end
	end

	ep = 1
	step = 1
	optimal_path = false
	while (step <= max_steps) && (ep <= num_episodes) && !optimal_path
		s = mdp.state_init()
		l = 0
		while !mdp.isterm(s) && step <= max_steps
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			updateQ!(Q, i_s, i_a, i_s′, r)
			state_transition_map[i_a, i_s] = i_s′
			reward_transition_map[i_a, i_s] = r
			#save state action pair visited
			if haskey(history, i_s)
				push!(history[i_s], i_a)
			else
				history[i_s] = Set([i_a])
			end
			q_planning!(Q, history)
			π = create_ϵ_greedy_policy(Q, ϵ; π = π)
			s = s′
			rewards[step] = r
			step += 1
			l += 1
		end
		steps[ep] = l
		optimal_path = isoptimal(l)
		ep += 1
	end
	return Q, steps[1:ep-1], rewards[1:step-1], history, state_transition_map, reward_transition_map, num_updates
end

# ╔═╡ b9054ed4-7f16-4920-b13e-5f4c6f50dcf3
function tabular_dynaQplus(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; 
	num_episodes = 1000, max_steps = 10000, n = 100, qinit = zero(T), ϵinit = one(T)/10, 
	Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), 
	decay_ϵ = false, history_state::S = first(mdp.states), 
	save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), 
	history = zeros(Int64, length(mdp.actions), length(mdp.states)),
	#each column contains the index of the state reached from the state represented by the column index while taking the action represented by the row index.  the values are initialized to produce transitions to the original state
	state_transition_map = mapreduce(i_s -> i_s .* ones(Int64, length(mdp.actions)), hcat, eachindex(mdp.states)),
	#each column contains the reward received from the state represented by the column index while taking the action represented by the row index.  the state_transition_map can be used to determine if any of these values have been updated from the zero initialization
	reward_transition_map = zeros(T, length(mdp.actions), length(mdp.states)),
	κ = 0.1f0, 
	init_step = 0) where {S, A, F, G, H, T<:AbstractFloat}

	π = copy(πinit)
	Q = copy(Qinit)
	ϵ = ϵinit
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q[:, terminds] .= zero(T)
	
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards per step and steps per episode
	rewards = zeros(T, max_steps)
	steps = zeros(Int64, num_episodes)

	function updateQ!(Q, i_s, i_a, i_s′, r)
		qmax = maximum(Q[i, i_s′] for i in eachindex(mdp.actions))
		Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
	end

	function q_planning!(Q, history, step)
		for _ in 1:n
			i_s = ceil(Int64, rand()*length(mdp.states))
			i_a = ceil(Int64, rand()*length(mdp.actions))
			i_s′ = state_transition_map[i_a, i_s]
			τ = step - history[i_a, i_s]
			r = reward_transition_map[i_a, i_s] + κ*sqrt(τ)
			updateQ!(Q, i_s, i_a, i_s′, r)
		end
	end

	step = 1
	ep = 1
	while (ep <= num_episodes) && (step <= max_steps)
		s = mdp.state_init()
		l = 0
		
		while !mdp.isterm(s) && step <= max_steps
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			updateQ!(Q, i_s, i_a, i_s′, r)
			state_transition_map[i_a, i_s] = i_s′
			reward_transition_map[i_a, i_s] = r
			
			#save the step for which that state action pair was visited
			history[i_a, i_s] = step + init_step
			rewards[step] = r
			q_planning!(Q, history, step + init_step)
			step += 1
			π = create_ϵ_greedy_policy(Q, ϵ; π = π)
			s = s′
			l += 1
			
		end
		steps[ep] = l
		
		ep += 1
	end
	return Q, steps, rewards, history, state_transition_map, reward_transition_map
end

# ╔═╡ 269f4505-e807-446c-8fd8-3458482e00ab
function tabular_dynaQplus′(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; 
	num_episodes = 1000, max_steps = 10000, n = 100, qinit = zero(T), ϵinit = one(T)/10, 
	Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), 
	decay_ϵ = false, history_state::S = first(mdp.states), 
	save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), 
	history = zeros(Int64, length(mdp.actions), length(mdp.states)),
	#each column contains the index of the state reached from the state represented by the column index while taking the action represented by the row index.  the values are initialized to produce transitions to the original state
	state_transition_map = mapreduce(i_s -> i_s .* ones(Int64, length(mdp.actions)), hcat, eachindex(mdp.states)),
	#each column contains the reward received from the state represented by the column index while taking the action represented by the row index.  the state_transition_map can be used to determine if any of these values have been updated from the zero initialization
	reward_transition_map = zeros(T, length(mdp.actions), length(mdp.states)),
	κ = 0.1f0, 
	init_step = 0) where {S, A, F, G, H, T<:AbstractFloat}

	π = copy(πinit)
	Q = copy(Qinit)
	ϵ = ϵinit
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q[:, terminds] .= zero(T)
	
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards per step and steps per episode
	rewards = zeros(T, max_steps)
	steps = zeros(Int64, num_episodes)

	function updateQ!(Q, i_s, i_a, i_s′, r)
		qmax = maximum(Q[i, i_s′] for i in eachindex(mdp.actions))
		Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
	end

	function q_planning!(Q, history)
		for _ in 1:n
			i_s = ceil(Int64, rand()*length(mdp.states))
			i_a = ceil(Int64, rand()*length(mdp.actions))
			i_s′ = state_transition_map[i_a, i_s]
			r = reward_transition_map[i_a, i_s]
			updateQ!(Q, i_s, i_a, i_s′, r)
		end
	end

	step = 1
	ep = 1
	while (ep <= num_episodes) && (step <= max_steps)
		s = mdp.state_init()
		l = 0
		
		while !mdp.isterm(s) && step <= max_steps
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			updateQ!(Q, i_s, i_a, i_s′, r)
			state_transition_map[i_a, i_s] = i_s′
			reward_transition_map[i_a, i_s] = r
			
			#save the step for which that state action pair was visited
			history[i_a, i_s] = step + init_step
			rewards[step] = r
			q_planning!(Q, history)
			step += 1
			π .= Q .+ κ .* sqrt.(T.(step .+ init_step .- history))
			π = create_greedy_policy(π; π = π)
			s = s′
			l += 1
			
		end
		steps[ep] = l
		
		ep += 1
	end
	return Q, steps, rewards, history, state_transition_map, reward_transition_map
end

# ╔═╡ 898f1b06-a34f-496b-99db-9ca23498cbee
function prioritized_sweeping_deterministic(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; 
	num_episodes = 1000, max_steps = 10000, n = 100, qinit = zero(T), ϵinit = one(T)/10, 
	Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), 
	decay_ϵ = false, history_state::S = first(mdp.states), 
	save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), 
	history = zeros(Int64, length(mdp.actions), length(mdp.states)),
	#each column contains the index of the state reached from the state represented by the column index while taking the action represented by the row index.  the values are initialized at 0 which represents a transition that has not occured
	state_transition_map = zeros(Int64, length(mdp.actions), length(mdp.states)),
	#each column contains the reward received from the state represented by the column index while taking the action represented by the row index.  the state_transition_map can be used to determine if any of these values have been updated from the zero initialization
	reward_transition_map = zeros(T, length(mdp.actions), length(mdp.states)),
	θ = one(T)/100,
	init_step = 0,
	isoptimal = l -> false) where {S, A, F, G, H, T<:AbstractFloat}

	π = copy(πinit)
	Q = copy(Qinit)
	ϵ = ϵinit
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q[:, terminds] .= zero(T)
	num_updates = 0
	
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards per step and steps per episode
	rewards = zeros(T, max_steps)
	steps = zeros(Int64, num_episodes)

	#maintains a queue of state action pairs ranked by their priority with the largest value first
	priority_queue = PriorityQueue{Tuple{Int64, Int64}, T}(Base.Order.Reverse)

	function updateQ!(Q, i_s, i_a, i_s′, r)
		qmax = calculate_qmax(Q, i_s′)
		Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
		num_updates += 1
	end

	calculate_priority(r, qmax, i_s, i_a) = abs(r + γ*qmax - Q[i_a, i_s])
	calculate_qmax(Q, i_s) = maximum(Q[i, i_s] for i in eachindex(mdp.actions))

	function q_planning!(Q, queue)
		step = 1
		while (step <= n) && !isempty(queue)
			(i_a, i_s) = first(keys(queue))
			delete!(queue, (i_a, i_s))
			i_s′ = state_transition_map[i_a, i_s]
			r = reward_transition_map[i_a, i_s]
			updateQ!(Q, i_s, i_a, i_s′, r)
			#loop for all state action pairs that the model predicts will lead to s
			qmax = calculate_qmax(Q, i_s)
			for i_s̄ in eachindex(mdp.states)
				for i_ā in eachindex(mdp.actions)
					if state_transition_map[i_ā, i_s̄] == i_s
						r̄ = reward_transition_map[i_ā, i_s̄]
						p = calculate_priority(r̄, qmax, i_s̄, i_ā)
						if p > θ
							priority_queue[(i_ā, i_s̄)] = p
						end
					end
				end
			end
			step += 1
		end
	end

	step = 1
	ep = 1
	optimal_path = false
	while (ep <= num_episodes) && (step <= max_steps) && !optimal_path
		s = mdp.state_init()
		l = 0
		
		while !mdp.isterm(s) && step <= max_steps
			#take policy action based on Q
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)

			#replace model value with observed transition
			state_transition_map[i_a, i_s] = i_s′
			reward_transition_map[i_a, i_s] = r

			#calculate expected update from previous Q value
			p = calculate_priority(r, calculate_qmax(Q, i_s′), i_s, i_a)
			if p > θ
				priority_queue[(i_a, i_s)] = p
			end
			
			rewards[step] = r
			q_planning!(Q, priority_queue)
			step += 1
			s = s′
			
			#update policy for next state action selection
			vhold .= Q[:, i_s′]
			update_policy!(vhold, ϵ, s′)
			π[:, i_s′] .= vhold
			l += 1
		end
		steps[ep] = l
		optimal_path = isoptimal(l)
		
		ep += 1
	end
	return Q, steps, rewards, state_transition_map, reward_transition_map, num_updates
end

# ╔═╡ 0f37ec0a-b737-478b-bf6a-027899250c4e
#take a step in the environment from state s using policy π and generate the subsequent action selection as well
function sarsa_step(mdp::MDP_TD{S, A, F, G, H}, π::Matrix{T}, s::S, a::A) where {S, A, F<:Function, G<:Function, H<:Function, T<:Real}
	(r, s′) = mdp.step(s, a)
	i_s′ = mdp.statelookup[s′]
	i_a′ = sample_action(π, i_s′)
	a′ = mdp.actions[i_a′]
	return (s′, i_s′, r, a′, i_a′)
end

# ╔═╡ f424edac-388d-4465-900f-9459d2a88f79
#take a step in the environment from state s using policy π and generate the subsequent action selection as well
function init_step(mdp::MDP_TD{S, A, F, G, H}, π::Matrix{T}, s::S) where {S, A, F<:Function, G<:Function, H<:Function, T<:Real}
	i_s = mdp.statelookup[s]
	i_a = sample_action(π, i_s)
	a = mdp.actions[i_a]
	return (i_s, i_a, a)
end

# ╔═╡ 4494cb61-ee2c-467b-9bf6-0afb59023e91
function sarsa(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), history_state::S = first(mdp.states), update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), save_history = false, decay_ϵ = false) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q = copy(Qinit)
	Q[:, terminds] .= zero(T)
	π = copy(πinit)
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)

	if save_history
		action_history = Vector{A}(undef, num_episodes)
	end

	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		(i_s, i_a, a) = init_step(mdp, π, s)
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			(s′, i_s′, r, a′, i_a′) = sarsa_step(mdp, π, s, a)
			if save_history && (s == history_state)
				action_history[ep] = a
			end
			Q[i_a, i_s] += α * (r + γ*Q[i_a′, i_s′] - Q[i_a, i_s])
			
			#update terms for next step
			vhold .= Q[:, i_s]
			update_policy!(vhold, ϵ, s)
			π[:, i_s] .= vhold
			s = s′
			a = a′
			i_s = i_s′
			i_a = i_a′
			
			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	default_return =  Q, π, steps, rewards
	save_history && return (default_return..., action_history)
	return default_return
end

# ╔═╡ 143fff7d-0bb2-43b4-b810-53784fe848bd
function q_learning(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), decay_ϵ = false, history_state::S = first(mdp.states), save_history = false, update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ)) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q = copy(Qinit)
	Q[:, terminds] .= zero(T)
	π = copy(πinit)
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)
	
	if save_history
		history_actions = Vector{A}(undef, num_episodes)
	end
	
	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			if save_history && (s == history_state)
				history_actions[ep] = a
			end
			qmax = maximum(Q[i, i_s′] for i in eachindex(mdp.actions))
			Q[i_a, i_s] += α*(r + γ*qmax - Q[i_a, i_s])
			
			#update terms for next step
			vhold .= Q[:, i_s]
			update_policy!(vhold, ϵ, s)
			π[:, i_s] .= vhold
			s = s′
			
			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	save_history && return Q, π, steps, rewards, history_actions
	return Q, π, steps, rewards
end

# ╔═╡ 9be963b9-f3a1-4f92-8ff9-f5be75ed52f2
function expected_sarsa(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit = initialize_state_action_value(mdp; qinit=qinit), πinit = create_ϵ_greedy_policy(Qinit, ϵinit), update_policy! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), decay_ϵ = false, save_history = false, save_state = first(mdp.states)) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	Q = copy(Qinit)
	Q[:, terminds] .= zero(T)
	π = copy(πinit)
	vhold = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)
	if save_history
		action_history = Vector{A}(undef, num_episodes)
	end
	
	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π, s)
			if save_history && (s == save_state)
				action_history[ep] = a
			end
			q_expected = sum(π[i, i_s′]*Q[i, i_s′] for i in eachindex(mdp.actions))
			Q[i_a, i_s] += α*(r + γ*q_expected - Q[i_a, i_s])
			
			#update terms for next step
			vhold .= Q[:, i_s]
			update_policy!(vhold, ϵ, s)
			π[:, i_s] .= vhold
			s = s′
			
			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	base_return = (Q, π, steps, rewards)
	save_history && return (base_return..., action_history)
	return base_return
end

# ╔═╡ 113d2bc2-1f77-479f-86e5-a65b20672d7a
function double_expected_sarsa(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; num_episodes = 1000, qinit = zero(T), ϵinit = one(T)/10, Qinit::Matrix{T} = initialize_state_action_value(mdp; qinit=qinit), decay_ϵ = false, target_policy_function! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), behavior_policy_function! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), πinit_target::Matrix{T} = create_ϵ_greedy_policy(Qinit, ϵinit), πinit_behavior::Matrix{T} = create_ϵ_greedy_policy(Qinit, ϵinit), save_state::S = first(mdp.states), save_history = false) where {S, A, F, G, H, T<:AbstractFloat}
	terminds = findall(mdp.isterm(s) for s in mdp.states)
	
	Q1 = copy(Qinit)
	Q2 = copy(Qinit) 
	Q1[:, terminds] .= zero(T)
	Q2[:, terminds] .= zero(T)
	π_target1 = copy(πinit_target)
	π_target2 = copy(πinit_target)
	π_behavior = copy(πinit_behavior)
	vhold1 = zeros(T, length(mdp.actions))
	vhold2 = zeros(T, length(mdp.actions))
	vhold3 = zeros(T, length(mdp.actions))
	#keep track of rewards and steps per episode as a proxy for training speed
	rewards = zeros(T, num_episodes)
	steps = zeros(Int64, num_episodes)

	if save_history
		action_history = Vector{A}(undef, num_episodes)
	end
	
	for ep in 1:num_episodes
		ϵ = decay_ϵ ? ϵinit/ep : ϵinit
		s = mdp.state_init()
		rtot = zero(T)
		l = 0
		while !mdp.isterm(s)
			
			(i_s, i_s′, r, s′, a, i_a) = takestep(mdp, π_behavior, s)
			if save_history && (s == save_state)
				action_history[ep] = a
			end
			
			# q_expected = sum(π_target[i, i_s′]*(Q1[i, i_s′]*toggle + Q2[i, i_s′]*(1-toggle)) for i in eachindex(mdp.actions))
			toggle = rand() < 0.5
			q_expected = if toggle 
				sum(π_target2[i, i_s′]*Q1[i, i_s′] for i in eachindex(mdp.actions))
			else
				sum(π_target1[i, i_s′]*Q2[i, i_s′] for i in eachindex(mdp.actions))
			end

			if toggle
				Q2[i_a, i_s] += α*(r + γ*q_expected - Q2[i_a, i_s])
			else
				Q1[i_a, i_s] += α*(r + γ*q_expected - Q1[i_a, i_s])
			end
			
			#update terms for next step
			if toggle
				vhold2 .= Q2[:, i_s]
				target_policy_function!(vhold2, ϵ, s)
				π_target2[:, i_s] .= vhold2
			else
				vhold1 .= Q1[:, i_s]
				target_policy_function!(vhold1, ϵ, s)
				π_target1[:, i_s] .= vhold1
			end
			
			vhold3 .= vhold1 .+ vhold2
			behavior_policy_function!(vhold3, ϵ, s)
			π_behavior[:, i_s] .= vhold3
			
			s = s′

			l+=1
			rtot += r
		end
		steps[ep] = l
		rewards[ep] = rtot
	end

	Q1 .+= Q2
	Q1 ./= 2
	plain_return = Q1, create_greedy_policy(Q1), steps, rewards

	save_history && return (plain_return..., action_history)
	return plain_return
end

# ╔═╡ 5d2abde0-7128-41c3-bd1f-b6940492d1ae
function double_q_learning(mdp::MDP_TD{S, A, F, G, H}, α::T, γ::T; 
	num_episodes = 1000, 
	qinit = zero(T), 
	ϵinit = one(T)/10, 
	Qinit::Matrix{T} = initialize_state_action_value(mdp; qinit=qinit), 
	decay_ϵ = false, 
	target_policy_function! = (v, ϵ, s) -> make_greedy_policy!(v), 
	behavior_policy_function! = (v, ϵ, s) -> make_ϵ_greedy_policy!(v, ϵ), 
	πinit_target::Matrix{T} = create_greedy_policy(Qinit), 
	πinit_behavior::Matrix{T} = create_ϵ_greedy_policy(Qinit, ϵinit), 
	save_state::S = first(mdp.states), 
	save_history = false) where {S, A, F, G, H, T<:AbstractFloat} 
	
	double_expected_sarsa(mdp, α, γ; num_episodes = num_episodes, qinit = qinit, ϵinit = ϵinit, Qinit = Qinit, decay_ϵ = decay_ϵ, target_policy_function! = target_policy_function!, behavior_policy_function! = behavior_policy_function!, πinit_target = πinit_target, πinit_behavior = πinit_behavior, save_state = save_state, save_history = save_history)
end

# ╔═╡ 4e1c115a-4020-4a5a-a79a-56056892a953
md"""
## Gridworld Environment
"""

# ╔═╡ 729197ce-2c27-467d-ba5f-47a1ecd539f2
begin
	abstract type GridworldAction end
	struct Up <: GridworldAction end
	struct Down <: GridworldAction end
	struct Left <: GridworldAction end
	struct Right <: GridworldAction end
	struct UpRight <: GridworldAction end
	struct DownRight <: GridworldAction end
	struct UpLeft <: GridworldAction end
	struct DownLeft <: GridworldAction end
	struct Stay <: GridworldAction end
	
	struct GridworldState
		x::Int64
		y::Int64
	end

	rook_actions = [Up(), Down(), Left(), Right()]
	
	move(::Up, x, y) = (x, y+1)
	move(::Down, x, y) = (x, y-1)
	move(::Left, x, y) = (x-1, y)
	move(::Right, x, y) = (x+1, y)
	move(::UpRight, x, y) = (x+1, y+1)
	move(::UpLeft, x, y) = (x-1, y+1)
	move(::DownRight, x, y) = (x+1, y-1)
	move(::DownLeft, x, y) = (x-1, y-1)
	move(::Stay, x, y) = (x, y)
	apply_wind(w, x, y) = (x, y+w)
	const wind_vals = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]
end

# ╔═╡ bb439641-30bd-495d-ba70-06b2e27efdbd
function make_gridworld(;actions = rook_actions, sterm = GridworldState(8, 4), start = GridworldState(1, 4), xmax = 10, ymax = 7, stepreward = 0.0f0, termreward = 1.0f0, iscliff = s -> false, iswall = s -> false, cliffreward = -100f0, goal2 = GridworldState(start.x, ymax), goal2reward = 0.0f0, usegoal2 = false)
	
	states = [GridworldState(x, y) for x in 1:xmax for y in 1:ymax]
	
	boundstate(x::Int64, y::Int64) = (clamp(x, 1, xmax), clamp(y, 1, ymax))
	
	function step(s::GridworldState, a::GridworldAction)
		(x, y) = move(a, s.x, s.y)
		s′ = GridworldState(boundstate(x, y)...)
		iswall(s′) && return s
		return s′
	end
		
	function isterm(s::GridworldState) 
		s == sterm && return true
		usegoal2 && (s == goal2) && return true
		return false
	end

	function tr(s::GridworldState, a::GridworldAction) 
		isterm(s) && return (0f0, s)
		s′ = step(s, a)
		iscliff(s′) && return (cliffreward, start)
		x = Float32(isterm(s′))
		usegoal2 && (s′ == goal2) && return (goal2reward, goal2)
		r = (1f0 - x)*stepreward + x*termreward
		(r, s′)
	end	
	MDP_TD(states, actions, () -> start, tr, isterm)
end	

# ╔═╡ c04c803c-cdca-4b8b-9c9d-e456ee677906
begin
	const maze_walls = Set(GridworldState(x, y) for (x, y) in [(3, 3), (3, 4), (3, 5), (6, 2), (8, 4), (8, 5), (8, 6)])
	const dyna_maze = make_gridworld(;xmax = 9, ymax = 6, sterm = GridworldState(9, 6), iswall = s -> in(s, maze_walls))
end

# ╔═╡ cd139745-1877-43a2-97a0-3333e544cbd8
function figure8_2(;num_episodes = 50, α = 0.1f0, nlist = [0, 5, 50], γ = 0.95f0, samples = 5)
	traces = [begin
		step_avg = zeros(Int64, num_episodes)
		for s in 1:samples
			Random.seed!(s)
			(Q, steps, rewards, _, _) = tabular_dynaQ(dyna_maze, α, γ; num_episodes = num_episodes, n = n)
			step_avg .+= steps
		end
		scatter(x = 2:num_episodes, y = step_avg[2:end] ./ samples, name = "$n planning steps")
	end
	for n in nlist]
	plot(traces, Layout(xaxis_title = "Episodes", yaxis_title = "Steps per episode"))
end

# ╔═╡ 8dbc76fd-ac73-47ca-983e-0e90023390e3
figure8_2()

# ╔═╡ beae0491-ed11-4edf-a136-d384578b088b
monte_carlo_tree_search(dyna_maze, 0.95f0, rollout(;max_steps = 10_000),  dyna_maze.state_init(); nsims = 100, depth = 10, c = 1f0)

# ╔═╡ 41bb1f78-b83a-4a45-ba5c-faa94e112f45
function get_mcts_statistics()
	# visit_counts = Dict{GridworldState, Vector{Int64}}()
	# Q = Dict{GridworldState, Vector{Float32}}()
	mean(runepisode(dyna_maze, s -> monte_carlo_tree_search(dyna_maze, 0.95f0, rollout(;max_steps = 10),  s; nsims = 100, depth = 100, c = 1f0)[1]) |> first |> length for _ in 1:10)
end

# ╔═╡ 39ae7727-ea72-48f3-8d63-e03a9f607f87
function make_dyna_maze(scale)
	scale_value(v) = ceil(Int64, v*scale)
	xmax = scale_value(9)
	ymax = scale_value(6)
	sterm = GridworldState(xmax, ymax)
	wall1 = Set(GridworldState(scale_value(3), y) for y in scale_value(3):scale_value(5))
	wall2 = Set([GridworldState(scale_value(6), scale_value(3) - 1)])
	wall3 = Set(GridworldState(xmax - 1, y) for y in scale_value(4):ymax)
	walls = union(wall1, wall2, wall3)
	iswall(s) = in(s, walls)
	start = GridworldState(1, scale_value(4))
	optimal_length = xmax - 1 + (ymax - scale_value(3) + 1) + start.y - scale_value(3) + 1
	(maze = make_gridworld(;start = start, xmax=xmax, ymax=ymax, sterm = sterm, iswall = iswall), iswall = iswall, optimal_length = optimal_length)
end

# ╔═╡ fb00aedd-e103-4463-b4f8-d0dce6275c64
function example_8_4(;α = 1f0, n = 5, γ = 0.95f0, samples = 2, maze_scales = [1, 1.25, 1.5, 2])
	function get_optimal_steps(algo, mdp, optimal_steps)
		updates_until_optimal = 0
		for s in 1:samples
			Random.seed!(s)
			out = algo(mdp, α, γ; n = n, ϵinit = 0.1f0, isoptimal = l -> l == optimal_steps)
			updates_until_optimal += last(out)
		end
		return updates_until_optimal / samples
	end

	mazes = [make_dyna_maze(s) for s in maze_scales]

	function make_trace(algo, name)
		y = mazes |> Map(maze -> get_optimal_steps(algo, maze.maze, maze.optimal_length)) |> tcollect
		x = [length(maze.maze.states) for maze in mazes]
		scatter(x = x, y = y, name = name)
	end
	
	traces = [make_trace(prioritized_sweeping_deterministic, "prioritized sweeping"), make_trace(tabular_dynaQ, "Dyna-Q")]
	plot(traces, Layout(yaxis_type = "log", xaxis_type = "log", xaxis_title = "Maze States", yaxis_title = "Updates until optimal solution"))
end

# ╔═╡ b3a5adcb-5343-44e9-9466-1c51c1143a0d
example_8_4()

# ╔═╡ 77fde69f-2119-41eb-8993-a93b2c47ca7e
md"""
## Gridworld Visualization
"""

# ╔═╡ eded8b72-70f4-4579-ba69-2eca409fa684
function plot_path(mdp, states::Vector, sterm; title = "Optimal policy <br> path example", iscliff = s -> false, iswall = s -> false)
	xmax = maximum([s.x for s in mdp.states])
	ymax = maximum([s.y for s in mdp.states])
	start = mdp.state_init()
	goal = mdp.states[findlast(mdp.isterm(s) for s in mdp.states)]
	start_trace = scatter(x = [start.x + 0.5], y = [start.y + 0.5], mode = "text", text = ["S"], textposition = "left", showlegend=false)
	finish_trace = scatter(x = [goal.x + .5], y = [goal.y + .5], mode = "text", text = ["G"], textposition = "left", showlegend=false)
	
	path_traces = [scatter(x = [states[i].x + 0.5, states[i+1].x + 0.5], y = [states[i].y + 0.5, states[i+1].y + 0.5], line_color = "blue", mode = "lines", showlegend=false, name = "Optimal Path") for i in 1:length(states)-1]
	finalpath = scatter(x = [states[end].x + 0.5, sterm.x + .5], y = [states[end].y + 0.5, sterm.y + 0.5], line_color = "blue", mode = "lines", showlegend=false, name = "Optimal Path")

	h1 = 30*ymax
	traces = [start_trace; finish_trace; path_traces; finalpath]

	cliff_squares = filter(iscliff, mdp.states)
	for s in cliff_squares
		push!(traces, scatter(x = [s.x + 0.6], y = [s.y+0.5], mode = "text", text = ["C"], textposition = "left", showlegend = false))
	end


	wall_squares = filter(iswall, mdp.states)
	for s in wall_squares
		push!(traces, scatter(x = [s.x + 0.8], y = [s.y+0.5], mode = "text", text = ["W"], textposition = "left", showlegend = false))
	end

	plot(traces, Layout(xaxis = attr(showgrid = true, showline = true, gridwith = 1, gridcolor = "black", zeroline = true, linecolor = "black", mirror=true, tickvals = 1:xmax, ticktext = fill("", 10), range = [1, xmax+1]), yaxis = attr(linecolor="black", mirror = true, gridcolor = "black", showgrid = true, gridwidth = 1, showline = true, tickvals = 1:ymax, ticktext = fill("", ymax), range = [1, ymax+1]), width = max(30*xmax, 200), height = max(h1, 200), autosize = false, padding=0, paper_bgcolor = "rgba(0, 0, 0, 0)", title = attr(text = title, font_size = 14, x = 0.5)))
end

# ╔═╡ 1f7d77a6-d774-436d-a745-5a160cc15f2b
function plot_path(mdp, π; max_steps = 100, kwargs...)
	(states, actions, rewards, sterm) = runepisode(mdp, π; max_steps = max_steps)
	plot_path(mdp, states, sterm; kwargs...)
end

# ╔═╡ 502a7125-4460-4d39-be14-4852fb6d9ad2
plot_path(mdp; title = "Random policy <br> path example", kwargs...) = plot_path(mdp, make_random_policy(mdp); title = title, kwargs...)

# ╔═╡ 5ae2d740-13c7-4568-8f04-25bc82fecbdb
begin
	const block1 = Set(GridworldState(x, 3) for x in 1:8)
	const block2 = Set(GridworldState(x, 3) for x in 2:9)
	const block_maze_base_args = (xmax = 9, ymax = 6, sterm = GridworldState(9, 6), start = GridworldState(4, 1))
	const blocking_maze1 = make_gridworld(;iswall = s -> in(s, block1), block_maze_base_args...)
	const blocking_maze2 = make_gridworld(;iswall = s -> in(s, block2), block_maze_base_args...)
	[plot_path(blocking_maze1, iswall = s -> in(s, block1), max_steps = 2000, title = "Blocking Maze Steps <= 1000") plot_path(blocking_maze2, iswall = s -> in(s, block2), max_steps = 1000, title = "Blocking Maze Steps > 1000")]
end

# ╔═╡ 2ff6d187-e06f-47b5-9834-d06bfc820c26
function figure_8_4(; max_steps1 = 1000, max_steps2 = 2000, ntrials = 10, n = 10, α = 0.1f0, ϵ = 0.1f0)
	y = zeros(max_steps1 + max_steps2)
	x = 1:(max_steps1 + max_steps2)
	for _ in 1:ntrials
		(Q, steps, rewards, history, stm, rtm) = tabular_dynaQ(blocking_maze1, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps1, n = n)
		(Q2, steps2, rewards2, _, _) = tabular_dynaQ(blocking_maze2, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps2, n = n, Qinit = Q, history = history, state_transition_map = stm, reward_transition_map = rtm)
		y .+= cumsum([rewards; rewards2])
	end
	plot(scatter(x = x, y = y ./ ntrials))
end

# ╔═╡ 4d1f0065-e5ab-46fa-8ab1-a0bbcf523c27
figure_8_4(;ntrials = 10, ϵ = 0.1f0, α = 0.8f0, fig_8_4_params...)

# ╔═╡ 5b688057-06c7-4ae4-95d6-0a2ff451f11c
function figure_8_4′(; max_steps1 = 1000, max_steps2 = 2000, ntrials = 10, n = 10, α = 0.1f0, ϵ = 0.1f0, κ = 0.01f0)
	x = 1:(max_steps1 + max_steps2)
	function test_algo(f)
		y = zeros(max_steps1 + max_steps2)
		for _ in 1:ntrials
			(Q, steps, rewards, history, stm, rtm) = f(blocking_maze1, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps1, n = n)
			(Q2, steps2, rewards2, _, _) = f(blocking_maze2, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps2, n = n, Qinit = Q, history = history, state_transition_map = stm, reward_transition_map = rtm, init_step = max_steps1)

			y .+= cumsum([rewards; rewards2])
		end
		return x, y
	end

	(x1, y1) = test_algo(tabular_dynaQ)
	(x2, y2) = test_algo((args...; kwargs...) -> tabular_dynaQplus(args...; κ = κ, kwargs...))
	
	plot([scatter(x = x, y = y1 ./ ntrials, name = "Dyna-Q"), scatter(x = x, y = y2 ./ ntrials, name = "Dyna-Q+")])
end

# ╔═╡ 0dbd2b87-d000-408b-8d04-25fc0fa512d1
figure_8_4′(;ntrials = 10, ϵ = 0.2f0, α = 1.0f0, κ = 0.0001f0, n = 50)

# ╔═╡ cd1980ff-2f35-4599-b08c-2037ddf5e995
function figure_8_4′′(; max_steps1 = 1000, max_steps2 = 2000, ntrials = 10, n = 10, α = 0.1f0, ϵ = 0.1f0, κ = 0.01f0)
	x = 1:(max_steps1 + max_steps2)
	function test_algo(f)
		y = zeros(max_steps1 + max_steps2)
		for _ in 1:ntrials
			(Q, steps, rewards, history, stm, rtm) = f(blocking_maze1, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps1, n = n)
			(Q2, steps2, rewards2, _, _) = f(blocking_maze2, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps2, n = n, Qinit = Q, history = history, state_transition_map = stm, reward_transition_map = rtm, init_step = max_steps1)

			y .+= cumsum([rewards; rewards2])
		end
		return x, y
	end

	(x1, y1) = test_algo((args...; kwargs...) -> tabular_dynaQplus(args...; κ = κ, kwargs...))
	(x2, y2) = test_algo((args...; kwargs...) -> tabular_dynaQplus′(args...; κ = κ, kwargs...))
	
	plot([scatter(x = x, y = y1 ./ ntrials, name = "Dyna-Q+"), scatter(x = x, y = y2 ./ ntrials, name = "Dyna-Q+′")])
end

# ╔═╡ c31f4646-aa8a-41e3-9c68-ae8a349d4ed1
figure_8_4′′(;ntrials = 10, ϵ = 0.1f0, α = 1.0f0, κ = 0.0001f0, n = 50, max_steps1 = 1000, max_steps2 = 2000)

# ╔═╡ db66615e-fbbc-4ea8-b529-bdc14e58a215
begin
	const block3 = Set(GridworldState(x, 3) for x in 2:8)
	const blocking_maze3 = make_gridworld(;iswall = s -> in(s, block3), block_maze_base_args...)
	[plot_path(blocking_maze2, iswall = s -> in(s, block2), max_steps = 2000, title = "Shortcut Maze Steps <= 3000") plot_path(blocking_maze3, iswall = s -> in(s, block3), max_steps = 2000, title = "Shortcut Maze Steps > 3000")]
end

# ╔═╡ 562e824c-34b5-415d-b186-d8e2cf1980e7
function figure_8_5(; max_steps = 3000, ntrials = 10, n = 10, α = 0.1f0, ϵ = 0.1f0)
	x = 1:2*max_steps
	y = zeros(2*max_steps)
	for _ in 1:ntrials
		(Q, steps, rewards, history, stm, rtm) = tabular_dynaQ(blocking_maze2, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps, n = n)
		(Q2, steps2, rewards2, _, _) = tabular_dynaQ(blocking_maze3, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps, n = n, Qinit = Q, history = history, state_transition_map = stm, reward_transition_map = rtm)
	
		y .+= cumsum([rewards; rewards2])
	end
	plot(scatter(x = x, y = y ./ ntrials))
end

# ╔═╡ 98547223-05a6-43da-80b2-63c67d2de283
figure_8_5(;ntrials = 10, n = 10)

# ╔═╡ aee1f6a8-de43-402e-b375-86c0f2f9e6b8
function figure_8_5′(; max_steps1 = 3000, max_steps2 = 3000, ntrials = 10, n = 50, α = 0.5f0, ϵ = 0.2f0, κ = 0.001f0)
	x = 1:(max_steps1 + max_steps2)
	function test_algo(f)
		y = zeros(max_steps1 + max_steps2)
		for _ in 1:ntrials
			(Q, steps, rewards, history, stm, rtm) = f(blocking_maze2, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps1, n = n)
			(Q2, steps2, rewards2, _, _) = f(blocking_maze3, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps2, n = n, Qinit = Q, history = history, state_transition_map = stm, reward_transition_map = rtm, init_step = max_steps1)

			y .+= cumsum([rewards; rewards2])
		end
		return x, y
	end

	(x1, y1) = test_algo(tabular_dynaQ)
	(x2, y2) = test_algo((args...; kwargs...) -> tabular_dynaQplus(args...; κ = κ, kwargs...))
	
	plot([scatter(x = x, y = y1 ./ ntrials, name = "Dyna-Q"), scatter(x = x, y = y2 ./ ntrials, name = "Dyna-Q+")])
end

# ╔═╡ f0e88db8-e3ee-4b74-923e-c34038024824
figure_8_5′()

# ╔═╡ c32d0943-ba8f-438f-83b6-5ed42221f630
function figure_8_5′′(; max_steps1 = 3000, max_steps2 = 3000, ntrials = 10, n = 50, α = 0.5f0, ϵ = 0.2f0, κ = 0.001f0)
	x = 1:(max_steps1 + max_steps2)
	function test_algo(f)
		y = zeros(max_steps1 + max_steps2)
		for _ in 1:ntrials
			(Q, steps, rewards, history, stm, rtm) = f(blocking_maze2, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps1, n = n)
			(Q2, steps2, rewards2, _, _) = f(blocking_maze3, α, 0.95f0; ϵinit = ϵ, max_steps = max_steps2, n = n, Qinit = Q, history = history, state_transition_map = stm, reward_transition_map = rtm, init_step = max_steps1)

			y .+= cumsum([rewards; rewards2])
		end
		return x, y
	end

	(x1, y1) = test_algo((args...; kwargs...) -> tabular_dynaQplus(args...; κ = κ, kwargs...))
	(x2, y2) = test_algo((args...; kwargs...) -> tabular_dynaQplus′(args...; κ = κ, kwargs...))
	
	plot([scatter(x = x, y = y1 ./ ntrials, name = "Dyna-Q+"), scatter(x = x, y = y2 ./ ntrials, name = "Dyna-Q+′")])
end

# ╔═╡ 1aa76f3d-6041-4886-a6cd-787bdf1ec63c
figure_8_5′′(;ϵ = 0.01f0)

# ╔═╡ 4b424a47-dfeb-4380-8ab8-8bd24a080c2e
begin
	newmaze = make_dyna_maze(2)
	newmaze2 = make_dyna_maze(3)
	[plot_path(dyna_maze; iswall = s -> in(s, maze_walls)) plot_path(newmaze.maze; iswall = newmaze.iswall) plot_path(newmaze2.maze; iswall = newmaze2.iswall)]
end

# ╔═╡ 2c587d5b-7b62-4835-ad02-9575c13d5874
function show_mcts_solution()
	# visit_counts = Dict{GridworldState, SparseVector{Float32, Int64}}()
	visit_counts = Dict{GridworldState, Dict{Int64, Float32}}()
	# Q = Dict{GridworldState, SparseVector{Float32, Int64}}()
	Q = Dict{GridworldState, Dict{Int64, Float32}}()
	plot_path(dyna_maze, s -> monte_carlo_tree_search(dyna_maze, 0.95f0, rollout(;max_steps = 100),  s; nsims = 100, depth = 100, c = 1.0f0, visit_counts = visit_counts, Q = Q)[1]; max_steps = 1000, iswall = s -> in(s, maze_walls))
end

# ╔═╡ 6a4116c9-87cf-4ee7-8030-aa1150853984
show_mcts_solution()

# ╔═╡ 4b3604db-0c1b-4770-95b1-5f5bb34d071b
function addelements(e1, e2)
	@htl("""
	$e1
	$e2
	""")
end

# ╔═╡ 39c96fc8-8259-46e3-88a0-a14eb6752b5c
function show_grid_value(mdp, Q, name; scale = 1.0, title = "", sigdigits = 2)
	width = maximum(s.x for s in mdp.states)
	height = maximum(s.y for s in mdp.states)
	start = mdp.state_init()
	termind = findfirst(mdp.isterm, mdp.states)
	sterm = mdp.states[termind]
	ngrid = width*height

	displayvalue(Q::Matrix, i) = round(maximum(Q[:, i]), sigdigits = sigdigits)
	displayvalue(V::Vector, i) = round(V[i], sigdigits = sigdigits)
	@htl("""
		<div style = "display: flex; transform: scale($scale); background-color: white; color: black; font-size: 16px;">
			<div>
				$title
				<div class = "gridworld $name value">
					$(HTML(mapreduce(i -> """<div class = "gridcell $name value" x = "$(mdp.states[i].x)" y = "$(mdp.states[i].y)" style = "grid-row: $(height - mdp.states[i].y + 1); grid-column: $(mdp.states[i].x); font-size: 12px; color: black; $(displayvalue(Q, i) != 0 ? "background-color: lightblue;" : "")">$(displayvalue(Q, i))</div>""", *, eachindex(mdp.states))))
				</div>
			</div>
		</div>
	
		<style>
			.$name.value.gridworld {
				display: grid;
				grid-template-columns: repeat($width, 20px);
				grid-template-rows: repeat($height, 20px);
				background-color: white;
			}

			.$name.value[x="$(start.x)"][y="$(start.y)"] {
				content: '';
				background-color: rgba(0, 255, 0, 0.5);
				
			}

			.$name.value[x="$(sterm.x)"][y="$(sterm.y)"] {
				content: '';
				background-color: rgba(255, 0, 0, 0.5);
				
			}

		</style>
	""")
end

# ╔═╡ 3bf9e526-826d-42b8-84ee-75f1c7f79c69
function display_rook_policy(v::Vector{T}; scale = 1.0) where T<:AbstractFloat
	@htl("""
		<div style = "display: flex; align-items: center; justify-content: center; transform: scale($scale);">
		<div class = "downarrow" style = "position: absolute; transform: rotate(180deg); opacity: $(v[1]);"></div>	
		<div class = "downarrow" style = "position: absolute; opacity: $(v[2])"></div>
		<div class = "downarrow" style = "position: absolute; transform: rotate(90deg); opacity: $(v[3])"></div>
		<div class = "downarrow" style = "transform: rotate(-90deg); opacity: $(v[4])"></div>
		</div>
	""")
end

# ╔═╡ 9d69687a-8df6-4e74-aa99-fbfcc84bcccf
const rook_action_display = @htl("""
<div style = "display: flex; flex-direction: column; align-items: center; justify-content: center; color: black; background-color: rgba(100, 100, 100, 0.1);">
	<div style = "display: flex; align-items: center; justify-content: center;">
	<div class = "downarrow" style = "transform: rotate(90deg);"></div>
	<div class = "downarrow" style = "position: absolute; transform: rotate(180deg);"></div>
	<div class = "downarrow" style = "position: absolute; transform: rotate(270deg);"></div>
	<div class = "downarrow" style = "position: absolute;"></div>
	</div>
	<div>Actions</div>
</div>
""")

# ╔═╡ 563b6dbd-ce51-4904-b1cc-d766bd1fd1d6
@htl("""
<div style = "background-color: white; color: black; display: flex; align-items: center; justify-content: center;">
<div>$(plot_path(dyna_maze; title = "Random policy path example in Dyna Maze", max_steps = 10000, iswall = s -> in(s, maze_walls)))</div>
<div>$rook_action_display</div>
</div>
""")

# ╔═╡ f5e52b2f-ea14-423d-8ca9-2ed68cd27c69
function show_grid_policy(mdp, π, name; display_function = display_rook_policy, action_display = rook_action_display, scale = 1.0)
	width = maximum(s.x for s in mdp.states)
	height = maximum(s.y for s in mdp.states)
	start = mdp.state_init()
	termind = findfirst(mdp.isterm, mdp.states)
	sterm = mdp.states[termind]
	ngrid = width*height
	@htl("""
		<div style = "display: flex; transform: scale($scale); background-color: white;">
			<div>
				<div class = "gridworld $name">
					$(HTML(mapreduce(i -> """<div class = "gridcell $name" x = "$(mdp.states[i].x)" y = "$(mdp.states[i].y)" style = "grid-row: $(height - mdp.states[i].y + 1); grid-column: $(mdp.states[i].x);">$(display_function(π[:, i], scale =0.8))</div>""", *, eachindex(mdp.states))))
				</div>
			</div>
			<div style = "display: flex; flex-direction: column; align-items: flex-start; justify-content: flex-end; color: black; font-size: 18px; width: 5em; margin-left: 1em;">
				$(action_display)
			</div>
		</div>
	
		<style>
			.$name.gridworld {
				display: grid;
				grid-template-columns: repeat($width, 40px);
				grid-template-rows: repeat($height, 40px);
				background-color: white;

			.$name[x="$(start.x)"][y="$(start.y)"]::before {
				content: 'S';
				position: absolute;
				color: green;
				opacity: 1.0;
			}

			.$name[x="$(sterm.x)"][y="$(sterm.y)"]::before {
				content: 'G';
				position: absolute;
				color: red;
				opacity: 1.0;
			}

		</style>
	""")
end

# ╔═╡ 0d0bbf62-b1ac-45f6-8a92-1e77b0709cb3
function figure8_3(num_episodes; α = 0.1f0, nlist = [0, 50], γ = 0.95f0,)
	(Q1, _) = tabular_dynaQ(dyna_maze, α, γ; num_episodes = num_episodes, n = first(nlist))
	d1 = show_grid_policy(dyna_maze, create_greedy_policy(Q1), "test")
	(Q2, _) = tabular_dynaQ(dyna_maze, α, γ; num_episodes = num_episodes, n = last(nlist))
	d2 = show_grid_policy(dyna_maze, create_greedy_policy(Q2), "test")
	@htl("""
	<div style = "display: flex;">
	<div>Without planning (n = $(first(nlist)))$d1</div>
	<div>With planning (n = $(last(nlist)))$d2</div>
	</div>
	""")
end

# ╔═╡ 4d4baa61-b5bd-4bcf-a491-9a35a1695f0b
figure8_3(num_episodes_8_3)

# ╔═╡ 0f9080af-f166-4a78-a003-8df07f6c27d4
HTML("""
<style>
	.downarrow {
		display: flex;
		justify-content: center;
		align-items: center;
		flex-direction: column;
	}

	.downarrow::before {
		content: '';
		width: 2px;
		height: 40px;
		background-color: black;
	}
	.downarrow::after {
		content: '';
		width: 0px;
		height: 0px;
		border-left: 5px solid transparent;
		border-right: 5px solid transparent;
		border-top: 10px solid black;
	}

	.gridcell {
			display: flex;
			justify-content: center;
			align-items: center;
			border: 1px solid black;
		}

	.windbox {
		height: 40px;
		width: 40px;
		display: flex;
		justify-content: center;
		align-items: center;
		transform: rotate(180deg);
		background-color: green;
	}

	.windbox * {
		background-color: green;
		color: green;
	}

	.windbox[w="0"] {
		opacity: 0.0; 
	}

	.windbox[w="1"] {
		opacity: 0.5;
	}

	.windbox[w="2"] {
		opacity: 1.0;
	}
</style>
""")

# ╔═╡ 00000000-0000-0000-0000-000000000001
PLUTO_PROJECT_TOML_CONTENTS = """
[deps]
Arrow = "69666777-d1a9-59fb-9406-91d4454c9d45"
DataStructures = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
HypertextLiteral = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
PlutoPlotly = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
PlutoUI = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"
Serialization = "9e88b42a-f829-5b0c-bbe9-9e923198166b"
StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
Statistics = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
Transducers = "28d57a85-8fef-5791-bfe6-a80928e7c999"

[compat]
Arrow = "~2.7.2"
DataStructures = "~0.18.18"
HypertextLiteral = "~0.9.5"
PlutoPlotly = "~0.4.6"
PlutoUI = "~0.7.58"
StaticArrays = "~1.9.3"
StatsBase = "~0.34.2"
Transducers = "~0.4.81"
"""

# ╔═╡ 00000000-0000-0000-0000-000000000002
PLUTO_MANIFEST_TOML_CONTENTS = """
# This file is machine-generated - editing it directly is not advised

julia_version = "1.10.4"
manifest_format = "2.0"
project_hash = "7cebda43311e921b6e96d45ff788e4f65068ae13"

[[deps.AbstractPlutoDingetjes]]
deps = ["Pkg"]
git-tree-sha1 = "0f748c81756f2e5e6854298f11ad8b2dfae6911a"
uuid = "6e696c72-6542-2067-7265-42206c756150"
version = "1.3.0"

[[deps.Accessors]]
deps = ["CompositionsBase", "ConstructionBase", "Dates", "InverseFunctions", "LinearAlgebra", "MacroTools", "Markdown", "Test"]
git-tree-sha1 = "c0d491ef0b135fd7d63cbc6404286bc633329425"
uuid = "7d9f7c33-5ae7-4f3b-8dc6-eff91059b697"
version = "0.1.36"

    [deps.Accessors.extensions]
    AccessorsAxisKeysExt = "AxisKeys"
    AccessorsIntervalSetsExt = "IntervalSets"
    AccessorsStaticArraysExt = "StaticArrays"
    AccessorsStructArraysExt = "StructArrays"
    AccessorsUnitfulExt = "Unitful"

    [deps.Accessors.weakdeps]
    AxisKeys = "94b1ba4f-4ee9-5380-92f1-94cde586c3c5"
    IntervalSets = "8197267c-284f-5f27-9208-e0e47529a953"
    Requires = "ae029012-a4dd-5104-9daa-d747884805df"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
    StructArrays = "09ab397b-f2b6-538f-b94a-2f83cf4a842a"
    Unitful = "1986cc42-f94f-5a68-af5c-568840ba703d"

[[deps.Adapt]]
deps = ["LinearAlgebra", "Requires"]
git-tree-sha1 = "6a55b747d1812e699320963ffde36f1ebdda4099"
uuid = "79e6a3ab-5dfb-504d-930d-738a2a938a0e"
version = "4.0.4"
weakdeps = ["StaticArrays"]

    [deps.Adapt.extensions]
    AdaptStaticArraysExt = "StaticArrays"

[[deps.ArgCheck]]
git-tree-sha1 = "a3a402a35a2f7e0b87828ccabbd5ebfbebe356b4"
uuid = "dce04be8-c92d-5529-be00-80e4d2c0e197"
version = "2.3.0"

[[deps.ArgTools]]
uuid = "0dad84c5-d112-42e6-8d28-ef12dabb789f"
version = "1.1.1"

[[deps.Arrow]]
deps = ["ArrowTypes", "BitIntegers", "CodecLz4", "CodecZstd", "ConcurrentUtilities", "DataAPI", "Dates", "EnumX", "LoggingExtras", "Mmap", "PooledArrays", "SentinelArrays", "Tables", "TimeZones", "TranscodingStreams", "UUIDs"]
git-tree-sha1 = "f8d411d1b45459368567dc51f683ed78a919d795"
uuid = "69666777-d1a9-59fb-9406-91d4454c9d45"
version = "2.7.2"

[[deps.ArrowTypes]]
deps = ["Sockets", "UUIDs"]
git-tree-sha1 = "404265cd8128a2515a81d5eae16de90fdef05101"
uuid = "31f734f8-188a-4ce0-8406-c8a06bd891cd"
version = "2.3.0"

[[deps.Artifacts]]
uuid = "56f22d72-fd6d-98f1-02f0-08ddc0907c33"

[[deps.BangBang]]
deps = ["Accessors", "Compat", "ConstructionBase", "InitialValues", "LinearAlgebra", "Requires"]
git-tree-sha1 = "490e739172eb18f762e68dc3b928cad2a077983a"
uuid = "198e06fe-97b7-11e9-32a5-e1d131e6ad66"
version = "0.4.1"

    [deps.BangBang.extensions]
    BangBangChainRulesCoreExt = "ChainRulesCore"
    BangBangDataFramesExt = "DataFrames"
    BangBangStaticArraysExt = "StaticArrays"
    BangBangStructArraysExt = "StructArrays"
    BangBangTablesExt = "Tables"
    BangBangTypedTablesExt = "TypedTables"

    [deps.BangBang.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
    StructArrays = "09ab397b-f2b6-538f-b94a-2f83cf4a842a"
    Tables = "bd369af6-aec1-5ad0-b16a-f7cc5008161c"
    TypedTables = "9d95f2ec-7b3d-5a63-8d20-e2491e220bb9"

[[deps.Base64]]
uuid = "2a0f44e3-6c83-55bd-87e4-b1978d98bd5f"

[[deps.BaseDirs]]
git-tree-sha1 = "3e93fcd95fe8db4704e98dbda14453a0bfc6f6c3"
uuid = "18cc8868-cbac-4acf-b575-c8ff214dc66f"
version = "1.2.3"

[[deps.Baselet]]
git-tree-sha1 = "aebf55e6d7795e02ca500a689d326ac979aaf89e"
uuid = "9718e550-a3fa-408a-8086-8db961cd8217"
version = "0.1.1"

[[deps.BitIntegers]]
deps = ["Random"]
git-tree-sha1 = "a55462dfddabc34bc97d3a7403a2ca2802179ae6"
uuid = "c3b6d118-76ef-56ca-8cc7-ebb389d030a1"
version = "0.3.1"

[[deps.CodecLz4]]
deps = ["Lz4_jll", "TranscodingStreams"]
git-tree-sha1 = "b8aecef9f90530cf322a8386630ec18485c17991"
uuid = "5ba52731-8f18-5e0d-9241-30f10d1ec561"
version = "0.4.3"

[[deps.CodecZstd]]
deps = ["TranscodingStreams", "Zstd_jll"]
git-tree-sha1 = "23373fecba848397b1705f6183188a0c0bc86917"
uuid = "6b39b394-51ab-5f42-8807-6242bab2b4c2"
version = "0.8.2"

[[deps.ColorSchemes]]
deps = ["ColorTypes", "ColorVectorSpace", "Colors", "FixedPointNumbers", "PrecompileTools", "Random"]
git-tree-sha1 = "67c1f244b991cad9b0aa4b7540fb758c2488b129"
uuid = "35d6a980-a343-548e-a6ea-1d62b119f2f4"
version = "3.24.0"

[[deps.ColorTypes]]
deps = ["FixedPointNumbers", "Random"]
git-tree-sha1 = "eb7f0f8307f71fac7c606984ea5fb2817275d6e4"
uuid = "3da002f7-5984-5a60-b8a6-cbb66c0b333f"
version = "0.11.4"

[[deps.ColorVectorSpace]]
deps = ["ColorTypes", "FixedPointNumbers", "LinearAlgebra", "Requires", "Statistics", "TensorCore"]
git-tree-sha1 = "a1f44953f2382ebb937d60dafbe2deea4bd23249"
uuid = "c3611d14-8923-5661-9e6a-0046d554d3a4"
version = "0.10.0"

    [deps.ColorVectorSpace.extensions]
    SpecialFunctionsExt = "SpecialFunctions"

    [deps.ColorVectorSpace.weakdeps]
    SpecialFunctions = "276daf66-3868-5448-9aa4-cd146d93841b"

[[deps.Colors]]
deps = ["ColorTypes", "FixedPointNumbers", "Reexport"]
git-tree-sha1 = "fc08e5930ee9a4e03f84bfb5211cb54e7769758a"
uuid = "5ae59095-9a9b-59fe-a467-6f913c188581"
version = "0.12.10"

[[deps.Compat]]
deps = ["TOML", "UUIDs"]
git-tree-sha1 = "c955881e3c981181362ae4088b35995446298b80"
uuid = "34da2185-b29b-5c13-b0c7-acf172513d20"
version = "4.14.0"
weakdeps = ["Dates", "LinearAlgebra"]

    [deps.Compat.extensions]
    CompatLinearAlgebraExt = "LinearAlgebra"

[[deps.CompilerSupportLibraries_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "e66e0078-7015-5450-92f7-15fbd957f2ae"
version = "1.1.1+0"

[[deps.CompositionsBase]]
git-tree-sha1 = "802bb88cd69dfd1509f6670416bd4434015693ad"
uuid = "a33af91c-f02d-484b-be07-31d278c5ca2b"
version = "0.1.2"
weakdeps = ["InverseFunctions"]

    [deps.CompositionsBase.extensions]
    CompositionsBaseInverseFunctionsExt = "InverseFunctions"

[[deps.ConcurrentUtilities]]
deps = ["Serialization", "Sockets"]
git-tree-sha1 = "6cbbd4d241d7e6579ab354737f4dd95ca43946e1"
uuid = "f0e56b4a-5159-44fe-b623-3e5288b988bb"
version = "2.4.1"

[[deps.ConstructionBase]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "260fd2400ed2dab602a7c15cf10c1933c59930a2"
uuid = "187b0558-2788-49d3-abe0-74a17ed4e7c9"
version = "1.5.5"

    [deps.ConstructionBase.extensions]
    ConstructionBaseIntervalSetsExt = "IntervalSets"
    ConstructionBaseStaticArraysExt = "StaticArrays"

    [deps.ConstructionBase.weakdeps]
    IntervalSets = "8197267c-284f-5f27-9208-e0e47529a953"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"

[[deps.DataAPI]]
git-tree-sha1 = "abe83f3a2f1b857aac70ef8b269080af17764bbe"
uuid = "9a962f9c-6df0-11e9-0e5d-c546b8b5ee8a"
version = "1.16.0"

[[deps.DataStructures]]
deps = ["Compat", "InteractiveUtils", "OrderedCollections"]
git-tree-sha1 = "0f4b5d62a88d8f59003e43c25a8a90de9eb76317"
uuid = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
version = "0.18.18"

[[deps.DataValueInterfaces]]
git-tree-sha1 = "bfc1187b79289637fa0ef6d4436ebdfe6905cbd6"
uuid = "e2d170a0-9d28-54be-80f0-106bbe20a464"
version = "1.0.0"

[[deps.Dates]]
deps = ["Printf"]
uuid = "ade2ca70-3891-5945-98fb-dc099432e06a"

[[deps.DefineSingletons]]
git-tree-sha1 = "0fba8b706d0178b4dc7fd44a96a92382c9065c2c"
uuid = "244e2a9f-e319-4986-a169-4d1fe445cd52"
version = "0.1.2"

[[deps.DelimitedFiles]]
deps = ["Mmap"]
git-tree-sha1 = "9e2f36d3c96a820c678f2f1f1782582fcf685bae"
uuid = "8bb1440f-4735-579b-a4ab-409b98df4dab"
version = "1.9.1"

[[deps.Distributed]]
deps = ["Random", "Serialization", "Sockets"]
uuid = "8ba89e20-285c-5b6f-9357-94700520ee1b"

[[deps.DocStringExtensions]]
deps = ["LibGit2"]
git-tree-sha1 = "2fb1e02f2b635d0845df5d7c167fec4dd739b00d"
uuid = "ffbed154-4ef7-542d-bbb7-c09d3a79fcae"
version = "0.9.3"

[[deps.Downloads]]
deps = ["ArgTools", "FileWatching", "LibCURL", "NetworkOptions"]
uuid = "f43a241f-c20a-4ad4-852c-f6b1247861c6"
version = "1.6.0"

[[deps.EnumX]]
git-tree-sha1 = "bdb1942cd4c45e3c678fd11569d5cccd80976237"
uuid = "4e289a0a-7415-4d19-859d-a7e5c4648b56"
version = "1.0.4"

[[deps.ExprTools]]
git-tree-sha1 = "27415f162e6028e81c72b82ef756bf321213b6ec"
uuid = "e2ba6199-217a-4e67-a87a-7c52f15ade04"
version = "0.1.10"

[[deps.FileWatching]]
uuid = "7b1f6079-737a-58dc-b8bc-7a2ca5c1b5ee"

[[deps.FixedPointNumbers]]
deps = ["Statistics"]
git-tree-sha1 = "335bfdceacc84c5cdf16aadc768aa5ddfc5383cc"
uuid = "53c48c17-4a7d-5ca2-90c5-79b7896eea93"
version = "0.8.4"

[[deps.Future]]
deps = ["Random"]
uuid = "9fa8497b-333b-5362-9e8d-4d0656e87820"

[[deps.Hyperscript]]
deps = ["Test"]
git-tree-sha1 = "179267cfa5e712760cd43dcae385d7ea90cc25a4"
uuid = "47d2ed2b-36de-50cf-bf87-49c2cf4b8b91"
version = "0.0.5"

[[deps.HypertextLiteral]]
deps = ["Tricks"]
git-tree-sha1 = "7134810b1afce04bbc1045ca1985fbe81ce17653"
uuid = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
version = "0.9.5"

[[deps.IOCapture]]
deps = ["Logging", "Random"]
git-tree-sha1 = "8b72179abc660bfab5e28472e019392b97d0985c"
uuid = "b5f81e59-6552-4d32-b1f0-c071b021bf89"
version = "0.2.4"

[[deps.InitialValues]]
git-tree-sha1 = "4da0f88e9a39111c2fa3add390ab15f3a44f3ca3"
uuid = "22cec73e-a1b8-11e9-2c92-598750a2cf9c"
version = "0.3.1"

[[deps.InlineStrings]]
deps = ["Parsers"]
git-tree-sha1 = "9cc2baf75c6d09f9da536ddf58eb2f29dedaf461"
uuid = "842dd82b-1e85-43dc-bf29-5d0ee9dffc48"
version = "1.4.0"

[[deps.InteractiveUtils]]
deps = ["Markdown"]
uuid = "b77e0a4c-d291-57a0-90e8-8db25a27a240"

[[deps.InverseFunctions]]
deps = ["Test"]
git-tree-sha1 = "896385798a8d49a255c398bd49162062e4a4c435"
uuid = "3587e190-3f89-42d0-90ee-14403ec27112"
version = "0.1.13"
weakdeps = ["Dates"]

    [deps.InverseFunctions.extensions]
    DatesExt = "Dates"

[[deps.IrrationalConstants]]
git-tree-sha1 = "630b497eafcc20001bba38a4651b327dcfc491d2"
uuid = "92d709cd-6900-40b7-9082-c6be49f344b6"
version = "0.2.2"

[[deps.IteratorInterfaceExtensions]]
git-tree-sha1 = "a3f24677c21f5bbe9d2a714f95dcd58337fb2856"
uuid = "82899510-4779-5014-852e-03e436cf321d"
version = "1.0.0"

[[deps.JLLWrappers]]
deps = ["Artifacts", "Preferences"]
git-tree-sha1 = "7e5d6779a1e09a36db2a7b6cff50942a0a7d0fca"
uuid = "692b3bcd-3c85-4b1f-b108-f13ce0eb3210"
version = "1.5.0"

[[deps.JSON]]
deps = ["Dates", "Mmap", "Parsers", "Unicode"]
git-tree-sha1 = "31e996f0a15c7b280ba9f76636b3ff9e2ae58c9a"
uuid = "682c06a0-de6a-54ab-a142-c8b1cf79cde6"
version = "0.21.4"

[[deps.LaTeXStrings]]
git-tree-sha1 = "50901ebc375ed41dbf8058da26f9de442febbbec"
uuid = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
version = "1.3.1"

[[deps.LibCURL]]
deps = ["LibCURL_jll", "MozillaCACerts_jll"]
uuid = "b27032c2-a3e7-50c8-80cd-2d36dbcbfd21"
version = "0.6.4"

[[deps.LibCURL_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll", "Zlib_jll", "nghttp2_jll"]
uuid = "deac9b47-8bc7-5906-a0fe-35ac56dc84c0"
version = "8.4.0+0"

[[deps.LibGit2]]
deps = ["Base64", "LibGit2_jll", "NetworkOptions", "Printf", "SHA"]
uuid = "76f85450-5226-5b5a-8eaa-529ad045b433"

[[deps.LibGit2_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll"]
uuid = "e37daf67-58a4-590a-8e99-b0245dd2ffc5"
version = "1.6.4+0"

[[deps.LibSSH2_jll]]
deps = ["Artifacts", "Libdl", "MbedTLS_jll"]
uuid = "29816b5a-b9ab-546f-933c-edad1886dfa8"
version = "1.11.0+1"

[[deps.Libdl]]
uuid = "8f399da3-3557-5675-b5ff-fb832c97cbdb"

[[deps.LinearAlgebra]]
deps = ["Libdl", "OpenBLAS_jll", "libblastrampoline_jll"]
uuid = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"

[[deps.LogExpFunctions]]
deps = ["DocStringExtensions", "IrrationalConstants", "LinearAlgebra"]
git-tree-sha1 = "18144f3e9cbe9b15b070288eef858f71b291ce37"
uuid = "2ab3a3ac-af41-5b50-aa03-7779005ae688"
version = "0.3.27"

    [deps.LogExpFunctions.extensions]
    LogExpFunctionsChainRulesCoreExt = "ChainRulesCore"
    LogExpFunctionsChangesOfVariablesExt = "ChangesOfVariables"
    LogExpFunctionsInverseFunctionsExt = "InverseFunctions"

    [deps.LogExpFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    ChangesOfVariables = "9e997f8a-9a97-42d5-a9f1-ce6bfc15e2c0"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.Logging]]
uuid = "56ddb016-857b-54e1-b83d-db4d58db5568"

[[deps.LoggingExtras]]
deps = ["Dates", "Logging"]
git-tree-sha1 = "c1dd6d7978c12545b4179fb6153b9250c96b0075"
uuid = "e6f89c97-d47a-5376-807f-9c37f3926c36"
version = "1.0.3"

[[deps.Lz4_jll]]
deps = ["Artifacts", "JLLWrappers", "Libdl"]
git-tree-sha1 = "6c26c5e8a4203d43b5497be3ec5d4e0c3cde240a"
uuid = "5ced341a-0733-55b8-9ab6-a4889d929147"
version = "1.9.4+0"

[[deps.MIMEs]]
git-tree-sha1 = "65f28ad4b594aebe22157d6fac869786a255b7eb"
uuid = "6c6e2e6c-3030-632d-7369-2d6c69616d65"
version = "0.1.4"

[[deps.MacroTools]]
deps = ["Markdown", "Random"]
git-tree-sha1 = "2fa9ee3e63fd3a4f7a9a4f4744a52f4856de82df"
uuid = "1914dd2f-81c6-5fcd-8719-6d5c9610ff09"
version = "0.5.13"

[[deps.Markdown]]
deps = ["Base64"]
uuid = "d6f4376e-aef5-505a-96c1-9c027394607a"

[[deps.MbedTLS_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "c8ffd9c3-330d-5841-b78e-0817d7145fa1"
version = "2.28.2+1"

[[deps.MicroCollections]]
deps = ["Accessors", "BangBang", "InitialValues"]
git-tree-sha1 = "44d32db644e84c75dab479f1bc15ee76a1a3618f"
uuid = "128add7d-3638-4c79-886c-908ea0c25c34"
version = "0.2.0"

[[deps.Missings]]
deps = ["DataAPI"]
git-tree-sha1 = "f66bdc5de519e8f8ae43bdc598782d35a25b1272"
uuid = "e1d29d7a-bbdc-5cf2-9ac0-f12de2c33e28"
version = "1.1.0"

[[deps.Mmap]]
uuid = "a63ad114-7e13-5084-954f-fe012c677804"

[[deps.Mocking]]
deps = ["Compat", "ExprTools"]
git-tree-sha1 = "bf17d9cb4f0d2882351dfad030598f64286e5936"
uuid = "78c3b35d-d492-501b-9361-3d52fe80e533"
version = "0.7.8"

[[deps.MozillaCACerts_jll]]
uuid = "14a3606d-f60d-562e-9121-12d972cd8159"
version = "2023.1.10"

[[deps.NetworkOptions]]
uuid = "ca575930-c2e3-43a9-ace4-1e988b2c1908"
version = "1.2.0"

[[deps.OpenBLAS_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "Libdl"]
uuid = "4536629a-c528-5b80-bd46-f80d51c5b363"
version = "0.3.23+4"

[[deps.OrderedCollections]]
git-tree-sha1 = "dfdf5519f235516220579f949664f1bf44e741c5"
uuid = "bac558e1-5e72-5ebc-8fee-abe8a469f55d"
version = "1.6.3"

[[deps.Parameters]]
deps = ["OrderedCollections", "UnPack"]
git-tree-sha1 = "34c0e9ad262e5f7fc75b10a9952ca7692cfc5fbe"
uuid = "d96e819e-fc66-5662-9728-84c9c7592b0a"
version = "0.12.3"

[[deps.Parsers]]
deps = ["Dates", "PrecompileTools", "UUIDs"]
git-tree-sha1 = "8489905bcdbcfac64d1daa51ca07c0d8f0283821"
uuid = "69de0a69-1ddd-5017-9359-2bf0b02dc9f0"
version = "2.8.1"

[[deps.Pkg]]
deps = ["Artifacts", "Dates", "Downloads", "FileWatching", "LibGit2", "Libdl", "Logging", "Markdown", "Printf", "REPL", "Random", "SHA", "Serialization", "TOML", "Tar", "UUIDs", "p7zip_jll"]
uuid = "44cfe95a-1eb2-52ea-b672-e2afdf69b78f"
version = "1.10.0"

[[deps.PlotlyBase]]
deps = ["ColorSchemes", "Dates", "DelimitedFiles", "DocStringExtensions", "JSON", "LaTeXStrings", "Logging", "Parameters", "Pkg", "REPL", "Requires", "Statistics", "UUIDs"]
git-tree-sha1 = "56baf69781fc5e61607c3e46227ab17f7040ffa2"
uuid = "a03496cd-edff-5a9b-9e67-9cda94a718b5"
version = "0.8.19"

[[deps.PlutoPlotly]]
deps = ["AbstractPlutoDingetjes", "BaseDirs", "Colors", "Dates", "Downloads", "HypertextLiteral", "InteractiveUtils", "LaTeXStrings", "Markdown", "Pkg", "PlotlyBase", "Reexport", "TOML"]
git-tree-sha1 = "1ae939782a5ce9a004484eab5416411c7190d3ce"
uuid = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
version = "0.4.6"

    [deps.PlutoPlotly.extensions]
    PlotlyKaleidoExt = "PlotlyKaleido"
    UnitfulExt = "Unitful"

    [deps.PlutoPlotly.weakdeps]
    PlotlyKaleido = "f2990250-8cf9-495f-b13a-cce12b45703c"
    Unitful = "1986cc42-f94f-5a68-af5c-568840ba703d"

[[deps.PlutoUI]]
deps = ["AbstractPlutoDingetjes", "Base64", "ColorTypes", "Dates", "FixedPointNumbers", "Hyperscript", "HypertextLiteral", "IOCapture", "InteractiveUtils", "JSON", "Logging", "MIMEs", "Markdown", "Random", "Reexport", "URIs", "UUIDs"]
git-tree-sha1 = "71a22244e352aa8c5f0f2adde4150f62368a3f2e"
uuid = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
version = "0.7.58"

[[deps.PooledArrays]]
deps = ["DataAPI", "Future"]
git-tree-sha1 = "36d8b4b899628fb92c2749eb488d884a926614d3"
uuid = "2dfb63ee-cc39-5dd5-95bd-886bf059d720"
version = "1.4.3"

[[deps.PrecompileTools]]
deps = ["Preferences"]
git-tree-sha1 = "5aa36f7049a63a1528fe8f7c3f2113413ffd4e1f"
uuid = "aea7be01-6a6a-4083-8856-8a6e6704d82a"
version = "1.2.1"

[[deps.Preferences]]
deps = ["TOML"]
git-tree-sha1 = "9306f6085165d270f7e3db02af26a400d580f5c6"
uuid = "21216c6a-2e73-6563-6e65-726566657250"
version = "1.4.3"

[[deps.Printf]]
deps = ["Unicode"]
uuid = "de0858da-6303-5e67-8744-51eddeeeb8d7"

[[deps.REPL]]
deps = ["InteractiveUtils", "Markdown", "Sockets", "Unicode"]
uuid = "3fa0cd96-eef1-5676-8a61-b3b8758bbffb"

[[deps.Random]]
deps = ["SHA"]
uuid = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"

[[deps.Reexport]]
git-tree-sha1 = "45e428421666073eab6f2da5c9d310d99bb12f9b"
uuid = "189a3867-3050-52da-a836-e630ba90ab69"
version = "1.2.2"

[[deps.Requires]]
deps = ["UUIDs"]
git-tree-sha1 = "838a3a4188e2ded87a4f9f184b4b0d78a1e91cb7"
uuid = "ae029012-a4dd-5104-9daa-d747884805df"
version = "1.3.0"

[[deps.SHA]]
uuid = "ea8e919c-243c-51af-8825-aaa63cd721ce"
version = "0.7.0"

[[deps.Scratch]]
deps = ["Dates"]
git-tree-sha1 = "3bac05bc7e74a75fd9cba4295cde4045d9fe2386"
uuid = "6c6a2e73-6563-6170-7368-637461726353"
version = "1.2.1"

[[deps.SentinelArrays]]
deps = ["Dates", "Random"]
git-tree-sha1 = "363c4e82b66be7b9f7c7c7da7478fdae07de44b9"
uuid = "91c51154-3ec4-41a3-a24f-3f23e20d615c"
version = "1.4.2"

[[deps.Serialization]]
uuid = "9e88b42a-f829-5b0c-bbe9-9e923198166b"

[[deps.Setfield]]
deps = ["ConstructionBase", "Future", "MacroTools", "StaticArraysCore"]
git-tree-sha1 = "e2cc6d8c88613c05e1defb55170bf5ff211fbeac"
uuid = "efcf1570-3423-57d1-acb7-fd33fddbac46"
version = "1.1.1"

[[deps.Sockets]]
uuid = "6462fe0b-24de-5631-8697-dd941f90decc"

[[deps.SortingAlgorithms]]
deps = ["DataStructures"]
git-tree-sha1 = "66e0a8e672a0bdfca2c3f5937efb8538b9ddc085"
uuid = "a2af1166-a08f-5f64-846c-94a0d3cef48c"
version = "1.2.1"

[[deps.SparseArrays]]
deps = ["Libdl", "LinearAlgebra", "Random", "Serialization", "SuiteSparse_jll"]
uuid = "2f01184e-e22b-5df5-ae63-d93ebab69eaf"
version = "1.10.0"

[[deps.SplittablesBase]]
deps = ["Setfield", "Test"]
git-tree-sha1 = "e08a62abc517eb79667d0a29dc08a3b589516bb5"
uuid = "171d559e-b47b-412a-8079-5efa626c420e"
version = "0.1.15"

[[deps.StaticArrays]]
deps = ["LinearAlgebra", "PrecompileTools", "Random", "StaticArraysCore"]
git-tree-sha1 = "bf074c045d3d5ffd956fa0a461da38a44685d6b2"
uuid = "90137ffa-7385-5640-81b9-e52037218182"
version = "1.9.3"

    [deps.StaticArrays.extensions]
    StaticArraysChainRulesCoreExt = "ChainRulesCore"
    StaticArraysStatisticsExt = "Statistics"

    [deps.StaticArrays.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    Statistics = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"

[[deps.StaticArraysCore]]
git-tree-sha1 = "36b3d696ce6366023a0ea192b4cd442268995a0d"
uuid = "1e83bf80-4336-4d27-bf5d-d5a4f845583c"
version = "1.4.2"

[[deps.Statistics]]
deps = ["LinearAlgebra", "SparseArrays"]
uuid = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
version = "1.10.0"

[[deps.StatsAPI]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1ff449ad350c9c4cbc756624d6f8a8c3ef56d3ed"
uuid = "82ae8749-77ed-4fe6-ae5f-f523153014b0"
version = "1.7.0"

[[deps.StatsBase]]
deps = ["DataAPI", "DataStructures", "LinearAlgebra", "LogExpFunctions", "Missings", "Printf", "Random", "SortingAlgorithms", "SparseArrays", "Statistics", "StatsAPI"]
git-tree-sha1 = "1d77abd07f617c4868c33d4f5b9e1dbb2643c9cf"
uuid = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
version = "0.34.2"

[[deps.SuiteSparse_jll]]
deps = ["Artifacts", "Libdl", "libblastrampoline_jll"]
uuid = "bea87d4a-7f5b-5778-9afe-8cc45184846c"
version = "7.2.1+1"

[[deps.TOML]]
deps = ["Dates"]
uuid = "fa267f1f-6049-4f14-aa54-33bafae1ed76"
version = "1.0.3"

[[deps.TZJData]]
deps = ["Artifacts"]
git-tree-sha1 = "1607ad46cf8d642aa779a1d45af1c8620dbf6915"
uuid = "dc5dba14-91b3-4cab-a142-028a31da12f7"
version = "1.2.0+2024a"

[[deps.TableTraits]]
deps = ["IteratorInterfaceExtensions"]
git-tree-sha1 = "c06b2f539df1c6efa794486abfb6ed2022561a39"
uuid = "3783bdb8-4a98-5b6b-af9a-565f29a5fe9c"
version = "1.0.1"

[[deps.Tables]]
deps = ["DataAPI", "DataValueInterfaces", "IteratorInterfaceExtensions", "LinearAlgebra", "OrderedCollections", "TableTraits"]
git-tree-sha1 = "cb76cf677714c095e535e3501ac7954732aeea2d"
uuid = "bd369af6-aec1-5ad0-b16a-f7cc5008161c"
version = "1.11.1"

[[deps.Tar]]
deps = ["ArgTools", "SHA"]
uuid = "a4e569a6-e804-4fa4-b0f3-eef7a1d5b13e"
version = "1.10.0"

[[deps.TensorCore]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1feb45f88d133a655e001435632f019a9a1bcdb6"
uuid = "62fd8b95-f654-4bbd-a8a5-9c27f68ccd50"
version = "0.1.1"

[[deps.Test]]
deps = ["InteractiveUtils", "Logging", "Random", "Serialization"]
uuid = "8dfed614-e22c-5e08-85e1-65c5234f0b40"

[[deps.TimeZones]]
deps = ["Dates", "Downloads", "InlineStrings", "Mocking", "Printf", "Scratch", "TZJData", "Unicode", "p7zip_jll"]
git-tree-sha1 = "96793c9316d6c9f9be4641f2e5b1319a205e6f27"
uuid = "f269a46b-ccf7-5d73-abea-4c690281aa53"
version = "1.15.0"

    [deps.TimeZones.extensions]
    TimeZonesRecipesBaseExt = "RecipesBase"

    [deps.TimeZones.weakdeps]
    RecipesBase = "3cdcf5f2-1ef4-517c-9805-6587b60abb01"

[[deps.TranscodingStreams]]
git-tree-sha1 = "5d54d076465da49d6746c647022f3b3674e64156"
uuid = "3bb67fe8-82b1-5028-8e26-92a6c54297fa"
version = "0.10.8"
weakdeps = ["Random", "Test"]

    [deps.TranscodingStreams.extensions]
    TestExt = ["Test", "Random"]

[[deps.Transducers]]
deps = ["Accessors", "Adapt", "ArgCheck", "BangBang", "Baselet", "CompositionsBase", "ConstructionBase", "DefineSingletons", "Distributed", "InitialValues", "Logging", "Markdown", "MicroCollections", "Requires", "SplittablesBase", "Tables"]
git-tree-sha1 = "47e516e2eabd0cf1304cd67839d9a85d52dd659d"
uuid = "28d57a85-8fef-5791-bfe6-a80928e7c999"
version = "0.4.81"

    [deps.Transducers.extensions]
    TransducersBlockArraysExt = "BlockArrays"
    TransducersDataFramesExt = "DataFrames"
    TransducersLazyArraysExt = "LazyArrays"
    TransducersOnlineStatsBaseExt = "OnlineStatsBase"
    TransducersReferenceablesExt = "Referenceables"

    [deps.Transducers.weakdeps]
    BlockArrays = "8e7c35d0-a365-5155-bbbb-fb81a777f24e"
    DataFrames = "a93c6f00-e57d-5684-b7b6-d8193f3e46c0"
    LazyArrays = "5078a376-72f3-5289-bfd5-ec5146d43c02"
    OnlineStatsBase = "925886fa-5bf2-5e8e-b522-a9147a512338"
    Referenceables = "42d2dcc6-99eb-4e98-b66c-637b7d73030e"

[[deps.Tricks]]
git-tree-sha1 = "eae1bb484cd63b36999ee58be2de6c178105112f"
uuid = "410a4b4d-49e4-4fbc-ab6d-cb71b17b3775"
version = "0.1.8"

[[deps.URIs]]
git-tree-sha1 = "67db6cc7b3821e19ebe75791a9dd19c9b1188f2b"
uuid = "5c2747f8-b7ea-4ff2-ba2e-563bfd36b1d4"
version = "1.5.1"

[[deps.UUIDs]]
deps = ["Random", "SHA"]
uuid = "cf7118a7-6976-5b1a-9a39-7adc72f591a4"

[[deps.UnPack]]
git-tree-sha1 = "387c1f73762231e86e0c9c5443ce3b4a0a9a0c2b"
uuid = "3a884ed6-31ef-47d7-9d2a-63182c4928ed"
version = "1.0.2"

[[deps.Unicode]]
uuid = "4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5"

[[deps.Zlib_jll]]
deps = ["Libdl"]
uuid = "83775a58-1f1d-513f-b197-d71354ab007a"
version = "1.2.13+1"

[[deps.Zstd_jll]]
deps = ["Artifacts", "JLLWrappers", "Libdl"]
git-tree-sha1 = "e678132f07ddb5bfa46857f0d7620fb9be675d3b"
uuid = "3161d3a3-bdf6-5164-811a-617609db77b4"
version = "1.5.6+0"

[[deps.libblastrampoline_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850b90-86db-534c-a0d3-1478176c7d93"
version = "5.8.0+1"

[[deps.nghttp2_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850ede-7688-5339-a07c-302acd2aaf8d"
version = "1.52.0+1"

[[deps.p7zip_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "3f19e933-33d8-53b3-aaab-bd5110c3b7a0"
version = "17.4.0+2"
"""

# ╔═╡ Cell order:
# ╟─516234a8-2748-11ed-35df-432eebaa5162
# ╠═5afad18b-1d87-450e-a0ff-8c1249d663ed
# ╟─65818e67-c146-4686-a9aa-d0859ef662fb
# ╠═c04c803c-cdca-4b8b-9c9d-e456ee677906
# ╠═39ae7727-ea72-48f3-8d63-e03a9f607f87
# ╠═563b6dbd-ce51-4904-b1cc-d766bd1fd1d6
# ╟─d5ac7c6f-9636-46d9-806f-34d6c8e4d4d5
# ╟─76489f21-677e-4c25-beaa-afaf2244cd94
# ╠═8dbc76fd-ac73-47ca-983e-0e90023390e3
# ╟─27d12c1c-ddb0-4bc1-af51-3388ff806705
# ╟─4d4baa61-b5bd-4bcf-a491-9a35a1695f0b
# ╠═cd139745-1877-43a2-97a0-3333e544cbd8
# ╠═0d0bbf62-b1ac-45f6-8a92-1e77b0709cb3
# ╟─cd79ce14-14a1-43c6-93e0-b4a786f7f9fb
# ╟─e0cc1ca1-595d-44e2-8612-261df9e2d327
# ╟─4f4551fe-54a9-4186-ab8f-3535dc2bf4c5
# ╠═5ae2d740-13c7-4568-8f04-25bc82fecbdb
# ╟─69ff1b72-cb1b-4724-a445-38e4c9846964
# ╟─8987052b-0828-43a5-982e-5f3d6209f2aa
# ╟─4d1f0065-e5ab-46fa-8ab1-a0bbcf523c27
# ╠═2ff6d187-e06f-47b5-9834-d06bfc820c26
# ╟─dab6eac0-7958-4d3a-a1af-781fb57e7adb
# ╠═db66615e-fbbc-4ea8-b529-bdc14e58a215
# ╟─b0df4dad-74c7-4469-a13f-5ef6bb81199f
# ╟─98547223-05a6-43da-80b2-63c67d2de283
# ╠═562e824c-34b5-415d-b186-d8e2cf1980e7
# ╟─976ff0c3-54aa-41f0-b963-baf77bea8cb8
# ╠═b9054ed4-7f16-4920-b13e-5f4c6f50dcf3
# ╠═0dbd2b87-d000-408b-8d04-25fc0fa512d1
# ╠═5b688057-06c7-4ae4-95d6-0a2ff451f11c
# ╠═f0e88db8-e3ee-4b74-923e-c34038024824
# ╠═aee1f6a8-de43-402e-b375-86c0f2f9e6b8
# ╟─24efe9b4-9308-4ad1-8ef0-69f6f93407c0
# ╟─26fe0c28-8f0f-4cff-87fb-76f04fce1be1
# ╟─340ba72b-172a-4d92-99b2-17687ab511c7
# ╟─caca8d95-e40e-4592-b29e-a7e8b19faeb5
# ╠═269f4505-e807-446c-8fd8-3458482e00ab
# ╠═c31f4646-aa8a-41e3-9c68-ae8a349d4ed1
# ╟─1aa76f3d-6041-4886-a6cd-787bdf1ec63c
# ╠═cd1980ff-2f35-4599-b08c-2037ddf5e995
# ╟─c32d0943-ba8f-438f-83b6-5ed42221f630
# ╟─1127c36f-9bc0-49b1-9481-8a5861bdf6ca
# ╟─a3243bc4-7ae7-418a-9881-a265ca95f5ef
# ╟─08ff749b-f4ff-4639-99f5-48262aa4643e
# ╠═898f1b06-a34f-496b-99db-9ca23498cbee
# ╟─dff6f326-ab7e-45e5-8c5e-28bfbb1d99bc
# ╠═4b424a47-dfeb-4380-8ab8-8bd24a080c2e
# ╟─466268c4-664d-42dd-84c1-7b8ade49936f
# ╟─b3a5adcb-5343-44e9-9466-1c51c1143a0d
# ╠═fb00aedd-e103-4463-b4f8-d0dce6275c64
# ╟─08e81e30-119d-4f4e-a865-f1f85cbffd31
# ╟─63bf9d16-4516-4cec-895f-f010275bca16
# ╟─31072f9b-de1b-42cf-a187-cbff99b49b50
# ╟─cfdaa9c2-265f-4540-9d04-d1b7a72aee3e
# ╠═df62fd47-6627-4931-b429-964c65960446
# ╟─3b0d2c55-2123-4b51-b946-6bc352e3d00a
# ╟─aab6ca56-57ca-421e-9b71-e3e96681c4c5
# ╟─72b40384-9ca1-4bc1-8e1a-8b639d39e215
# ╠═a8ec05ad-8333-4423-ab42-883ab806ebd7
# ╠═d0b18699-7d3a-418d-9d15-be41f1643f09
# ╠═81f2f335-6606-4506-bfb3-d0d95e651f24
# ╠═094321bc-2d44-4e67-9ac6-5216a42e0cd3
# ╠═6e273f2b-a1af-421f-aca7-772a836b89ef
# ╠═ffdd925e-b2b4-4cb1-9d6f-b8c9397729f6
# ╠═00c8f62f-dee2-476c-b896-68d3ab57a168
# ╟─e8a6e672-b860-404f-83c1-62a080f23112
# ╠═8586d633-7c50-49ba-9b74-b5bdad27c317
# ╠═fcaddc19-c0c9-4e8e-8f84-adc7a02cc1f5
# ╠═4b2a4fb1-7395-4293-9ff9-e2f9da50f56b
# ╠═6778296c-ab05-47e7-86d2-e98c075a8a0c
# ╠═6605b946-3010-47ed-8d88-3c4dca993cf8
# ╠═636d768c-670d-4485-a1dd-2bab6cf086d0
# ╟─04ea981e-337e-4324-a5cc-178eb3c7605b
# ╟─ee0f55c6-e9c6-4199-9f27-5706f3c84863
# ╠═f9e4baec-c988-4abd-9bb0-c618c0ec07b9
# ╠═fad6e3b9-6d6e-4ea0-ac3d-5e57374c7056
# ╠═f9cdd5a8-3a9b-4be4-9a33-bb0047ac4a96
# ╠═d4e1e807-7e87-4b43-9c36-f59999dfcd2d
# ╠═466b1cbf-586f-4b53-8b4b-2dc32e1c8b0a
# ╠═bf7950f2-05fa-4455-ad08-27735148d95c
# ╠═f143446c-e44b-4d75-baa7-3b24eafad003
# ╠═13f08473-f0d8-47d1-aa48-de3e4a083dbf
# ╠═c04c91be-de42-4dfc-bd0d-b9fbdde0c9cf
# ╠═1eb9a2ad-4584-4d32-8abb-e0e0bc0a771b
# ╠═308fd488-a009-4de0-8b27-c1f6b0677fed
# ╠═cffc9f11-77d7-4076-aa3b-821f1c741f58
# ╠═f369a092-420d-4660-b802-93f05d5e7972
# ╠═82e5719c-bbdb-4a18-b2f0-ad746b6acd41
# ╠═f6486854-4892-4fb6-a805-de56b19b3571
# ╠═aa898360-f802-438a-9081-a2e517230db2
# ╟─bc295bb5-addb-4bcf-a3e3-c839ccc346bd
# ╠═beae0491-ed11-4edf-a136-d384578b088b
# ╠═6a4116c9-87cf-4ee7-8030-aa1150853984
# ╠═2c587d5b-7b62-4835-ad02-9575c13d5874
# ╠═41bb1f78-b83a-4a45-ba5c-faa94e112f45
# ╟─245d9616-e0d2-497e-bfdd-4729a7215bfd
# ╟─321bdf5a-bff7-4181-986f-d3884ea96d27
# ╠═0fff8e1b-d0c2-49b8-93b4-8d1615c26690
# ╠═e689df6b-d6f0-4928-9212-a940aa00b0ef
# ╟─b03087e9-e15d-4563-bdae-4d9ba7d2cec6
# ╠═c1ff1bea-649c-4483-b4be-55134f0e8cb7
# ╠═6858ef8b-1ca7-4e96-b57e-26553423cc13
# ╠═037f1804-b24e-46e7-b2a8-6747e669db66
# ╠═e25ec0d5-f70f-4269-b2a1-efa194936f72
# ╠═47ce2eda-b2c4-4f81-8d91-955bc35bab49
# ╠═304e6afd-11e0-4011-9929-85889b988400
# ╠═c62cc32c-0d29-4ea2-8284-ac4c883df6db
# ╠═fb5601b0-06d4-43c4-81a6-23a4a8f29f00
# ╠═1d97325a-8b9a-438d-a5f9-e17638e64627
# ╠═7ae23e8e-d554-4d26-a08a-83dab507af13
# ╠═eb9ba23d-bee5-4bb1-b3e1-fe40d9f681dc
# ╠═618b5f35-2df5-4ffb-a34f-add542691080
# ╠═3edc09ab-8fa1-440f-8a45-546898a2b2a3
# ╠═b15f1262-1acf-40e5-87a7-bc4b1b437a42
# ╠═6612f482-2f10-43fa-9b7b-2f0c6a94b8e8
# ╠═e43513e8-2517-43b7-9a16-e57d4125edc4
# ╠═94b339bb-6e2d-422f-8043-615e8be9a217
# ╠═f8bf29fe-568f-437f-ba82-6b861988a18e
# ╠═82e1ceb8-b1bb-4dea-b041-bf462041793f
# ╠═e4f73889-af82-4304-89d5-ee50172eb3da
# ╠═ecd8742c-2e10-4814-b477-7024e85b7fa6
# ╠═fc137613-7b4b-414c-93af-eeb2ace5d67f
# ╠═6c8cd429-2c2e-4515-98b2-d0394962e479
# ╠═3b4e27e7-8065-44b3-bc2a-e540913aa540
# ╠═250ea9da-dea3-4bf3-932d-cdda6756ae33
# ╠═726af565-8905-4409-864f-a5c1b5767e09
# ╠═33f66659-1a87-4890-9137-dbc7776a19d8
# ╠═195d2a34-c44c-4088-8ec4-dece3107f16d
# ╠═16c68a13-c295-4a64-bc2b-2ae8451f332f
# ╠═44364e7f-1910-421a-b961-63fbbaac8230
# ╠═80affd41-b5e6-4b9c-b827-4e3b39bd7767
# ╠═0899f37c-5def-4d15-8ca3-ebdec8e96b43
# ╠═2fa207dd-749f-4dc0-b4ab-159edf1d9bce
# ╠═6cf35193-dba5-4f78-a4ac-245dda7a0846
# ╠═96bd8d33-d4e8-45bf-9b75-43e8bda6fa07
# ╠═5e49504e-9623-48f9-aeb5-360906b92a09
# ╠═679b6096-b3ca-422d-9d9a-225832510ab1
# ╠═0adcbce8-2be5-48ef-af43-04815e10dc5c
# ╠═9a1b250f-b404-4db3-a4b7-4cd33b79d921
# ╠═324181f0-b890-4198-9b4b-c36547e6629a
# ╠═0f37ec0a-b737-478b-bf6a-027899250c4e
# ╠═f424edac-388d-4465-900f-9459d2a88f79
# ╠═4494cb61-ee2c-467b-9bf6-0afb59023e91
# ╠═143fff7d-0bb2-43b4-b810-53784fe848bd
# ╠═9be963b9-f3a1-4f92-8ff9-f5be75ed52f2
# ╠═113d2bc2-1f77-479f-86e5-a65b20672d7a
# ╠═5d2abde0-7128-41c3-bd1f-b6940492d1ae
# ╟─4e1c115a-4020-4a5a-a79a-56056892a953
# ╠═729197ce-2c27-467d-ba5f-47a1ecd539f2
# ╠═bb439641-30bd-495d-ba70-06b2e27efdbd
# ╟─77fde69f-2119-41eb-8993-a93b2c47ca7e
# ╠═eded8b72-70f4-4579-ba69-2eca409fa684
# ╠═1f7d77a6-d774-436d-a745-5a160cc15f2b
# ╠═502a7125-4460-4d39-be14-4852fb6d9ad2
# ╠═4b3604db-0c1b-4770-95b1-5f5bb34d071b
# ╠═39c96fc8-8259-46e3-88a0-a14eb6752b5c
# ╠═f5e52b2f-ea14-423d-8ca9-2ed68cd27c69
# ╠═3bf9e526-826d-42b8-84ee-75f1c7f79c69
# ╠═9d69687a-8df6-4e74-aa99-fbfcc84bcccf
# ╠═0f9080af-f166-4a78-a003-8df07f6c27d4
# ╟─00000000-0000-0000-0000-000000000001
# ╟─00000000-0000-0000-0000-000000000002
