### A Pluto.jl notebook ###
# v0.19.25

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local iv = try Base.loaded_modules[Base.PkgId(Base.UUID("6e696c72-6542-2067-7265-42206c756150"), "AbstractPlutoDingetjes")].Bonds.initial_value catch; b -> missing; end
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : iv(el)
        el
    end
end

# ╔═╡ d04d4234-d97f-11ed-2ea3-85ee0fc3bd70
begin
	using PlutoUI, PlutoPlotly, Random, Distributions, StatsBase, LinearAlgebra, LaTeXStrings, Base.Threads, ProfileCanvas, HypertextLiteral, ProgressLogging, BenchmarkTools, Transducers, StaticArrays
	TableOfContents()
end

# ╔═╡ 36a6e43f-6bcf-4c27-bfbb-047760e77ada
md"""
# Chapter 13 Policy Gradient Methods Introduction
Instead of selection actions based on *action-value estimates* we learn a *parameterized policy* with parameters θ.  $\pi(a|s, \mathbf{\theta}) = \text{Pr}\{A_t=a|S_t=s, \mathbf{\theta}_t=\mathbf{\theta\}}$ denotes the probability that action *a* is taken at time *t* given that the environment is in state *s* at time *t* with parameter **θ**.  

We consider methods that improve the policy parameter using the gradient of some scalar performance measure $J(\mathbf{\theta})$ with respect to the policy parameters.  We follow gradient ascent since we are trying to maximize this value and methods that use this approach are called *policy gradient methods*.  Methods that learn approximations to both policy and value functions are often called *actor-critic methods*, where 'actor' is a reference to the learned policy, and 'critic' refers to the learned value function, usually a state-value function.
# 13.1 Policy Approximation and its Advantages
"""

# ╔═╡ 2501cbc0-9772-4b2f-ab01-ef7903e62950
md"""
If the state/action space is discrete and not too large then we can have numerical preferences for each state/action pair parameterized by θ.  $h(s, a, \mathbf{\theta})$ and the corresponding policy can be to select actions according to the probability distribution generated by the soft-max.  $\pi(a|s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \frac{\exp{h(s, a, \mathbf{\theta})}}{\sum_b \exp{h(s, b, \mathbf{\theta})}}$.  One advantage of using the soft-max is that the optimal policy can be stochastic or we can approach a deterministic policy by selecting the action with the highest probability.  If we include a temperature parameter in the soft-max then we can vary the same policy to be more or less stochastic as needed.

Another advantage is that for some problems the policy may be easier to approximate than the action-value function.  We can also inject some prior knowledge of the environment into how the policy is parametrized.

## Soft-max notation and gradients
For future reference, we can denote the soft-max function of a vector $\mathbf{x}$ as follows:

$\sigma(\mathbf{x}) = \frac{e^{\mathbf{x}}}{\sum_j{e^{x_j}}} \text{ where we abuse the notation } e^{\mathbf{x}} = \begin{pmatrix}
e^{x_1} \\
e^{x_2} \\
\vdots \\
e^{x_n}
\end{pmatrix}$

The gradient of this vector function will in general be an nxn matrix where n is the length of the input vector.  The components of this matrix are given by:

$\begin{align}
\nabla\sigma(\mathbf{x})_{i, j} &= \frac{\partial}{\partial x_j} \left ( \frac{e^{x_i}}{\sum_k{e^{x_k}}} \right ) \\
&=\left ( \frac{1}{{\sum_k{e^{x_k}}}} \right )^2 \left ( e^{x_i} \frac{\partial{x_i}}{\partial{x_j}}  \sum_k{e^{x_k}}  - e^{x_i} \sum_k{e^{x_k} \frac{\partial{x_k}}{\partial{x_j}}} \right )  \\
&=\left ( \frac{e^{x_i}}{{\sum_k{e^{x_k}}}} \right ) \left ( \frac{1}{{\sum_k{e^{x_k}}}} \right ) \left ( \delta_{i, j}  \sum_k{e^{x_k}}  - \sum_k{e^{x_k} \delta_{k, j}} \right )  \\
&=\sigma(\mathbf{x})_i \left ( \frac{1}{{\sum_k{e^{x_k}}}} \right ) \left ( \delta_{i, j}  \sum_k{e^{x_k}}  - e^{x_j} \right ) \tag{by delta function property}  \\
&=\sigma(\mathbf{x})_i  \left ( \delta_{i, j}    -   \sigma(\mathbf{x})_j \right ) \tag{using softmax definition}  \\
\end{align}$ 

So we have our final expression for each component of the gradient:

$\nabla\sigma(\mathbf{x})_{i, j} = \sigma(\mathbf{x})_i  \left ( \delta_{i, j}    -   \sigma(\mathbf{x})_j \right ) \tag{softmax gradient}$

Note that when $i \neq j$ these terms are identical swapping i and j.  When $i=j$ the swap keeps us at the same diagonal term, so this matrix is symmetric as shown below:

$\nabla \mathbf{\sigma}(\mathbf{x}) = -\begin{bmatrix}
\mathbf{\sigma}(\mathbf{x})_1 (1 - \mathbf{\sigma}(\mathbf{x})_1) & -\mathbf{\sigma}(\mathbf{x})_1 \mathbf{\sigma}(\mathbf{x})_2 & \cdots & -\mathbf{\sigma}(\mathbf{x})_1 \mathbf{\sigma}(\mathbf{x})_{n-1} & -\mathbf{\sigma}(\mathbf{x})_1 \mathbf{\sigma}(\mathbf{x})_n \\
-\mathbf{\sigma}(\mathbf{x})_2 \mathbf{\sigma}(\mathbf{x})_1 & \mathbf{\sigma}(\mathbf{x})_2 (1 - \mathbf{\sigma}(\mathbf{x})_2) & -\mathbf{\sigma}(\mathbf{x})_2 \mathbf{\sigma}(\mathbf{x})_3 & \cdots & -\mathbf{\sigma}(\mathbf{x})_2 \mathbf{\sigma}(\mathbf{x})_n \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
-\mathbf{\sigma}(\mathbf{x})_{n-1} \mathbf{\sigma}(\mathbf{x})_1 & -\mathbf{\sigma}(\mathbf{x})_{n-1} \mathbf{\sigma}(\mathbf{x})_1 & \cdots & \mathbf{\sigma}(\mathbf{x})_{n-1}(1 - \mathbf{\sigma}(\mathbf{x})_{n-1})  & -\mathbf{\sigma}(\mathbf{x})_{n-1}\mathbf{\sigma}(\mathbf{x})_n \\
-\mathbf{\sigma}(\mathbf{x})_n \mathbf{\sigma}(\mathbf{x})_1 & -\mathbf{\sigma}(\mathbf{x})_n \mathbf{\sigma}(\mathbf{x})_2 & -\mathbf{\sigma}(\mathbf{x})_n \mathbf{\sigma}(\mathbf{x})_3 & \cdots & \mathbf{\sigma}(\mathbf{x})_n(1-\mathbf{\sigma}(\mathbf{x})_n)
\end{bmatrix}$
Note that the columns of this matrix can be thought of the gradient of the soft-max for a fixed $i=a$ which is a vector using the notation that $\mathbf{e}_i$ is the ith identity vector: $\nabla \sigma(x_a) = \sigma(x_a)(\mathbf{e}_a - \mathbf{\sigma}(\mathbf{x}))$ 
"""

# ╔═╡ 7a6fb1f0-fc3c-4c29-a6d9-769d32ca98a9
md"""
## Example 13.1 Short corridor gridworld
"""

# ╔═╡ 1b89a5be-d4f6-43b6-b778-0895d77d0962
abstract type LinearMove end

# ╔═╡ 759afa53-2b01-4d9b-b398-80120626634f
struct Left <: LinearMove end

# ╔═╡ 97046258-7753-4edb-b0c9-0981d587ad35
struct Right <: LinearMove end

# ╔═╡ 423321cc-1c8c-44a0-bd8e-a4d3cb68962b
function make_corridor()
	function step(s::Integer, a::LinearMove)
		f(s) = ifelse(s == 2, -1, 1) #reverse actions for 2nd state
		move(s::Integer, ::Left) = s - f(s)
		move(s::Integer, ::Right) = s + f(s) 
		s′ = max(1, move(s, a))
		(s′, -1.0)
	end
	(states = 1:3, sterm = 4, actions = [Left(), Right()], step = step)
end	

# ╔═╡ 980af3e7-2f1c-49be-8f6b-fc61271dff52
function run_corridor_episode(π; cor = make_corridor())
	s = first(cor.states)
	G = 0.0
	state_history = [s]
	rewardhistory = Vector{Float64}()
	select_action(vec) = wsample(eachindex(vec), vec)
	action_history = []
	while s != cor.sterm
		a = cor.actions[select_action(π(s))]
		(s, r) = cor.step(s, a)
		s != cor.sterm && push!(state_history, s)
		push!(rewardhistory, r)
		push!(action_history, a)
	end
	return state_history, rewardhistory, action_history
end

# ╔═╡ edb145d7-95e0-44c9-a60f-57d517edb0c7
reduce(hcat, run_corridor_episode(s -> [0.5, 0.5])) #this policy chooses randomly between both actions

# ╔═╡ 23291878-b49d-4626-8313-1e7b2d1f8d44
#statistics on time to exit corridor for random policy
summarystats([run_corridor_episode(s -> [0.5, 0.5]) |> first |> length for _ in 1:10_000])

# ╔═╡ 9d815d9c-6e5a-473e-a395-6f92d504dbf3
md"""
### Exercise 13.1
>*Exercise 13.1* Use your knowledge of the gridworld and its dynamics to determine an *exact* symbolic expression for the optimal probability of selecting the right action in Example 13.1

Example 13.1 is a gridworld with 3 non-terminal states and a terminal state at the far right.  The reward is -1 per step.  States 1 and 3 have actions left/right that move in the expected directions but state 2 reverses the directions.  We use a performance measure $J(\mathbf{\theta}) = v_{\pi_\theta}(S)$.  Given our feature representations of $\mathbf{x}(s, \text{right}) = [1, 0]^{\top}$ and $\mathbf{x}(s, \text{left}) = [0, 1]^{\top}$, we can only learn policies that are stochastic in terms of left/right action selection but do not vary between states.  Also observe that due to probability constraints $p_{\text{right}} = 1 - p_{\text{left}}$. For simplicity, we will use the notation $p \hspace{5px} \dot = \hspace{5px} p_{\text{left}}$.

$\begin{flalign}
v(S_1) &= p \times v(S_1) + (1-p) \times v(S_2) - 1 \tag{1} \\
v(S_2) &= p \times v(S_3) +(1-p) \times v(S_1) - 1 \tag{2} \\
v(S_3) &= p\times v(S_2) - 1 \tag{3}\\
v(S_2) &= p \times [p\times v(S_2) - 1] +(1-p) \times v(S_1) - 1  \tag{substituting 3 into 2} \\
v(S_2) &= \frac{(1-p) \times v(S_1) - (1+p)}{(1+p)(1-p)} \tag{collecting terms} \\
v(S_1) &= p \times v(S_1) + (1-p) \times \left [ \frac{(1-p) \times v(S_1) - (1+p)}{(1+p)(1-p)} \right ] - 1 \tag{substituting above into 1} \\
v(S_1) &= p \times v(S_1) + \left [ \frac{(1-p) \times v(S_1) - (1+p)}{1+p} \right ] - 1 \tag{simplifying} \\
2 &= v(S_1) (-1 + p + \frac{1-p}{1+p}) \tag{collecting v terms} \\
2 &= v(S_1) \frac{(p - 1)(1+p) + 1 - p}{1+p} \tag{combining fractions} \\
2 &= v(S_1) \frac{p^2 - p}{1+p} \tag{simpifying} \\
v(S_1) &= \frac{2(1+p)}{p^2 - p} \\
v(S_2) &= \frac{(1-p) \times [\frac{2(1+p)}{p^2 - p} ] - (1+p)}{(1+p)(1-p)} =  \frac{-\frac{2(1+p)}{p} - (1+p)}{(1+p)(1-p)} = \frac{-2(1+p) - p(1+p)}{p(1+p)(1-p)}\\
&=-\frac{2 + 3p + p^2}{p(1+p)(1-p)} = \frac{(p+2)(p+1)}{p(1+p)(p - 1)} = \frac{p+2}{p(p - 1)} \\
v(S_3) &= p\times \frac{p+2}{p(p - 1)} - 1 = \frac{p+2}{p - 1} - \frac{p-1}{p-1} = \frac{3}{p-1}\\
\end{flalign}$
In order to find the p that maximizes the expected value for state 1, we should differentiate by p and set the result to 0

$\begin{flalign}
0 &= \frac{p(p - 1) - (1+p)(2p - 1)}{(p(p-1))^2} \\
(1+p)(2p - 1) &= p(p - 1) \\
2p - 1 + 2p^2 - p &= p^2 - p \\
2p - 1 + p^2 &= 0 \\
\end{flalign}$

Using the quadratic equation, there are two solutions but since we know p has to be positive we only take that one.

$p = \frac{-2 \pm \sqrt{4 + 4}}{2} = -1 \pm \sqrt{2} \implies p = \sqrt{2} - 1 \approx 0.414$

So, in order to maximize the value at state 1, we have $p_{\text{left}} \approx 0.414$ and $p_{\text{right}} \approx 0.586$.  That also implies that $v(S_1) = \frac{2(1+p)}{p^2 - p} = \frac{2\sqrt{2}}{2 - 2 \sqrt{2} + 1 + 1 - \sqrt{2}} = \frac{2\sqrt{2}}{4 - 3\sqrt{2}} = \frac{2\sqrt{2}(4 + 3\sqrt{2})}{16 - 18}=\frac{8\sqrt{2}+12}{-2} = -6-4\sqrt{2}\approx-11.657$

If we solve the same problem at state 2 we get:

$\begin{flalign}
0 &= \frac{p^2 + 4p - 2}{(p-1)^2p^2} \\
0 &= p^2 + 4p - 2 \\
\end{flalign}$

Using the quadratic equation and keeping only the positive solution gives:
$p = \frac{-4 + \sqrt{16 + 8}}{2} = \frac{-4 + 2\sqrt{6}}{2} = \sqrt{6} - 2 \approx 0.4495$.

So, in order to maximize the value at state 1, we have $p_{\text{left}} \approx 0.4495$ and $p_{\text{right}} \approx 0.55$. Which is different from the value we got for state 1.  So There is a different optimal policy depending on the starting state.  It should be obvious for example that starting in the third state results in an optimial policy of choosing the right action every time.  The value functions for each state are plotted below.  The behavior of V(S3) is not well defined at $p=0$ because for any finite V(S2) it should be 0 but the limit approaching from the right side is -3.  This is because for $p=0$ both V(S1) and V(S2) are not finite and the episode never terminates.   
"""

# ╔═╡ 9c342958-1971-48ec-b919-5dfdcbc915a4
md"""
#### Change Plot Background Color $(@bind bgcolor ColorStringPicker(default = "#121212"))
"""

# ╔═╡ e5faaa1b-88cb-43e2-8d04-8972b58b4bda
begin
	v1(p) = (2*(1+p))/(p^2 - p)
	v2(p) = (p+2)/(p*(p - 1))
	v3(p) = 3/(p-1)
	plist = 0.:0.001:1.
	traces = [scatter(x = plist, y = f.(1 .- plist), name = n) for (f, n) in zip([v1, v2, v3], ["V(S1)", "V(S2)", "V(S3)"])]
	plot(traces, Layout(font_color = "LightGray", plot_bgcolor = bgcolor, paper_bgcolor = "rgb(40, 40, 40)", yaxis_range = [-100, 0], xaxis_title = "probability of right action", yaxis_title = "State Value", width = 900, height = 600))
end

# ╔═╡ 406638af-1e08-44d2-9ee4-97aa9294a94b
md"""
# 13.2 The Policy Gradient Theorem
"""

# ╔═╡ aa450da4-fe84-4eea-b6c4-9820b7982437
md"""
With continuous policy parametrization, we can smoothly very action selection probabilities by arbitrarily small amounts, something that was not possible with ϵ-greedy action selection.  Therefore stronger convergence guarantees are possible for policy-gradient methods than for action-value methods.

In the episodic case, assuming some particular non-random starting state $s_0$, we define the performance of a policy parametrized by *θ* as:

$\begin{align}
J(\mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} v_{\pi_\mathbf{\theta}}(s_0) \tag{13.4}
\end{align}$

where $v_{\pi_\mathbf{\theta}}$ is the true value function for $\pi_\mathbf{\theta}$, the policy determined by $\mathbf{\theta}$.

The *policy gradient theorem* provides an analytic expression for the gradient of performance with respect to the policy parameter that does *not* involve the derivative of the state distribution:

$\begin{align}
\nabla J(\mathbf{\theta}) \propto \sum_s \mu (s) \sum_a q_\pi (s, a) \nabla \pi (a|s,\mathbf{\theta}) \tag{13.5}
\end{align}$

where the gradients are column vectors of partial derivatives with respect to the components of $\mathbf{\theta}$.  In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1.  The distribution here $\mu$ is the on-policy distribution under $\pi$.
"""


# ╔═╡ f924eb30-d1cc-4941-8fb5-ff70ad425ab9
md"""
# 13.3 REINFORCE: Monte Carlo Policy Gradient

If we replace the true action-value function in (13.5) with a learned approximation $\hat q_\pi$, then we have a method called the *all-actions* method because the update involves the sum over all actions.  For the REINFORCE algorithm, we instead sample this value using the actual return and the policy distribution.

We can re-write (13.5) using an expected value under the policy and continue from there:

$\begin{flalign}
\nabla J(\mathbf{\theta}) & \propto \mathbb{E}_\pi \left [ \sum_a q_\pi (S_t, a) \nabla \pi(a|S_t, \mathbf{\theta}) \right ] \tag{13.6}\\
 &= \mathbb{E}_\pi \left [ \sum_a \pi(a|S_t, \mathbf{\theta}) q_\pi (S_t, a) \frac{\nabla \pi(a|S_t, \mathbf{\theta})}{\pi(a|S_t, \mathbf{\theta})} \right ] \tag{multiply and divide by policy} \\
 &= \mathbb{E}_\pi \left [ q_\pi (S_t, A_t) \frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \right ] \tag{replace a with sample under policy} \\
 &= \mathbb{E}_\pi \left [ G_t \frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \right ] \tag{replace value with sample return} \\
\end{flalign}$

Using the expression in the brackets we can write down an update rule for the parameters that can be sampled on each time step.  This is the **REINFORCE update**:

$\begin{align}
\mathbf{\theta}_{t+1} \hspace{5px} \dot = \hspace{5px} \mathbf{\theta}_t + \alpha G_t \frac{\nabla \pi(A_t|S_t, \mathbf{\theta}_t)}{\pi(A_t|S_t, \mathbf{\theta}_t)} \tag{13.8}
\end{align}$

Because it uses all future returns after step t, REINFORCE is a Monte Carlo algorithm and is well defined only for the episodic case.  For implementation purposes we can replace $\frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})}$ with $\nabla \ln \pi(A_t|S_t, \mathbf{\theta}_t)$ which is usually refered to as the *eligibility vector*.
"""

# ╔═╡ b406577a-5478-42fd-8ed0-e36b5574cfc6
select_action(vec) = wsample(eachindex(vec), vec)

# ╔═╡ 2b11ef08-288f-4110-b741-ba580782b6a7
"""
	reinforce_monte_carlo_control(π, ∇lnπ, d, s0, α, step, sterm, actions; 
                             	γ = 1.0, max_episodes = 1000, maxsteps = Inf,
								baseline = 0.0, θ = zeros(d))

Implements the REINFORCE algorithm for Monte Carlo control, which is a policy gradient method for reinforcement learning. Given a function π that maps states to probability distributions over actions, and a function ∇lnπ that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ, this function learns the optimal policy for a given Markov decision process (MDP).

Required arguments:
- π: A function that maps a state to a probability distribution over actions. This function must take two arguments: the current state and the policy parameters θ.
- ∇lnπ: A function that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ. This function must take three arguments: the action, the current state, and the policy parameters θ.
- d: An integer representing the number of policy parameters to be learned.
- s0: The initial state of the MDP.
- α: The learning rate for the policy gradient update.
- step: A function that takes a state and an action and returns the next state and the reward received. This function must take two arguments: the current state and the chosen action.
- sterm: A state representing the terminal state of the MDP.
- actions: A collection of all possible actions in the MDP.

Optional keyword arguments:
- γ: The discount factor for future rewards. Default value is 1.0.
- max_episodes: The maximum number of episodes to run the algorithm. Default value is 1000.
- maxsteps: The maximum number of steps to take in each episode. Default value is Inf.
- baseline: The baseline value for the policy gradient update. Default value is 0.0.
- θ: The initial policy parameters. Default value is a vector of zeros with length d.
"""
function reinforce_monte_carlo_control(π::Function, ∇lnπ::Function, d::Int64, s0, α, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), maxsteps = Inf, baseline = 0.0)
	rewards = zeros(max_episodes)
	
	function run_episode(maxsteps)
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s, r) = step(s0, actions[a])
		reward_history = [r]
		while s != sterm && length(state_history) < maxsteps
			a = select_action(π(s, θ))
			push!(action_history, a)
			(s, r) = step(s, actions[a])
			push!(reward_history, r)
			push!(state_history, s)
		end
		return state_history, action_history, reward_history
	end	
	for i in eachindex(rewards)
		state_history, action_history, reward_history = run_episode(maxsteps)
		G = 0.0
		#iterate through episode beginning at the end
		for i in reverse(eachindex(reward_history))
			G = (γ * G) + reward_history[i]
			θ .+= α * γ^(i-1) * (G - baseline) .* ∇lnπ(action_history[i], state_history[i], θ)
		end
		rewards[i] = sum(reward_history)
	end
	return rewards, θ
end

# ╔═╡ 71973c41-5fbb-40bf-8cc9-e063c7372a1c
"""
    soft_max!(v::AbstractVector, out::AbstractVector)

Calculate the softmax of a vector `v` and store the result in another vector `out` of equal length.

# Arguments
- `v::AbstractVector`: The input vector.
- `out::AbstractVector`: The output vector of equal length as `v`.

# Output
- `out` is modified in-place to contain the softmax of `v`.

# Examples
```julia
julia> v = [1.0, 2.0, 3.0]
3-element Vector{Float64}:
 1.0
 2.0
 3.0

julia> out = similar(v)
3-element Vector{Float64}:
 0.0
 0.0
 0.0

julia> soft_max!(v, out)
3-element Vector{Float64}:
 0.09003057317038046
 0.24472847105479767
 0.6652409557748219
```
"""
function soft_max!(v::AbstractVector, out::AbstractVector)
	out .= exp.(v)
	s = sum(out)
	out .= out ./ s
end

# ╔═╡ cb83e57f-3b3b-44ae-8c75-69b9b12ec6f5
function soft_max!(v::AbstractVector)
	v .= exp.(v)
	s = sum(v)
	v .= v ./ s
end

# ╔═╡ 49a1d508-b491-4d3a-8415-f5def06884e9
"""
    soft_max(v::AbstractVector) -> AbstractVector

Calculate the softmax of a vector `v` and return the result in a new vector.

# Arguments
- `v::AbstractVector`: The input vector.

# Output
- A new vector of the same length as `v`, containing the softmax of `v`.

# Examples
```julia
julia> v = [1.0, 2.0, 3.0]
3-element Vector{Float64}:
 1.0
 2.0
 3.0

julia> out = soft_max(v)
3-element Vector{Float64}:
 0.09003057317038046
 0.24472847105479767
 0.6652409557748219
```
"""
soft_max(v::AbstractVector) = soft_max!(v, similar(v))

# ╔═╡ c6b61679-8a06-47ae-abab-6997ad5cbfea
md"""
Using one hot encoding feature vectors each parameter θ simply represents each state preference, so the policy just takes a softmax of the feature vector to get the action distribution.

$\pi(a | s, \theta) = \sigma(\mathbf{\theta}_{a, s}) = \frac{e^{\theta_{a, s}}}{\sum_{j \in \mathcal{A}}{e^{\theta_{j, s}}}} \forall a \in \mathcal{A}$

The components of the gradient of the softmax function σ is given by:

$\frac{\partial \sigma(\theta_{a, s})}{\partial \theta_{j, s}} = \sigma(\theta)_i(\delta_{ij} - \sigma(\theta)_j)$ 

We can use this expression to get the gradient of the policy output with respect to the parameters:

$\nabla\pi(a| s, \theta) = \sigma(\theta)_a(\delta_{a
j} - \sigma(\theta)_j)$ where we overload the notation for a to also be the index of the selected action

Combining these the *eligibility vector* for one hot encoding and action a is:
$\frac{\nabla \pi(a|s, \mathbf{\theta})}{\pi(a|s, \mathbf{\theta})} = (\delta_{aj} - \sigma(\theta)_j) \forall j$

To calculate this practically, we can use the columns of a one hot matrix who's dimension is dxd and subtract from that the policy vector for state s.
"""

# ╔═╡ c2d8a622-b8f9-454b-9fd1-dc940280624c
function run_corridor_reinforce(;α = 0.0002, θ_0 = [0.0, 0.0], kwargs...)
	features = [1.0 0.0; 0.0 1.0] #feature vectors of length 2 for each action
	avec = zeros(2) #vector to store action output distribution
	e_vec = zeros(2) #storage for eligibility vector

	corridor = make_corridor()

	#we have one parameter for each action
	d = length(corridor.actions)

	#starting state is always 1
	s0 = 1
	
	#policy does not distinguish between states and updates the distribution vector
	π!(s, θ) = soft_max!(θ, avec)
	function ∇lnπ!(a, s, θ)
		π!(s, θ) #fill avec with the appropriate softmax
		#softmax derivative
		for i in eachindex(e_vec)
			e_vec[i] = features[a, i] - avec[i]
		end
		return e_vec
	end

	reinforce_monte_carlo_control(π!, ∇lnπ!, d, s0, α, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), kwargs...)
end

# ╔═╡ 5f91ce14-c9d4-4818-8955-8e7381b4943b
function average_runs(f, n; kwargs...) 
	runs = Vector{Any}(undef, n)
	# for i in 1:n
	@threads for i in 1:n
		Random.seed!(i)
		runs[i] = f(;kwargs...)[1]
	end

	[begin
		v = [r[i] for r in runs]
		cleanv = filter(x -> !isnan(x) && !isinf(x), v)
		if isempty(v)
			-Inf
		else
			mean(cleanv)
		end
	end
	for i in eachindex(first(runs))]
	
	# reduce(+, runs) ./ n
end

# ╔═╡ a45c1930-ad70-44f4-a6bc-10ccb03f65ab
function figure_13_1(αlist; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	traces = [begin
		v = average_runs(run_corridor_reinforce, nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("α = 2^{$(log2(α))}"))
	end
	for α in αlist]

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([traces; baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld", width = 900, height = 600))
end

# ╔═╡ 71c8d422-8177-4324-b048-98dd39198fee
#in the source code used to generate this for the book found here: http://incompleteideas.net/book/code/figure_13_1.py-remove the episodes start with poor performace because the parameter vector is initialized to prefer left with 95% probability
figure_13_1(2.0 .^ [-12, -13, -14]; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ a206c759-3f6e-4003-8cba-5f6ce6742646
md"""
## Figure 13.1
"""

# ╔═╡ 2e6d0374-1c93-48c8-b8ba-dd1a0c682d01
md"""
### Exercise 13.3
> *Exercise 13.3* In Section 13.1 we considered policy parameterizations using the soft-max in action preferences (13.2) with linear action preferences (13.3).  For this parameterization, prove that the eligibility vector is
 
$\begin{flalign}
	\nabla \ln \pi(a|s, \mathbf{\theta}) = \mathbf{x}(s, a) - \sum_b \pi(b|s, \mathbf{\theta}) \mathbf{x}(s, b) \tag{13.9}
\end{flalign}$

> using the definitions and elementary calculus.

$\begin{flalign}
\pi(a|s, \mathbf{\theta}) \hspace{5 px} &\dot = \hspace{5 px} \frac{e^{h(s, a, \mathbf{\theta})}}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{13.2} \\
h(s, a, \mathbf{\theta}) &= \mathbf{\theta}^\top \mathbf{x}(s, a) \tag{13.3}
\end{flalign}$

Working from these definitions we can derive the following:

$\begin{flalign}
\nabla h(s, a, \mathbf{\theta}) &= \mathbf{x}(s, a) \tag{by linearity of h}\\
\ln \pi(a|s, \mathbf{\theta}) &= h(s, a, \mathbf{\theta}) - \ln{\sum_b e^{h(s, b, \mathbf{\theta})}} \\
\nabla \ln \pi(a|s, \mathbf{\theta}) &= \nabla h(s, a, \mathbf{\theta}) - \nabla \ln{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{distributing gradient} \\
&= \mathbf{x}(s, a) - \frac{\sum_b \nabla  e^{h(s, b, \mathbf{\theta})}}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{using chain rule} \\
&= \mathbf{x}(s, a) - \frac{\sum_b e^{h(s, b, \mathbf{\theta})} \mathbf{x}(s, b)}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{using chain rule} \\
&= \mathbf{x}(s, a) - \sum_i \frac{e^{h(s, i, \mathbf{\theta})} \mathbf{x}(s, i)}{\sum_t e^{h(s, t, \mathbf{\theta})}} \forall i \tag{separating fractions} \\
&= \mathbf{x}(s, a) - \sum_i \pi(i|s, \mathbf{\theta}) \mathbf{x}(s, i) \tag{definition of π} \\
\square\\
\end{flalign}$
"""

# ╔═╡ cc45091e-b889-4d5a-9eef-84d80f792046
md"""
# 13.4 REINFORCE with Baseline

The policy gradient theorem (13.5) can be generalized to include a comparison of teh action value to an arbitrary *baseline* b(s):

$\nabla J(\mathbf{\theta}) \propto \sum_s \mu(s)\sum_a\left( q_\pi(s,a)-b(s) \right ) \nabla\pi(a|s,\mathbf{\theta}) \tag{13.10}$

The baseline can be any function, even a random variable, as long as it does not vary with $a$; the euation remains valid because the subtracted quantity is zero:

$\sum_ab(s)\nabla\pi(a|s,\mathbf{\theta})=b(s)\nabla\sum_a\pi(a|s,\mathbf{\theta})=b(s)\nabla1=0$

The policy gradient theorem with baseline (13.10) can be used to derive an update rule using similar steps as in the previous section.  The update rule that we end up with is a new version of REINFORCE that includes a general baseline:

$\mathbf{\theta}_{t+1} \dot = \mathbf{\theta}_t+\alpha(G_t-b(S_t))\frac{\nabla\pi(A_t|S_t,\mathbf{\theta}_t)}{\pi(A_t|S_t,\mathbf{\theta}_t)} \tag{13.11}$

Since the baseline could be uniformly zero, this is a strict generalization of REINFORCE.  To have an effective baseline that depends on state we can use a state value estimate that is also updated with gradient steps: $\hat v(S_t, \mathbf{w})$.  Using such an estimate we can revise the previous REINFORCE algorithm.
"""

# ╔═╡ 5bebef34-e266-4c18-95c3-28e1f1cb4b64
"""
	reinforce_with_baseline_MC_control(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)

Implements the REINFORCE algorithm for Monte Carlo control, which is a policy gradient method for reinforcement learning. Given a function π that maps states to probability distributions over actions, and a function ∇lnπ that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ, this function learns the optimal policy for a given Markov decision process (MDP).  The baseline value is determined by a state value estimator function parametrized by w.

Required arguments:
- π: A function that maps a state to a probability distribution over actions. This function must take two arguments: the current state and the policy parameters θ.
- ∇lnπ: A function that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ. This function must take three arguments: the action, the current state, and the policy parameters θ.
- d: An integer representing the number of policy parameters to be learned.
- s0: The initial state of the MDP.
- α: The learning rate for the policy gradient update.
- step: A function that takes a state and an action and returns the next state and the reward received. This function must take two arguments: the current state and the chosen action.
- sterm: A state representing the terminal state of the MDP.
- actions: A collection of all possible actions in the MDP.

Optional keyword arguments:
- γ: The discount factor for future rewards. Default value is 1.0.
- max_episodes: The maximum number of episodes to run the algorithm. Default value is 1000.
- maxsteps: The maximum number of steps to take in each episode. Default value is Inf.
- baseline: The baseline value for the policy gradient update. Default value is 0.0.
- θ: The initial policy parameters. Default value is a vector of zeros with length d.
"""
function reinforce_with_baseline_MC_control(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)
	rewards = zeros(max_episodes)	
	function run_episode(maxsteps)
		s = s0
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s′, r) = step(s0, actions[a])
		reward_history = [r]
		while s′ != sterm && length(state_history) < maxsteps
			s = s′
			a = select_action(π(s, θ))
			(s′, r) = step(s, actions[a])
			push!(reward_history, r)
			push!(state_history, s)
			push!(action_history, a)
		end
		return state_history, action_history, reward_history
	end	
	for i in eachindex(rewards)
		state_history, action_history, reward_history = run_episode(maxsteps)
		#iterate through episode beginning at the end
		G = 0.0
		for i in reverse(eachindex(reward_history))
			G = (γ * G) + reward_history[i]
			s = state_history[i]
			δ = G - v̂(s, w)
			w .+= αw * δ .* ∇v̂(s, w)
			θ .+= αθ * γ^(i-1) * δ .* ∇lnπ(action_history[i], s, θ)		
		end
		rewards[i] = sum(reward_history)
	end
	return rewards, θ, w
end

# ╔═╡ d26cd4cb-9a62-4a03-8b71-b415c9be79f6
function run_corridor_critic(;αθ = 0.0002, αw = 0.0002, θ_0 = [0.0, 0.0], w_0 = [0.0, 0.0, 0.0, 0.0], f = reinforce_with_baseline_MC_control, kwargs...)
	features = [1.0 0.0; 0.0 1.0] #feature vectors of length 2 for each action
	avec = zeros(2) #vector to store action output distribution
	e_vec = zeros(2) #storage for eligibility vector

	#one hot vectors for each state including 0 for terminal state
	state_features = [[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 0.]]
	
	corridor = make_corridor()

	#we have one parameter for each action
	d = length(corridor.actions)
	d′ = 4

	#starting state is always 1
	s0 = 1
	
	#policy does not distinguish between states and updates the distribution vector
	π!(s, θ) = soft_max!(θ, avec)
	function ∇lnπ!(a, s, θ)
		π!(s, θ) #fill avec with the appropriate softmax
		#softmax derivative
		for i in eachindex(e_vec)
			e_vec[i] = features[a, i] - avec[i]
		end
		return e_vec
	end

	#linear model for state values simply returns the corresponding weight
	v̂(s::Int64, w::AbstractVector) = w[s]
	
	#gradient with respect to weights is just the state vector
	∇v̂(s::Int64, w::AbstractVector) = state_features[s]
	

	# reinforce_monte_carlo_control(π!, ∇lnπ!, d, s0, α, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), kwargs...)

	f(π!, ∇lnπ!, v̂, ∇v̂, d, d′, s0, αθ, αw, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), w = copy(w_0), kwargs...)
end

# ╔═╡ 5f042c7e-45ad-4f8f-94d8-9133e67dd0f6
function figure_13_2(α, αθ, αw; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	trace1 = begin
		v = average_runs(run_corridor_critic, nruns; αθ = αθ, αw = αw, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{with baseline } α^θ = 2^{$(log2(αθ))}, α^w = 2^{$(log2(αw))}"))
	end
	Random.seed!(seed)
	trace2 = begin
		v = average_runs(run_corridor_reinforce, nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{no baseline } α = 2^{$(log2(α))}"))
	end

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([trace1, trace2, baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld",  height = 500))
end

# ╔═╡ b72e030f-7d52-481f-b4f7-2b16b227e547
md"""
### Figure 13.2
"""

# ╔═╡ 87876de4-ca40-4736-81a8-bb26bc273d89
figure_13_2(2.0 ^-13, 2.0 .^ -9, 2.0 ^-3; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ ce33f710-fd9d-4dfa-acda-40204e54d518
md"""
# 13.5 Actor-Critic Methods

Here we also use the value function estimator to calculate the the return estimate using the one step bootstrap return.  When the state value function is used in this way we call it the *critic*.  In general we can use this function with n-step returns and eligibility traces.

The one-step actor-critic method is the analog of the one step methods such as TD(0), Sarsa(0), and Q learning.  These methods replace the full return of REINFORCE with the one step return as follows:

$\begin{flalign}
\mathbf{\theta}_{t+1} &\hspace{5px}   \dot = \hspace{5px} \mathbf{\theta}_t + \alpha(G_{t:t+1} - \hat v(S_t, \mathbf{w}))\ln\nabla\pi(A_t|S_t, \mathbf{\theta_t}) \tag{13.12} \\
& = \mathbf{\theta}_t + \alpha(R_{t+1} + \gamma \hat v(S_{t+1}, \mathbf{w}) - \hat v(S_t, \mathbf{w}))\ln\nabla\pi(A_t|S_t, \mathbf{\theta_t}) \tag{13.13} \\
& = \mathbf{\theta}_t + \delta_t\ln\nabla\pi(A_t|S_t, \mathbf{\theta_t}) \tag{13.14} \\
\end{flalign}$

This can be implemented as a fully online algorithm because we do not have to wait until the end of an episode to calculate return estimates.
"""

# ╔═╡ f4b6f10b-4cd0-4be6-98ec-4d4ffb696392
md"""
## One-step Actor-Critic
"""

# ╔═╡ bf656d55-19cb-4052-baaa-0896ab6d23a0
"""
    one_step_actor_critic(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; 
        γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)

Run the one-step actor-critic algorithm to learn a policy and state-value function for a given environment.

# Arguments
- `π::Function`: a function that maps a state `s` and a parameter vector `θ` to a probability distribution over actions.
- `∇lnπ::Function`: a function that maps an action `a`, a state `s`, and a parameter vector `θ` to the gradient of the log-probability of the action under the policy.
- `v̂::Function`: a function that maps a state `s` and a parameter vector `w` to an estimate of the state-value function.
- `∇v̂::Function`: a function that maps a state `s` and a parameter vector `w` to the gradient of the state-value function estimate.
- `d::Int64`: the dimension of the parameter vector `θ`.
- `d′::Int64`: the dimension of the parameter vector `w`.
- `s0`: the initial state of the environment.
- `αθ`: the step size parameter for updating the policy parameters `θ`.
- `αw`: the step size parameter for updating the state-value function parameters `w`.
- `step`: a function that maps a state `s` and an action `a` to a tuple `(s′, r)` representing the next state and reward.
- `sterm`: the terminal state of the environment.
- `actions`: an array of possible actions in the environment.
- `γ`: the discount factor for future rewards (default: `1.0`).
- `max_episodes`: the maximum number of episodes to run (default: `1000`).
- `θ`: the initial policy parameters (default: `zeros(d)`).
- `w`: the initial state-value function parameters (default: `zeros(d′)`).
- `maxsteps`: the maximum number of steps per episode (default: `Inf`).

# Returns
- `rewards`: an array of length `max_episodes` containing the total rewards obtained in each episode.
- `θ`: the learned policy parameters.
- `w`: the learned state-value function parameters.

# Notes
This function implements the one-step actor-critic algorithm, which updates the policy and state-value function estimates in an online fashion using the gradients of the log-probability and state-value function estimates, respectively, with respect to their parameters. The algorithm uses the eligibility trace method to update the state-value function estimates.
"""
function one_step_actor_critic(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)
	rewards = zeros(max_episodes)	
	function run_episode!(maxsteps)
		I = 1.0
		s = s0
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s′, r) = step(s0, actions[a])
		reward_history = [r]
		δ = r + γ*v̂(s′, w) - v̂(s, w)
		w .+= αw * δ .* ∇v̂(s, w)
		θ .+= αθ * I * δ .* ∇lnπ(a, s, θ)		
		
		while s′ != sterm && length(state_history) < maxsteps
			I = γ*I
			s = s′
			a = select_action(π(s, θ))
			(s′, r) = step(s, actions[a])
			push!(state_history, s)
			push!(action_history, a)
			push!(reward_history, r)
			δ = r + γ*v̂(s′, w) - v̂(s, w)
			w .+= αw * δ .* ∇v̂(s, w)
			θ .+= αθ * I * δ .* ∇lnπ(a, s, θ)		
		end
		return state_history, action_history, reward_history
	end	

	for i in eachindex(rewards)		
		state_history, action_history, reward_history = run_episode!(maxsteps)
		rewards[i] = sum(reward_history)
	end
	return rewards, θ, w
end

# ╔═╡ 4cbdb082-22ba-49e9-a6ed-4380917625ac
md"""
## Actor-Critic with Eligibility Traces
"""

# ╔═╡ 58ad84b0-f9c9-424e-8c05-0b15fbe7b349
function actor_critic_eligibility(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; λθ = 0.0, λw = 0.0, γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf, termination_threshold = (episode = Inf, reward = -Inf), zθ = zeros(size(θ)...), zw = zeros(size(w)...), showprogress = false, get_s0 = () -> s0)
	rewards = zeros(max_episodes)
	#initialize trace vectors
	zθ = zeros(size(θ)...)
	zw = zeros(size(w)...)
	
	function run_episode!(maxsteps)
		I = 1.0
		zθ .= 0.0
		zw .= 0.0
		s0 = get_s0()
		s = s0
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s′, r) = step(s0, actions[a])
		reward_history = [r]
		δ = r + γ*v̂(s′, w) - v̂(s, w)
		zw .= γ*λw .* zw .+ ∇v̂(s, w)
		zθ .= γ*λθ .* zθ .+ I .* ∇lnπ(a, s, θ)
		w .+= αw * δ .* zw
		θ .+= αθ * δ .* zθ		
		
		while s′ != sterm && length(state_history) < maxsteps
			I = γ*I
			s = s′
			a = select_action(π(s, θ))
			(s′, r) = step(s, actions[a])
			push!(state_history, s)
			push!(action_history, a)
			push!(reward_history, r)
			δ = r + γ*v̂(s′, w) - v̂(s, w)
			zw .= γ*λw .* zw .+ ∇v̂(s, w)
			zθ .= γ*λθ .* zθ .+ I .* ∇lnπ(a, s, θ)
			w .+= αw * δ .* zw
			θ .+= αθ * δ .* zθ			
		end
		return state_history, action_history, reward_history
	end	

	function loopfunction!(i)
		state_history, action_history, reward_history = run_episode!(maxsteps)
		rewards[i] = sum(reward_history)
		#end execution early if results don't meet criteria
		if i > termination_threshold.episode && rewards[i] < termination_threshold.reward
			rewards[i+1:end] .= -Inf
			return true
		else
			return false
		end
	end

	if showprogress
		@progress for i in eachindex(rewards)	
			stop = loopfunction!(i)
			stop && break
		end
	else
		for i in eachindex(rewards)	
			stop = loopfunction!(i)
			stop && break
		end
	end
		
	return rewards, θ, w
end

# ╔═╡ 8f11b8dc-2c3e-41a5-8dbb-9af06235fe85
function corridor_actor_critic(α, αθ, αw; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	trace1 = begin
		v = average_runs(run_corridor_critic, nruns; αθ = αθ, αw = αw, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{with baseline } α^θ = 2^{$(log2(αθ))}, α^w = 2^{$(log2(αw))}"))
	end
	Random.seed!(seed)
	trace2 = begin
		# v = average_runs((args...; kwargs...) -> run_corridor_critic(args...; f = one_step_actor_critic, kwargs...), nruns; αθ = αθ, αw = αw, θ_0 = θ_0, kwargs...)
		v = average_runs(run_corridor_critic, nruns; αθ = αθ, αw = αw, θ_0 = θ_0, f = one_step_actor_critic, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{actor-critic } α^θ = 2^{$(log2(αθ))}, α^w = 2^{$(log2(αw))}"))
	end
	Random.seed!(seed)
	trace3 = begin
		v = average_runs(run_corridor_reinforce, nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{no baseline } α = 2^{$(log2(α))}"))
	end

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([trace1, trace2, trace3, baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld",  height = 500))
end

# ╔═╡ 70d4e199-2941-46dd-99c0-0f0520bf976b
corridor_actor_critic(2.0 ^ -13, 2.0 ^ -9, 2.0 ^-3; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ 72900e88-98f4-4879-b005-d79ef6c7ee7f
function corridor_actor_critic_λ(αθ, αw, λlist; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	traces = [begin
		name = latexstring("\\lambda^θ = $λθ, \\lambda^w = $λw")
		v = average_runs(run_corridor_critic, nruns; f = actor_critic_eligibility, θ_0 = θ_0, λθ = λθ, λw = λw, αθ = αθ, αw = αw,  kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = name)
	end
	for λθ in λlist for λw in λlist]

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([traces; baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "Actor-Critic λ Comparison short-Corridor gridworld",  height = 500))
end

# ╔═╡ 511a847f-234c-465e-8f4a-688e79d9b975
md"""
# 13.6 Policy Gradient for Continuing Problems

In the continuing case we need to define the average reward per time step as discussed in Section 10.3.  In the update procedure the δ is calculated differently in terms of the reward compared to this long running average.  The value functions in this case will also learn the reward difference from the average which is assumed to have a well defined expected value under the stationary state distribution for the policy.  This shift in the value function will not affect performance since shifting the value function up and down by a constant does not affect the learned policy.  To implement this we need a new learning rate αr which controls how quickly the reward average updates.  This replaces γ in a sense since we no longer discount rewards of future time steps.
"""

# ╔═╡ 533cbf4b-ac14-47eb-98cf-e569f32cc215
function actor_critic_eligibility_continuing(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0::S, αθ, αw, αR, step, actions::AbstractVector{A}; λθ = 0.0, λw = 0.0, maxsteps = 1000, θ = zeros(d), w = zeros(d′)) where {S, A}
	state_history = Vector{S}(undef, maxsteps+1)
	action_history = Vector{A}(undef, maxsteps+1)
	reward_history = zeros(maxsteps+1)
	
	#initialize trace vectors
	zθ = zeros(d)
	zw = zeros(d′)

	#fill in first step history
	state_history[1] = s0

	#initialize reward average
	r̄ = 0.0
	
	s = s0
	state_history = [s0]
	a = select_action(π(s0, θ))
	action_history = [a]
	(s′, r) = step(s0, actions[a])
	reward_history = [r]
	δ = r + γ*v̂(s′, w) - v̂(s, w)
	zw .= γ*λw .* zw .+ ∇v̂(s, w)
	zθ .= γ*λθ .* zθ .+ I .* ∇lnπ(a, s, θ)
	w .+= αw * δ .* zw
	θ .+= αθ * δ .* zθ		
	
	for i in 2:maxsteps+1
		s = state_history[i-1]
		a = select_action(π(s, θ))
		action_history[i-1] = a
		(s′, r) = step(s, actions[a])
		reward_history[i-1] = r
		state_history[i] = s′
		
		δ = r - r̄ + v̂(s′, w) - v̂(s, w)
		r̄ += αR * δ
		zw .= γ*λw .* zw .+ ∇v̂(s, w)
		zθ .= γ*λθ .* zθ .+ ∇lnπ(a, s, θ)
		w .+= αw * δ .* zw
		θ .+= αθ * δ .* zθ			
	end
	
	return reward_history, state_history, action_history, θ, w
end

# ╔═╡ 735b548a-88f5-4a30-ab8f-dfb3d6401b2b
md"""
# 13.7 Policy Parameterization for Continuous Actions

With a parameterized policy we are to learn statistics of teh distribution that selects actions.  As a foundation consider the normal distribution:

$p(x) \hspace{5px} \dot = \hspace{5px} \frac{1}{\sigma \sqrt{2\pi}} \exp \left ( - \frac{(x-\mu)^2}{2\sigma^2} \right ) \tag{13.18}$
"""

# ╔═╡ 79c85707-ea09-4f6b-ad51-a2683c3923c0
let x = LinRange(-5, 5, 10_000)
	traces = [scatter(x = x, y = pdf.(Normal(0.0, σ), x), name = latexstring("\\sigma^2 = $(round(σ^2, sigdigits = 2))")) for σ in sqrt.([0.2, 1.0, 0.5, 5.0])]
	plot(traces, Layout(xaxis_title = "x", title = "Normal Distribution N(μ, σ)"))
end

# ╔═╡ 7ccadf01-fbba-4dfd-a5ad-770dab9946f9
md"""
We can define our policy as a normal distribution function over actions for a given state and parameter vector.

$\pi(a|s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \frac{1}{\sigma(s, \mathbf{\theta}) \sqrt{2\pi}} \exp \left ( - \frac{(a-\mu(s, \mathbf{\theta}))^2}{2\sigma(s, \mathbf{\theta})^2} \right ) \tag{13.19}$

This policy requires μ and σ to be parameterized by the parameter vector.  To make a linear model for both parameters we can use the following formulas:

$\mu(s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \mathbf{\theta}_\mu ^\top \mathbf{x}_\mu(s) \text{ and } \sigma(s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \exp{( \mathbf{\theta}_\sigma ^ \top \mathbf{x}_\sigma (s))} \tag{13.20}$

where $\mathbf{x}_\mu(s)$ and $\mathbf{x}_\sigma(s)$ are state feature vectors.  With these formulas we can apply the previous algorithms to solve environments with real-valued actions. 
"""

# ╔═╡ beb01fb8-c77d-4b5c-a66d-3812415e04a3
md"""
### Exercise 13.4
> *Exercise 13.4* For the Gaussian policy parameterization, derive the formula for the eligibility vector $\nabla \ln{\pi(a|s, \mathbf{\theta})}$

Starting with our expression for the parameter function, we can calculate the gradient: 

$\nabla \pi(a|s, \mathbf{\theta}) = \nabla \left ( \frac{1}{\sigma(s, \mathbf{\theta}) \sqrt{2\pi}} \exp \left ( - \frac{(a-\mu(s, \mathbf{\theta}))^2}{2\sigma(s, \mathbf{\theta})^2} \right ) \right )$

We will eventually need $\nabla \mu$ and $\nabla \sigma$ so let's calculate them now.

$\nabla (\sigma(s, \mathbf{\theta})) = \nabla \exp{( \mathbf{\theta}_\sigma ^ \top \mathbf{x}_\sigma (s))} = \sigma(s, \mathbf{\theta})\mathbf{x}_\sigma (s)$

$\nabla(\mu(s, \mathbf{\theta})) = \nabla ( \mathbf{\theta}_\mu ^\top \mathbf{x}_\mu(s)) = \mathbf{x}_\mu (s)$


The first application of the quotient rule is trivial, I will omit the input arguments to μ and σ keeping in mind that these are functions of the parameters.  Also let $\left ( - \frac{(a-\mu)^2}{2\sigma^2} \right ) = f(\mu, \sigma)$ which results in $\pi(a|s, \mathbf{\theta}) =  \frac{1}{\sigma \sqrt{2\pi}} \exp{(f(\mu, \sigma))}$.  Therefore:

$\begin{flalign}
\nabla \pi(a|s, \mathbf{\theta}) \sqrt{2\pi} &= \frac{1}{\sigma ^2} \left (- \exp{(f(\mu, \sigma))} \nabla \sigma + \sigma \exp{(f(\mu, \sigma))}\nabla f(\mu, \sigma) \right ) \\
&= \frac{1}{\sigma ^2} \left ( -\exp{(f(\mu, \sigma))} \sigma\mathbf{x}_\sigma + \sigma \exp{(f(\mu, \sigma))}\nabla f(\mu, \sigma) \right ) \\
&=\frac{\exp{(f(\mu, \sigma))}}{\sigma} \left (-\mathbf{x}_\sigma + \nabla f(\mu, \sigma) \right ) \\
\end{flalign}$

Now we need only calculate the gradient of $f$:

$\begin{flalign}
\nabla f(\mu, \sigma) &= \frac{-1}{2} \nabla \left [ \frac{(a-\mu)^2}{\sigma^2} \right ] \\
& = \frac{-1}{2\sigma^4} \left [-2 \sigma^2 (a - \mu) \nabla \mu - (a - \mu)^2 2\sigma \nabla \sigma \right ] \\
& = \frac{-1}{\sigma^3} \left [ -\sigma (a - \mu) \nabla \mu - (a - \mu)^2 \nabla \sigma \right ] \\
& = \frac{-1}{\sigma^3} \left [ -\sigma (a - \mu) \mathbf{x}_\mu (s) - (a - \mu)^2 \sigma \mathbf{x}_\sigma \right ] \tag{substituting gradients}\\
& = \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu) \tag{simplifying}\\
\end{flalign}$

Now substitute this back into the policy gradient:

$\nabla \pi(a|s, \mathbf{\theta}) \sqrt{2\pi} = \frac{\exp{(f(\mu, \sigma))}}{\sigma} \left (-\mathbf{x}_\sigma + \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu) \right )$

Furthermore, observe that $\pi(a|s, \mathbf{\theta}) = \frac{1}{\sigma\sqrt{2\pi}} \exp(f(\mu, \sigma))$

So our expression for the policy gradient is:

$\nabla \pi(a|s, \mathbf{\theta}) = \pi(a|s, \mathbf{\theta}) \left (-\mathbf{x}_\sigma + \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu) \right )$

To get the eligibility vector we must divide this by the policy which is conveniently already in the expression:

$\begin{flalign}
\frac{\nabla \pi(a|s, \mathbf{\theta})}{\pi(a|s, \mathbf{\theta})} &= -\mathbf{x}_\sigma + \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu)\\
&= \mathbf{x}_\mu \left [ \frac{(a - \mu)}{\sigma^2} \right ] + \mathbf{x}_\sigma \left [\frac{(a-\mu)^2}{\sigma^2} -1 \right ] \\
\end{flalign}$

There are two components to the sum, one for $\mu$ and one for $\sigma$.  If we think of the paramters and feature vectors as concatenated, then this sum would be an element by element sum where $\mathbf{x}_\mu$ has a zero value for all the feature indices corresponding to $\sigma$ and vice-versa.  This way doing the sum will form one complete vector that has gradient components for all the parameters $\mathbf{\theta}_\mu$ and $\mathbf{\theta}_\sigma$.  Alternatively, the sum can be separated and each gradient can be treated separately with only those components keeping them separated throughout the calculation.
"""

# ╔═╡ 68e6f17e-8c87-40f0-a673-1115ecd1b71d
md"""
### Exercise 13.5
> *Exercise 13.5* A *Bernoulli-logistic unit* is a stochastic neuron-like unit used in some ANNs.  Its input at time *t* is a feature vector $\mathbf{x}(S_t)$; its output, $A_t$, is a random variable having two values, 0 and 1, with $/Pr{A_t=1}=P_t$ and $/Pr{A_t=0}=1-P_t$ (the Bernoulli distribution).  Let $h(s, 0, \mathbf{\theta})$ and $h(s, 1, \mathbf{\theta})$ be the preferences in state $s$ for the unit's two actions given by policy parameter $\mathbf{\theta}$.  Assume that the difference between the action preferences is given by a weights sum of teh unit's input vector, that is, assume that $h(s, 1, \mathbf{\theta})-h(s,0, \mathbf{\theta}) = \mathbf{\theta}^\top \mathbf{x}(s)$, where $\mathbf{\theta}$ is the unit's weight vector.
> 1. Show that if the exponential soft-max distribution (13.2) is used to convert action preferences to policies, then ${P_t = \pi(1|S_t, \theta_t)=1/(1+\exp(-\theta_t^\top\mathbf{x}(S_t)))}$ (the logistic function). 
> 2. What is the Monte-Carlo REINFORCE update of $\theta_t$ to $\theta_{t+1}$ upon receipt of return $G_t$?
> 3. Express the eligility $\nabla \ln \pi(a|s, \theta)$ for a Bernoulli-logistic unit, in terms of $a$, $\mathbf{x}(s)$, and $\pi(a|s, \theta)$ by calculating the gradient.
> Hint for part (c): Define $P=\pi(1|s,\theta)$ and compute the derivative of the logarithm, for each action, using the chain rule on $P$.  Combine the two results into one expression that depends on $a$ and $P$, and then use the chain rule again, this time on $\theta^\top\mathbf{x}(s)$, noting that the derivative of the logistic function $f(x)=1/(1+e^{-x})$ is $f(x)(1-f(x))$.
"""

# ╔═╡ 692c1043-4eaf-491e-b8fe-368618867f99
md"""
1. The soft-max distribution is: 
$\sigma(a|s, \theta) = \frac{e^{h(s, a, \theta)}}{\sum_b e^{h(s, b, \theta)}}$ 

We only have two possible actions in each state so the policy for action 1 would be given by: 

$\pi(1|S_t, \theta_t) = \frac{e^{h(s, 1, \theta_t)}}{e^{h(S_t, 0, \theta_t)} + e^{h(S_t, 1, \theta)}}$ 

Simplify this expression by dividing by $e^{h(s, 1, \theta_t)}$ which results in: 

$\pi(1|S_t, \theta_t) = \frac{1}{e^{h(S_t, 0, \theta_t) - h(S_t, 1, \theta_t)} + 1}$ 

Given the assumption that $h(s, 1, \theta)-h(s, 0, \theta) = \theta^\top\mathbf{x}(s)$, we replace the expression in the exponent resulting in the final expression of: 

$\pi(1|S_t, \theta_t) = \frac{1}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1}$

Using the notation $f(x) = 1/(1+e^{-x})$ we can write $\pi(1|S_t, \theta_t) = f(\theta_t^\top \mathbf{x}(S_t))$ where $f$ is the logistic function.  Consider this notation for the rest of the exercises.

2. The REINFORCE update is given by: $\theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla\pi(A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)}$, so we need to compute the gradient of the policy in terms of the parameters for this action selection: $\nabla \pi(1|S_t, \theta_t)$.  Luckily, the derivative of the logistic function is simply given by: $f(x)(1-f(x))$ where $f(x)$ is the logistic function itself.  In our case $x = \theta_t^\top \mathbf{x}_t$  so after applying the chain rule we have: 

$\nabla\pi(1|S_t, \theta_t) = f(x)(1-f(x))\nabla x = f(x)(1-f(x)) \mathbf{x_t}$ since $x$ is just a linear function of the parameters.  So for the parameter update step we have: 

$\frac{\nabla\pi(1|S_t, \theta_t)}{\pi(1|S_t, \theta_t)} = \frac{f(x)(1-f(x))\mathbf{x}_t}{f(x)} = (1 - f(x))\mathbf{x}_t$

Also note that:

$1 - f(x) = 1 - \frac{1}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1} = \frac{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1 - 1}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1} = \frac{e^{-\theta_t^\top\mathbf{x}(S_t)}}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1}$

The REINFORCE update will then be: 

$\theta_{t+1} = \theta_t + \alpha G_t \left ( \frac{e^{-\theta_t^\top\mathbf{x}(S_t)}}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1} \right ) \mathbf{x}_t$

3. For the general case, we want to calculate $\frac{\nabla\pi(a|s, \theta)}{\pi(a|s, \theta)}$.  We already know this expression for $a = 1$.   

$\nabla {\pi(1|s, \mathbf{\theta})} = f(x)(1 - f(x))\mathbf{x}(s) = \pi(1|s, \mathbf{\theta})(1 - \pi(1|s, \mathbf{\theta})\mathbf{x}(s)$

Since $\pi(a|s, \theta)$ is a probability distribution across actions, we also know that 

$\pi(0|s, \theta) = 1 - \pi(1|s, \theta)$ 

which implies that 

$\nabla \pi(0|s, \theta) = -\nabla \pi(1|s, \theta) = -\pi(1|s, \mathbf{\theta})(1 - \pi(1|s, \mathbf{\theta}))\mathbf{x}(s)$ 

We can express this in terms of $\pi(0|s, \theta)$ completely:

$\nabla \pi(0|s, \theta) = (\pi(0|s, \mathbf{\theta}) - 1)\pi(0|s, \theta)\mathbf{x}(s) = -\pi(0|s, \theta)(1 - \pi(0|s, \mathbf{\theta}))\mathbf{x}(s)$ 

Let's now compare the two expressions for the policy gradient at each action:

$\begin{align}
\nabla {\pi(1|s, \mathbf{\theta})} &= \pi(1|s, \mathbf{\theta})(1 - \pi(1|s, \mathbf{\theta})\mathbf{x}(s) \\
\nabla \pi(0|s, \theta) &= -\pi(0|s, \theta)(1 - \pi(0|s, \mathbf{\theta}))\mathbf{x}(s) \\
\therefore \\
\nabla \pi(a|s, \theta) &= \chi (a) \pi(a|s, \theta)(1 - \pi(a|s, \mathbf{\theta}))\mathbf{x}(s) \\
\end{align}$

Where $\chi (a)$ is a function that returns 1 for $a=1$ and -1 for $a=0$.  There are many ways to achieve this but the following expression is simple and works: $\chi(a) = 2a - 1$.  Dividing by the gradient yields a unified expression for the eligibility vector:

$\nabla \ln{\pi(a|s,\theta)} = (2a - 1) (1 - \pi(a|s, \mathbf{\theta}))\mathbf{x}(s)$ 
"""

# ╔═╡ 4c34640f-efa2-4e1d-8a70-0acd2ce45428
md"""
# Bonus Problems: Comparing Techniques
Consider the case of applying the techniques in this chapter to problems where we choose feature vectors and parameters to effectively compute the tabular case.  That is we enumerate every state and state/action pair.  Our parameters for each function will store a single value for each case.  Let's consider the gradients for both the state-value estimate and the policy.  We will use two sets of parameters: $\mathbf{w}$ and $\mathbf{\theta}$.  $\mathbf{w}_s$ is the parameter for state s and $\mathbf{\theta}_{s, a}$ is the parameter for state/action pair $(s, a)$.  Using this notation $\mathbf{w}$ is a vector and $\theta$ is a matrix.

Starting with the state-value function:

$\begin{align}
\hat v(s, \mathbf{w}) &= \mathbf{w}_s \\
\nabla v(s, \mathbf{w}) &= \nabla \mathbf{w}_s \\
&= \mathbf{e}_s
\end{align}$

where $\mathbf{e}_s$ is the one-hot vector for index s and length equal to the number of states.

Now moving on to the policy, we will use a soft-max function to convert action preferences into probabilities.

$\begin{align}
\pi(a|s, \theta) &= \frac{\exp{\theta_{s, a}}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \\
\nabla \pi(a|s, \theta) &=  \nabla \frac{\exp{\theta_{s, a}}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \\
\end{align}$

But we already calculated the gradient of the soft-max function of a vector $\mathbf{x}$.  

$\nabla\sigma(\mathbf{x})_{i, j} = \sigma(\mathbf{x})_i  \left ( \delta_{i, j}    -   \sigma(\mathbf{x})_j \right )$

Comparing to what we desire, $\mathbf{x} = \mathbf{\theta}_s$ which is the parameter vector for the state s and $\sigma = \pi$.  So we can immediately write down the components of this gradient:

$\begin{align}
\nabla \pi(a|\theta_s)_i &= \pi(a|\theta_s) \left (\delta_{a, i} - \pi(i|\theta_s) \right ) \\
\frac{\nabla \pi(a|\theta_s)_i}{\pi(a|\theta_s)} = \nabla \ln \pi(a|\theta_s)_i &= \left (\delta_{a, i} - \pi(i|\theta_s) \right ) \\
\end{align}$

$\begin{equation}
\nabla \ln{\pi(a|\theta_s)}_i =
\begin{cases}
-\pi(i|\theta_s) & i \neq a \\
1 - \pi(i|\theta_s) & i = a
\end{cases}
\end{equation}$

This is a gradient vector which corresponds to the components of $\theta_s$ which is the parameter vector for each action at that state.  We have a new vector update for each unique state/action pair observed, but once those two are fixed the number of components that need to be calculated is just a vector with a length equal to the number of actions.
"""

# ╔═╡ 273e7735-91a6-45cd-81ad-49d0da665143
md"""
## Value Iteration Review

For this method we need an MDP defined more thoroughly than normal for sampling methods.  In particular we need a probability transition function that returns a probability for each set of (s′, r, s, a).  To perform calculations with this function it will be convenient to have lookup tables for the available actions in each state as well as all possible transitions from a given state/action pair.
"""

# ╔═╡ 8a4e2b43-15fe-49c4-a487-497875246f82
#p is the state transition function for an mdp which maps the 4 arguments to a probability.  This function uses p to generate two dictionaries.  The first maps each state to a set of possible actions in that state.  The second maps each state/action pair to a set of possible transition/reward pairs
function get_sa_keys(p::Dict{Tuple{A, B, A, C}, T}) where {T <: Real, A, B, C}
	#map from states to a list of possible actions
	state_actions = Dict{A, Set{C}}()

	#map from state action pairs to a list of possible newstate/reward pairs
	sa_s′rewards = Dict{Tuple{A, C}, Set{Tuple{A, B}}}()
	for k in keys(p)
		(s′, r, s, a) = k
		haskey(state_actions, s) ? push!(state_actions[s], a) : state_actions[s] = Set([a])
		haskey(sa_s′rewards, (s,a)) ? push!(sa_s′rewards[(s,a)], (s′, r)) : sa_s′rewards[(s,a)] = Set([(s′,r)])
	end
	return state_actions, sa_s′rewards
end	

# ╔═╡ ac43b613-5c74-45bd-a49e-5b30bb19f52d
function bellman_optimal_value!(V::Dict{S, Float64}, p::Dict{Tuple{S, Float64, S, A}, Float64}, sa_keys::Tuple{Dict{S, Set{A}}, Dict{Tuple{S, A}, Set{Tuple{S, Float64}}}}, γ::Float64; invert_state = s -> 1.0) where {S, A}
	delt = 0.0
	calcvalue(s′, r, s, a) = p[(s′,r,s,a)] * (r + γ*V[s′])
	sumvalue(list, s, a) = sum(calcvalue(s′, r, s, a) for (s′, r) in list)
	function updatestate(s, delt)
		c = invert_state(s)
		v = V[s]
		actions = first(sa_keys)[s]
		V[s] = c*maximum(c*sumvalue(sa_keys[2][(s,a)], s, a) for a in actions; init = -Inf)
		delt = max(delt, abs(v - V[s]))
	end

	for s in keys(first(sa_keys))
		delt = updatestate(s, delt)
	end
	return delt
end

# ╔═╡ 461e27bb-c38b-4dc6-aa68-d5d76ff79cbf
function calculatepolicy(mdp::NamedTuple, γ::Float64, V::Dict; invert_state = s -> 1.0)
	(p, sa_keys) = mdp
	calcvalue(s′, r, s, a) = p[(s′,r,s,a)] * (r + γ*V[s′])
	sumvalue(list, s, a) = sum(calcvalue(s′, r, s, a) for (s′, r) in list)
	makenewdist(s, actions) = Dict(a => invert_state(s)*sumvalue(sa_keys[2][(s, a)], s, a) for a in actions)
	function getpair(s)
		actions = sa_keys[1][s]
		newdist = makenewdist(s, actions)
		return (s, newdist)
	end
	πraw = Dict(getpair(s) for s in keys(sa_keys[1]))
	πstar = Dict(s => Dict(argmax(πraw[s]) => 1.0) for s in keys(πraw))
	πstar, πraw
end 

# ╔═╡ 1683b216-d310-4c66-81ba-0329898d90dd
function value_iteration_v(θ::Real, mdp::NamedTuple, γ::Float64, V::T, delt::Float64, nmax::Real, valuelist::AbstractVector{T}; kwargs...) where T <: Dict
	(p, sa_keys) = mdp
	if nmax <= 0 || delt <= θ
		(πstar, πraw) = calculatepolicy(mdp, γ, V; kwargs...)
		return (valuelist, πstar, πraw)
	else 
		newV = copy(V)
		delt = bellman_optimal_value!(newV, p, sa_keys, γ; kwargs...)
		newlist = [valuelist; [newV]]
		value_iteration_v(θ, mdp, γ, newV, delt, nmax - 1, newlist; kwargs...)	
	end
end

# ╔═╡ d538939d-df32-4766-b3c7-f9fc5af564df
function value_iteration_v!(θ::Real, mdp::NamedTuple, γ::Float64, V::T, delt::Float64, nmax::Real; kwargs...) where T <: Dict
	(p, sa_keys) = mdp
	if nmax <= 0 || delt <= θ
		(πstar, πraw) = calculatepolicy(mdp, γ, V; kwargs...)
		return (V, πstar, πraw)
	else 
		delt = bellman_optimal_value!(V, p, sa_keys, γ; kwargs...)
		value_iteration_v!(θ, mdp, γ, V, delt, nmax - 1; kwargs...)	
	end
end

# ╔═╡ ee23064b-499a-4061-bfed-242ccbcbf25e
#take a policy calculation over values for a given state and return a probability distribution for the greedy policy
function convertπs(πs::Dict)
	m = maximum(values(πs))
	c = count(a == m for a in values(πs))
	p = 1. / c
	Dict(a => πs[a] == m ? p : 0.0 for a in keys(πs))
end

# ╔═╡ 3825159e-a5db-45c8-b2bc-193b4494b53d
convertπ(π::Dict) = Dict(s => convertπs(π[s]) for s in keys(π))

# ╔═╡ ed2785a8-0fed-4052-8371-0e34982e8800
function begin_value_iteration_v(θ, γ, mdp, V, nmax; kwargs...)
	(p, sa_keys) = mdp
	newV = copy(V)
	delt = bellman_optimal_value!(newV, p, sa_keys, γ; kwargs...)
	value_iteration_v(θ, mdp, γ, newV, delt, nmax-1, [V, newV]; kwargs...)
end

# ╔═╡ f5745c5a-8dd9-4827-a222-df4036498a0e
function begin_value_iteration_v!(θ, γ, mdp, V, nmax; kwargs...)
	(p, sa_keys) = mdp
	delt = bellman_optimal_value!(V, p, sa_keys, γ; kwargs...)
	value_iteration_v!(θ, mdp, γ, V, delt, nmax-1; kwargs...)
end

# ╔═╡ a81d2380-b853-432e-9592-d5461daad7b2
function begin_value_iteration_v(mdp::NamedTuple, γ; θ = eps(0.0), nmax=Inf, Vinit = 0.0, kwargs...)
	#initialize value at a constant
	V = Dict(s => Vinit for s in keys(mdp[2][1]))
	f = if savelist
		begin_value_iteration_v
	else
		begin_value_iteration_v!
	end
	f(θ, γ, mdp, V, nmax; kwargs...)
end

# ╔═╡ fc68dd3e-e42d-4642-a5ba-bac9ba1b432d
#for an episodic task add a terminal state that will remain at 0 value
function begin_value_iteration_v(mdp::NamedTuple, sterm, γ::Real; θ = eps(0.0), nmax=Inf, Vinit = 0.0, savelist = true, kwargs...)
	#initialize value at a constant
	V = Dict(s => Vinit for s in keys(mdp[2][1]))
	V[sterm] = 0.0
	f = if savelist
		begin_value_iteration_v
	else
		begin_value_iteration_v!
	end
	f(θ, γ, mdp, V, nmax; kwargs...)
end

# ╔═╡ a7316ca6-28ae-4ee0-b0be-e8d451beb17f
# this is for continuing value iteration from some existing value function
begin_value_iteration_v(mdp::NamedTuple, γ::Real, V; θ = eps(0.0), nmax=Inf, kwargs...) = begin_value_iteration_v(θ, γ, mdp, V, nmax; kwargs...)

# ╔═╡ 5ce1af6b-847c-47f0-a6ca-867c35948caa
md"""
## Racetrack Environment
"""

# ╔═╡ ff60f48e-2055-4bb6-8cf4-fac1da45200b
const racetrack_velocities = [(vx, vy) for vx in 0:4 for vy in 0:4]

# ╔═╡ a79ed238-a6d3-40e6-9bf3-351b7494b446
const racetrack_actions = [(dx, dy) for dx in -1:1 for dy in -1:1]

# ╔═╡ 76af787d-7a3d-4c65-ab6a-898fba148705
#track is defined as a set of points for each of the start, body, and finish
const track1 = (  start = Set((x, 0) for x in 0:5), 
            finish = Set((13, y) for y in 26:31), 
            body = union(   Set((x, y) for x in 0:5 for y in 1:2),
                            Set((x, y) for x in -1:5 for y in 3:9),
                            Set((x, y) for x in -2:5 for y in 10:17),
                            Set((x, y) for x in -3:5 for y in 18:24),
                            Set((x, 25) for x in -3:6),
                            Set((x, y) for x in -3:12 for y in 26:27),
                            Set((x, 28) for x in -2:12),
                            Set((x, y) for x in -1:12 for y in 29:30),
                            Set((x, 31) for x in 0:12))
)

# ╔═╡ 0538aaf4-716b-4f3c-aa7e-dcb1dd456172
#given a position, velocity, and action takes a forward step in time and returns the new position, new velocity, and a set of points that represent the space covered in between
function project_path(p, v, a)
    (vx, vy) = v
    (dx, dy) = a

    vxnew = clamp(vx + dx, 0, 4)
    vynew = clamp(vy + dy, 0, 4)

    #ensure that the updated velocities are not 0
    if vxnew + vynew == 0
        if iseven(p[1] + p[2])
            vxnew += 1
        else
            vynew += 1
        end
    end

    #position the car ends up at
    pnew = (p[1] + vxnew, p[2] + vynew)

    #how to check if the path intersects the finish line or the boundary?  Form a square from vxnew and vynew and see if the off-track area or finish line is contained in that square
    pathsquares = Set((x, y) for x in p[1]:pnew[1] for y in p[2]:pnew[2])

    (pnew, (vxnew, vynew), pathsquares)
end

# ╔═╡ d37e21ac-b82a-423f-8719-b513bddf433d
function make_racetrack(track)
	positions = mapreduce(a -> collect(a), vcat, track)
	states = [(position = p, velocity = v) for p in positions for v in racetrack_velocities]

	sterm = (position = (-1, -1), velocity = (0, 0))

	#take a forward step from current state returning new state and reward of -1
	function step(s, a)
		s == sterm && return (sterm, 0.0)
		pnew, vnew, psquare = project_path(s.position, s.velocity, a)
		fsquares = intersect(psquare, track.finish)
		outsquares = setdiff(psquare, track.body, track.start)
		s′ = if !isempty(fsquares) #car finished race
			# println("Finished race")
			sterm
		elseif !isempty(outsquares) #car path went outside of track
			# println("car reset")
			(position = rand(track.start), velocity = (0, 0))
		else
			(position = pnew, velocity = vnew)
		end
		# println("starting state: $s, ending state: $s′")
		(s′, -1.0)
	end	

	s0 = (position = first(track.start), velocity = (0, 0))

	function runepisode(π; s0 = s0)
		traj = [s0]
		rewards = Vector{Float64}()
		s = s0
		while true
			(s, r) = step(s, π(s))
			push!(rewards, r)
			(s == sterm) && break
			push!(traj, s)
		end
		return traj, rewards
	end

	(states, sterm, racetrack_actions, step, s0, runepisode)
end

# ╔═╡ d314361e-4d4f-413b-b935-1e88c1112fa0
function setup_racetrack_actor_critic(track)
	(states, sterm, actions, step) = make_racetrack(track)
	s0 = (position = first(track.start), velocity = (0, 0))

	state_action_pairs = [(s, a) for s in states for a in actions]

	#convert states to index
	statelookup = Dict(zip(states, eachindex(states)))
	statelookup[sterm] = lastindex(states) + 1

	#create state feature vectors, leave the terminal state at all zeros
	xs = [zeros(lastindex(states)+1) for i in 1:(lastindex(states)+1)]
	for i in eachindex(states)
		xs[i][i] = 1.0
	end
	
	#allocations for outputs
	πoutput = zeros(lastindex(actions))
	gradoutput = zeros(lastindex(states)+1, lastindex(actions))

	#value function and gradient
	v̂(s, w) = w[statelookup[s]]
	∇v̂(s, w) = xs[statelookup[s]]

	function clean_output!(v::AbstractVector{T}) where T <: AbstractFloat
		for (i, x) in enumerate(v)
			if isnan(x) || isinf(x)
				v[i] = zero(T)
			end
		end
		return v
	end

	#policy function and gradient
	function π!(s, θ) 
		soft_max!(θ[statelookup[s], :], πoutput)
		clean_output!(πoutput)
	end

	function ∇lnπ!(a, s, θ)
		#ensure πoutput contains the current softmax output for this state
		# π!(s, θ)
		i = statelookup[s]
		 for n in eachindex(actions)
			@inbounds @simd for m in eachindex(states)
				gradoutput[m, n] = (i == m) * ((n == a) - πoutput[n])
				# if i == m
				# 	println("At state $i Updated gradient of $(gradoutput[m, n])")
				# end
			end
		end
		return gradoutput
	end

	#parameters
	θ = zeros(lastindex(states)+1, lastindex(actions))
	w = zeros(lastindex(states)+1)
	
	return (π!, ∇lnπ!, v̂, ∇v̂, s0, step, states, sterm, actions, θ, w)
end

# ╔═╡ 9040a58b-afd7-49cd-a253-054a5b26c603
track1_setup = setup_racetrack_actor_critic(track1)

# ╔═╡ e5a0a3fc-2eb3-4f31-8ab6-4a3130c70932
function execute_racetrack_actor_critic(track, setup, αθ, αw; kwargs...)
	(π!, ∇lnπ!, v̂, ∇v̂, s0, step, states, sterm, actions, θ, w) = setup
	#parameters
	θ .= zeros(lastindex(states)+1, lastindex(actions))
	w .= zeros(lastindex(states)+1)
	
	# reinforce_monte_carlo_control(π!, ∇lnπ!, length(θ), s0, αθ, step, sterm, actions; θ = θ, kwargs...)

	actor_critic_eligibility(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)

	# one_step_actor_critic(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)
end

# ╔═╡ 85fc29c9-e5ca-4bc8-b607-51d75906a1f2
# ╠═╡ show_logs = false
function eval_racetrack(track; nruns = nthreads(), αlist = 2. .^(-3:-1), λlist = [0.0, 0.1, 0.2, 0.4, 0.8, 1.0], kwargs...)
	opt_setup = setup_racetrack_actor_critic(track)
	params = [(α, λ) for α in αlist for λ in λlist]
	@progress traces = [begin
		 (α, λ) = p
		@info "running for α = $α and λ = $λ"
		out = average_runs((;kwargs...) -> execute_racetrack_actor_critic(track, opt_setup, α, α; kwargs...), nruns; λθ = λ, λw = λ, kwargs...) 
		scatter(x = eachindex(out), y = -cumsum(out) ./ (1:length(out)), name = "α = $α, λ = $λ")
	end
	for p in params]
	plot(traces, Layout(xaxis_title = "Episodes", yaxis_title = "Cumulative Average Steps to Finish So Far", yaxis_type="log", width = 900, height = 600))
end

# ╔═╡ 6e2e9c99-8664-40f2-a1df-bd182db9859e
@bind run_eval_racetrack CounterButton("Click to run `eval_racetrack` and plot rewards per episode for different α and λ")

# ╔═╡ b50282ed-e599-4687-bfbc-0ac9c4f30c84
function racetrack_optimize_λ(track, αθlist, αwlist; epavg = 100, nruns = nthreads(), λlist = [0.0, 0.1, 0.2, 0.4, 0.8, .9], kwargs...)
	opt_setup = setup_racetrack_actor_critic(track)
	function maketrace(αθ, αw) 
		@info "running for αθ = $αθ and αw = $αw"
		@progress rewards = [begin
			out = average_runs((;kwargs...) -> execute_racetrack_actor_critic(track, opt_setup, αθ, αw; kwargs...), nruns; λθ = λ, λw = λ, kwargs...) 
			mean(out[max(1, end-epavg):end])
		end
		for λ in λlist]
		scatter(x = λlist, y = rewards, name = "αθ = $αθ, αw = $αw")
	end

	params = [(a, b) for a in αθlist for b in αwlist]
	@progress traces = [maketrace(p...) for p in params]
	plot(traces, Layout(xaxis_title = "λ", yaxis_title = "Average Reward Last $epavg Episodes", width = 900, height = 600))
end

# ╔═╡ aeffb168-06d2-484e-beea-b507f329e4b8
@bind run_racetrack_optimize CounterButton("Click to run racetrack optimize λ")

# ╔═╡ 80e40d2b-a67b-46eb-86fd-294c0a87a80f
md"""
## Blackjack Environment
"""

# ╔═╡ 8edb3337-0902-45fa-a5b0-c7cc3d40f97f
const cards = (2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, :A)

# ╔═╡ 37dc5518-d378-41fd-b0ef-bc5e3b1b3687
const blackjackactions = (:hit, :stick)

# ╔═╡ 0ac08421-20d2-4e56-bce8-1bc47b36fe2e
#deal a card from an infinite deck and return either the value of that card or an ace
deal() = rand(cards)

# ╔═╡ 064b06ae-903b-4430-b925-534925bca733
const blackjackstates = [(s, c, ua) for s in 12:21 for c in 1:10 for ua in (true, false)]

# ╔═╡ 9be279fa-9325-4eb1-8c73-7742c066664d
const blackjack_sterm = (100, 100, false)

# ╔═╡ 6353a374-9eba-4184-a528-f8ca9f32dfe5
const blackjack_s0 = (0, 0, false)

# ╔═╡ b265b8e6-994a-4be2-a7c9-05adef570fda
makestatelookup(states) = Dict(zip(states, eachindex(states)))

# ╔═╡ 460d9e76-9841-4fb8-8e35-0efbbf6f9f08
const blackjackstatelookup = makestatelookup([blackjackstates; blackjack_s0; blackjack_sterm])

# ╔═╡ 8ea91577-57eb-4afc-8919-95bd16ae6865
#takes a previous sum, usable ace indicator, and a card to be added to the sum.  Returns the updated sum and whether an ace is still usable
function addsum(s::Int64, ua::Bool, c::Symbol)
	if !ua
		s >= 11 ? (s+1, false) : (s+11, true)
	else
		(s+1, true)
	end
end

# ╔═╡ 6324046e-c766-444f-8a74-f6e3569154fa
function addsum(s::Int64, ua::Bool, c::Int64)
	if !ua
		(s + c, false)
	else
		if (s + c) > 21
			(s + c - 10, false)
		else
			(s + c, true)
		end
	end
end

# ╔═╡ 7f8ea283-8b42-4bb7-8d49-a54855a98c5d
function playerstep(s, ua, a)
	a == :stick && return (s, ua)
	addsum(s, ua, deal())
end

# ╔═╡ 8f133852-12da-41b3-8071-51a12211f432
function dealer_sim(s::Int64, ua::Bool)
	(s >= 17) && return s
	(s, ua) = addsum(s, ua, deal())
	dealer_sim(s, ua)
end

# ╔═╡ ebeabff8-4779-49e6-a04f-16a76e0b9b04
function blackjack_step(state, action)
	#score a game in which the player didn't go bust
	function scoregame(playersum, dealersum)
		#if the dealer goes bust, the player wins
		dealersum > 21 && return 1.0

		#if the player is closer to 21 the player wins
		playersum > dealersum && return 1.0

		#if the dealer sum is closer to 21 the player loses
		playersum < dealersum && return -1.0

		#otherwise the outcome is a draw
		return 0.0
	end
	
	(s, c, ua) = state

	#initial state
	if s == 0 
		initstate = true
		#deal two cards and check for player natural
		(s, ua) = addsum(s, ua, deal())
		(s, ua) = addsum(s, ua, deal())
		playernatural = (s == 21)

		#if sum is less than 12 then keep dealing since these are not states with any action choice
		while s < 12
			(s, ua) = addsum(s, ua, deal())
		end

		#generate dealer card
		c = rand(1:10)
	else
		initstate = false
		playernatural = false
	end
	
	#generate hidden dealer card and final state
	hc = deal()
	(ds, dua) = if c == 1
		addsum(11, true, hc)
	else 
		addsum(c, false, hc)
	end

	dealernatural = ds == 21

	sdealer = dealer_sim(ds, dua)

	#calculate score in case of player natural
	playernatural && return (blackjack_sterm, Float64(!dealernatural))

	#if there is no playernatural and we are in the initial state, then return the new initial state ignoring the action selection and giving no reward
	initstate && return ((s, c, ua), 0.0)

	#sticking always ends the game 
	action == :stick && return (blackjack_sterm, scoregame(s, sdealer))

	#deal player new card if hitting
	(s, ua) = addsum(s, ua, deal())

	#player always looses if busts
	s > 21 && return (blackjack_sterm, -1.0)

	#if player has 21 game also ends
	s == 21 && return (blackjack_sterm, scoregame(s, sdealer))

	#otherwise return new state and 0 reward
	return ((s, c, ua), 0.0)
end			

# ╔═╡ 097b8fc1-b4a4-4b93-bc08-2ceebd5d759a
function execute_blackjack_actor_critic(αθ, αw, statelookup; kwargs...)
	nstates = length(statelookup)
	
	#create state feature one hot vectors
	xs = [zeros(nstates) for i in 1:nstates]
	for i in values(statelookup)
		xs[i][i] = 1.0
	end
	
	#parameters
	θ = zeros(nstates, lastindex(blackjackactions))
	w = zeros(nstates)

	#allocations for outputs
	πoutput = zeros(lastindex(blackjackactions))
	gradoutput = similar(θ)

	#value function and gradient
	v̂(s, w) = w[statelookup[s]]
	∇v̂(s, w) = xs[statelookup[s]]

	function clean_output!(v::AbstractVector{T}) where T <: AbstractFloat
		for (i, x) in enumerate(v)
			if isnan(x) || isinf(x)
				v[i] = zero(T)
			end
		end
		return v
	end

	#policy function and gradient
	function π!(s, θ) 
		soft_max!(θ[statelookup[s], :], πoutput)
		clean_output!(πoutput)
	end

	function ∇lnπ!(a, s, θ)
		#ensure πoutput contains the current softmax output for this state
		# π!(s, θ)
		i = statelookup[s]
		 for n in eachindex(blackjackactions)
			@inbounds @simd for m in 1:nstates
				gradoutput[m, n] = (i == m) * ((n == a) - πoutput[n])
				# if i == m
				# 	println("At state $i Updated gradient of $(gradoutput[m, n])")
				# end
			end
		end
		return gradoutput
	end
	
	# reinforce_monte_carlo_control(π!, ∇lnπ!, length(θ), s0, αθ, step, sterm, actions; θ = θ, kwargs...)

	actor_critic_eligibility(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), blackjack_s0, αθ, αw, blackjack_step, blackjack_sterm, blackjackactions; θ = θ, w = w, kwargs...)

	# one_step_actor_critic(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)
end

# ╔═╡ 519e6da0-efbf-4b0a-a61c-5849ba403389
function plotblackjackwinrate(αθ, αw, max_episodes; kwargs...)
	y = cumsum(execute_blackjack_actor_critic(αθ, αw, blackjackstatelookup; max_episodes = max_episodes, kwargs...)[1])[100:end]
	x = (100:max_episodes)

	l = length(y)

	i = ceil(Int64, l / 10_000)
	plot(scatter(x = x, y = y[1:i:l] ./ x[1:i:l]), Layout(xaxis_title = "Episode", yaxis_title = "Average Cumulative Reward", width = 800, height = 500))
end

# ╔═╡ 4c4ba58e-e3b7-4d02-81ae-b8d753487caa
plotblackjackwinrate(0.3, 0.3, 100_000; λθ = 0.5, λw = 0.5)

# ╔═╡ 06d508ea-640d-4e55-b3b6-05c929f82c3b
function blackjack_optimize_λ(αθlist, αwlist; epavg = 1000, nruns = nthreads(), λlist = [0.0, 0.1, 0.2, 0.4, 0.8, .9], kwargs...)
	function maketrace(αθ, αw) 
		@info "running for αθ = $αθ and αw = $αw"
		@progress rewards = [begin
			out = average_runs((;kwargs...) -> execute_blackjack_actor_critic(αθ, αw, blackjackstatelookup; kwargs...), nruns; λθ = λ, λw = λ, kwargs...) 
			mean(out[max(1, end-epavg):end])
		end
		for λ in λlist]
		scatter(x = λlist, y = rewards, name = "αθ = $(round(αθ, sigdigits = 2)), αw = $(round(αw, sigdigits = 2))")
	end

	paramlist = [(a, b) for a in αθlist for b in αwlist]

	@progress traces = [maketrace(p...) for p in paramlist]
	plot(traces, Layout(font_color = "white", plot_bgcolor = "black", paper_bgcolor="rgb(40, 40, 40)", xaxis_title = "λ", yaxis_title = "Average Reward Last $epavg Episodes", height = 600))
end

# ╔═╡ 0c3714fd-821a-4dae-8d1e-1db35ebef315
@bind blackjackruncount CounterButton("Click to run Blackjack Optimize λ")

# ╔═╡ 8cb58177-cc29-4bf0-af2f-704bebb9871f
_, blackjackθ, blackjackw = execute_blackjack_actor_critic(0.25, 0.25, blackjackstatelookup; max_episodes = 500_000, λθ = 0.2, λw = 0.2)

# ╔═╡ 0b6fb5bf-c21e-4727-aafb-65fc3f7b76fb
function plot_blackjack_policy(θ, w)
	πstargridua = zeros(10, 10)
	πstargridnua = zeros(10, 10)
	v̂starua = zeros(10, 10)
	v̂starnua = zeros(10, 10)
	for state in blackjackstates
		(s, c, ua) = state
		n = blackjackstatelookup[state]
		a = blackjackactions[argmax(θ[n, :])]
	
		(i, j) = (s-11, c)

		πout = soft_max(θ[n, :])[1]
		vout = w[n]
		
		(π, v̂) = if ua
			πstargridua, v̂starua
		else
			πstargridnua, v̂starnua
		end

		π[i, j] = πout
		v̂[i, j] = vout 
	end

	x = ["A"; string.([2, 3, 4, 5, 6, 7, 8, 9, 10])]
	y = 12:21

	layout =  Layout(title = "Usable Ace Policy, Stick Probability", width = 400, height = 300, x_label = "Dealer Showing", y_label = "Player sum")
	
	# vstar = eval_blackjack_policy(Dict(s => π[s] == :hit ? [1.0, 0.0] : [0.0, 1.0] for s in blackjackstates), 500_000)
	p1 = Plot(heatmap(z = πstargridua, x = x, y = y, colorbar=attr(y = .8, len = 0.4), name = "Policy Usable Ace"), Layout(title = attr(text="Policy Hit Probability", x = 0.5), margin = attr(b = 1, t = 1, l = 1, r = 1), yaxis_title = "Player sum", xaxis_title = "Usable Ace", font_color="white", xaxis_tickvals = fill("", length(x))))
	# p1 = Plot(heatmap(z = rand(10, 10), x = 1:10, y = 1:10, colorbar = attr(orientation = "h", len = 0.5)))
	p2 = Plot(heatmap(z = πstargridnua, x = x, y = y, showscale = false, name = "Policy No Usable Ace"), Layout(xaxis_tickvals = fill("", length(x)), xaxis_title = "No Usable Ace", title = "Policy Hit Probability"))
	
	# md"""
	# $p1 $p2
	# """
	p3 = Plot(heatmap(z = v̂starua, x = x, y = y, showscale=false, name = "Value Estimate Usable Ace"), Layout(title = "Value Estimate", xaxis_title = "Dealer Showing", yaxis_title = "Player sum"))
	p4 = Plot(heatmap(z = v̂starnua, x = x, y = y, colorbar = attr(len=0.4, y = .2), showscale=true, name = "Value Estimate No Usable Ace"), Layout(xaxis_title="Dealer Showing", title = "Value Estimate"))
	p = [p1 p2; p3 p4]
	relayout!(p, paper_bgcolor="rgb(40, 40, 40)", font_color="white", height = 700, width = 700,  title_text = "Blackjack Policy and Value Functions Estimates")
	plot(p)
end
	

# ╔═╡ 7550213b-8174-4623-9abc-9dcbdc0351a8
plot_blackjack_policy(blackjackθ, blackjackw)

# ╔═╡ 2b964c13-c961-4ed9-8b66-a6715ff7d0ef
md"""
## Tic Tac Toe Environment

This state space is small enough that it is also possible to solve with effectively tabular techniques.  To make the solution more efficient consider the inherent symmetries in the problem.  In particular, the value of game states should be invariant with respenct to d4 symmetries which include: mirror reflection across horizontal, vertical and both diagonal axes, 90, 180, and 270 degree rotations.  So each board and the 7 transformed versions should be treated the same.

Each of the 9 elements of the board can contain an X, O, or nothing.  To encode a board into a unique integer, I will use a ternary encoding.  Associate each cell state with an integer: 0 = nothing, 1 = X, and 2 = O.  A board can then be represented with a 9 digit value where each digit is 0, 1, 2 and this vector is equivalent to a ternary number calculated as follows: `sum(v[i]*3^(i-1) for v in boardvector)`.  So to get a list of unique states, apply all the symmetry operations to a board, encode each board as an integer, select the transformation with lowest integer.  This procedure can be performed for every possible board and a mapping between those and the unique states can be saved for future use.  

Additional filters for valid boards should include ensuring that the O count either 1 less than the X count or equal to it.  Also boards where more than one player has 3 in a row are invalid.
"""

# ╔═╡ 078a1739-911c-4673-821b-488a878bab37
md"""
### Game Setup
"""

# ╔═╡ 205fcf67-e79c-4f20-bb8b-ddb6b980ed9d
const d4_symmetries = SVector{9}.([
		[1, 2, 3, 4, 5, 6, 7, 8, 9], #identity
		[3, 2, 1, 6, 5, 4, 9, 8, 7], #x axis flip
		[7, 8, 9, 4, 5, 6, 1, 2, 3], #y axis flip
		[7, 4, 1, 8, 5, 2, 9, 6, 3], #90 degree rotation
		[9, 8, 7, 6, 5, 4, 3, 2, 1], #180 degree rotation
		[3, 6, 9, 2, 5, 8, 1, 4, 7], #270 degree rotation
		[9, 6, 3, 8, 5, 2, 7, 4, 1], #diagonal flip 1
		[1, 4, 7, 2, 5, 8, 3, 6, 9] #diagonal flip 2
])

# ╔═╡ 5f917c40-c1b2-4dd8-ac81-8e955d6af7af
#indices to transform back after doing symmetry operation
const d4_inverted = [SVector{9}(findfirst(==(i), v) for i in 1:9) for v in d4_symmetries]

# ╔═╡ 7f4b6d93-53dd-466e-8401-24c1a59c32d9
const BoardTTT = SVector{9, UInt8}

# ╔═╡ eadea57c-b3b6-44c0-bf5d-ed57fec3ff7c
#if a player has claimed any of these inds then the game is over
const winning_inds = ((1, 2, 3), (4, 5, 6), (7, 8, 9), (1, 4, 7), (2, 5, 8), (3, 6, 9), (1, 5, 9), (3, 5, 7))

# ╔═╡ 15ac8e2e-9ec6-4723-a0ba-0bd29a37f64e
const ttt_moves = SVector{9}(UInt8.(1:9))

# ╔═╡ 9fe44113-8232-4579-85b3-65725d30fd46
const X_VAL = 0x01

# ╔═╡ a0ddc362-cfeb-4585-86bd-0ad003e3d61d
const O_VAL = 0x02

# ╔═╡ cfdc3298-118e-4b87-b0c0-f46afe573a12
const EMPTY = 0x00

# ╔═╡ bd53c7c0-e77a-46a5-be88-0fd97e80c02c
#check if a player associated with a given value has won on a given board
val_win(board::BoardTTT, val::UInt8) = any(all(board[i] == val for i in inds) for inds in winning_inds)

# ╔═╡ 2deb0cf0-d83d-4a51-b76b-093c53f09c77
val_win(board, val) = val_win(BoardTTT(board), UInt8(val))

# ╔═╡ 62eb2abe-e418-4826-9c99-7d3b1df500eb
x_win(board) = val_win(board, X_VAL)

# ╔═╡ 727ea9ce-a670-4e8c-b2f2-8a19477d9a33
o_win(board) = val_win(board, O_VAL)

# ╔═╡ 9b8187b5-aba6-404c-b232-faf4ed200c89
canmove(board::BoardTTT, m::Integer) = board[m] == 0x00

# ╔═╡ 57f7fddd-01cb-489f-98cf-f1ce07977eb1
canmove(board, m) = canmove(BoardTTT(board), m)

# ╔═╡ 83988884-e42e-44aa-9ead-0b81258160cb
valid_moves(board::BoardTTT) = board .== 0 

# ╔═╡ 76e7a54d-8db6-43be-a994-e8469fce6760
valid_moves(board) = valid_moves(BoardTTT(board))

# ╔═╡ f53f464b-f9f6-4f34-b35f-e7e8cabc3600
is_term(board) = !any(==(0), board)

# ╔═╡ ef4b5d88-cae0-466b-baac-90c3cf8f65df
is_winner(board) = x_win(board) || o_win(board)

# ╔═╡ ec5bb245-051a-4c80-aa98-76b618ec65c6
is_draw(board) = is_term(board) && !is_winner(board)

# ╔═╡ a12c0d95-8e64-4a82-9c31-62604a1a03ce
is_active(board) = !is_term(board) && !is_winner(board)

# ╔═╡ d7850e01-34ff-48aa-b366-ee33584372a7
#determine if it is O's turn to move because the board sum should be 1 off from a multiple of 3
function is_o_move(board::BoardTTT)
	s = sum(board)
	Bool(s % 0x0003)
end

# ╔═╡ 58668067-0efe-4f23-94f4-d010016f568d
is_o_move(board) = is_o_move(BoardTTT(board))

# ╔═╡ d4f866a4-7e40-4e60-8382-cd78b2fe0a86
#check if a board is valid, i.e. can be reached during normal play where X starts, players alternate and the game ends after the first player gets 3 in a row
function isvalid(board::BoardTTT)
	winners = NamedTuple((Symbol(f), f(board)) for f in (x_win, o_win))
	#cannot have both x and o winning
	all(winners) && return false
	xnum = count(==(X_VAL), board)
	onum = count(==(O_VAL), board)
	#O count must be equal to or one less than X count
	!(0 <= xnum - onum <= 1) && return false
	#if O wins then the X count must be equal to the O count because if it is one greater then X played a move after O won
	winners.o_win && (xnum != onum) && return false
	#if X wins then it must have a count greater than O because otherwise O would have gone after X wins
	winners.x_win && (xnum == onum) && return false
	
	#in all other cases the board is fine
	return true
end

# ╔═╡ 475099d3-d3a4-4757-865d-1e1b4e7da10e
isvalid(board) = isvalid(BoardTTT(board))

# ╔═╡ e43b5edb-9a3b-4b88-9ef4-87ca1f267b2a
const score_functions = (x_win, o_win, is_draw, is_active)

# ╔═╡ bbb82a7d-8c54-4cdd-95fe-f6719ecaa5fd
#list of functions to compute all relevant properties of a board
const status_functions = (score_functions..., is_o_move, valid_moves)

# ╔═╡ c9ea915e-2bab-4648-8388-658ebf796d78
#rewards associated with arriving at a board with the following conditions for the X player.  rewards for the O player will be negative of this.  The value of draw differing from 0 is so it can be distinguished from an active board.  Also under these rewards a state with equal probability of win and loss would be 0 whereas a state with an expected draw would be valued at -0.5.
rewardsX = NamedTuple(zip(Symbol.(score_functions), (1.0, -1.0, -0.5, 0.0)))

# ╔═╡ 97d7bfbd-f821-4201-9d68-9d1654d2a86b
const BoardStatus = NamedTuple{Symbol.(status_functions)}

# ╔═╡ 6ce26626-1c4c-41ea-b23b-cf6d5bac230b
#check a board and return game status of each check
get_board_status(board::BoardTTT) = NamedTuple((Symbol(f), f(board)) for f in status_functions)

# ╔═╡ 3b3973d7-e26e-4e0d-b767-a4247304b9a0
#attempt to convert a different type to a valid board if possible
get_board_status(board) = get_board_status(BoardTTT(board))

# ╔═╡ 52fb1724-3e09-4514-b315-ffc83ba88ebe
#reward associated with arriving at a new board from the perspective of the x player, not that for valid boards only one of the values in status will be true so this will produce a value for invalid boards even though it isn't well defined
get_reward_x(status::BoardStatus) = sum(rewardsX[k]*status[k] for k in keys(rewardsX))

# ╔═╡ 2d811d01-d2c4-477d-8a06-fcf94e5ad798
get_reward_x(board) = get_reward_x(get_board_status(board))

# ╔═╡ a7be257c-d2d9-4d55-a498-3e4db491e644
get_reward_o(args...) = -get_reward_x(args...)

# ╔═╡ 2f147876-144d-4c7f-9c5a-affc3476753c
#get reward for a board assuming the desired perspective is the player with the available move
get_reward(status::BoardStatus) = (1 - 2*status.is_o_move) * get_reward_x(status)

# ╔═╡ 90170d3b-25d9-4fbc-a131-f8def2187435
get_reward(board) = get_reward(get_board_status(board))

# ╔═╡ 08f28c09-708c-40e0-ba16-71135fb438b4
# convert a board representation as a vector to an integer using powers of 3, need to use UInt16 here to have enough states.  Optionally permute the indices to calculate the state of a transformed board
mapboard(v::BoardTTT; inds = eachindex(v)) =  mapreduce(a -> v[last(a)]*0x0003^(first(a)-1), +, enumerate(inds)) 

# ╔═╡ 85a07db1-5135-472e-95e6-9d4e85928350
mapboard(v; kwargs...) = mapboard(BoardTTT(v); kwargs...)

# ╔═╡ 3e44c2aa-2d59-4dba-ba3c-0db868c6e460
# convert a number to a board representation vector
map_ttt_state(n::UInt16) = BoardTTT(digits(n, base = 0x03, pad=9))

# ╔═╡ e59efa67-9f52-4493-ba47-84f3ad8a87a2
map_ttt_state(n) = map_ttt_state(UInt16(n))

# ╔═╡ f9a2136c-7b8d-4427-9fd9-040084dc96fe
const unfiltered_ttt_boards = (map_ttt_state(n) for n in 0:(3^9-1))

# ╔═╡ 762bed69-bdd8-443b-9526-bf10442eef65
const valid_ttt_boards = (b for b in unfiltered_ttt_boards if isvalid(b))

# ╔═╡ ffb0962d-bf78-40ea-aa56-7eb198ef5234
#lookup table for getting board from a numerical state representation
const ttt_state_lookup = Dict(mapboard(b) => b for b in valid_ttt_boards)

# ╔═╡ 5481a261-45b3-4afd-9c28-03c11f884e69
#convert a board to its symmetry equivalent version and the index of the symmetry transformation used
function get_symmetric_board(board::BoardTTT)
	#only keep the board with the lowest state value
	(smin, imin) = findmin(inds -> mapboard(board; inds = inds), d4_symmetries)
	inds = d4_symmetries[imin]
	(BoardTTT(view(board, inds)), imin)
end

# ╔═╡ 9ea87c25-9acb-4ae9-8f39-6c684dbc6b19
get_symmetric_board(board) = get_symmetric_board(BoardTTT(board))

# ╔═╡ b89107cf-45e7-44da-9b73-40b5c995eb8e
#map a board to it's symmetric equivalent with the permutation indices
const symmetric_board_lookup = Dict(b => get_symmetric_board(b) for b in valid_ttt_boards)

# ╔═╡ 6e556453-694d-48f7-8c8c-adc04f2d80df
const symmetric_boards = unique(first(a) for a in values(symmetric_board_lookup))

# ╔═╡ a8137758-4ff3-42d2-a163-8e1daffdc869
#precompute the status of unique boards only
const ttt_status_lookup = Dict(b => get_board_status(b) for b in symmetric_boards)

# ╔═╡ 52f0411c-ac13-41c0-bb7b-8fc755849a35
function lookup_board_status(board::BoardTTT)
	sym_board, isym = symmetric_board_lookup[board]
	(status = ttt_status_lookup[sym_board], isym = isym)
end

# ╔═╡ 5bdcfe04-f343-414a-87cf-a2a7170cf9c4
lookup_board_status(board) = lookup_board_status(BoardTTT(board))

# ╔═╡ 70e2766d-bc8f-42e6-89fd-7036a0020177
const active_ttt_boards = filter(b->ttt_status_lookup[b].is_active, symmetric_boards)

# ╔═╡ ea0e22de-a9c1-4a02-8a01-97990348d571
const active_x_boards = filter(b -> !ttt_status_lookup[b].is_o_move, active_ttt_boards)

# ╔═╡ 2d7651cb-0226-44c4-ac3c-5de1b98c513a
const active_o_boards = filter(b -> ttt_status_lookup[b].is_o_move, active_ttt_boards)

# ╔═╡ d856f23f-72ef-44e5-aaf6-0213997f783f
struct TTTEnvironment{T, V}
	init_board::BoardTTT
	term_board::BoardTTT
	move::T
	apply_π::V
end

# ╔═╡ a8f7db69-4a50-4e0d-9be6-cf4eef5233c6
function make_ttt_environment()
	#the most straightforward board representation is a 3x3 matrix of a ternary value.  We could represent this with 2 bits that can take on 1 of 4 values so it would be one more value than is necessary.  With this representation an unocupied cell is 00, x cell is 01, and o cell is 10 with 11 being ignored.  We could use 2 bit matricies for this with each matrix representing the occupied positions of x and o respectively.  This could also be compressed down to a single number 9 bits long.  It would be nice to just use a 8 bit number though because that is a fundamental datatype UInt8.  Maybe we can ignore the last number because we'd never have a situation where every state was filled up by a single mark but these bits represent whether the mark is present in a given cell so we'd have some unintuitive mapping if we force ourselves to use UInt8.  We could also just use a vector or even static array of length 9.  The other approach is to generate all 3^9 possible boards and just have a lookup table from that maps a given board to one of those numbers.  We could do that by having 0, 1, 2 in each position and then calculating the ternary value of that.  For example let's say we have the following board where the cells are shown one row at a time [0 0 0; 0 1 0; 0 0 2].  This would map to 3^5 + 2*3^9.  

	init_board = BoardTTT(fill(0x00, 9))
	term_board = BoardTTT(fill(0x03, 9))

	#return a new board state after a move a where a should be the square where a mark is placed as a number from 1 to 9.
	function move(board::BoardTTT, a::UInt8)
		#if an illegal move is attempted 
		board[a] != 0x00 && begin @info "illegal move $a on board $board"; return term_board end
		#value to be filled into the board, 1 for X moves and 2 for O moves
		fillmove = 0x0001 + UInt8(lookup_board_status(board).status.is_o_move)
		state = mapboard(board) #convert board to integer to calculate new values and perform lookup
		newstate = state + (fillmove * 0x003^(a-0x0001)) #calculate new state
		newboard = ttt_state_lookup[newstate] #get new board from lookup table
	end

	move(board, a) = move(BoardTTT(board), UInt8(a))

	#take a policy π that is only defined for unique boards and calculate the action to take converting symmetries back to original board
	function apply_π(π, board::BoardTTT)
		(symboard, isym) = symmetric_board_lookup[board]
		prbs = copy(π(symboard))
		prbs[d4_inverted[isym]]
	end	

	TTTEnvironment(init_board, term_board, move, apply_π)
end

# ╔═╡ 4d67501b-1f27-40e3-8d76-f406c0de9e1c
const ttt_environment = make_ttt_environment()

# ╔═╡ a3e91897-ebc0-4b1f-bec2-c99338c92fb0
function get_random_move(board::BoardTTT)
	status, isym = lookup_board_status(board)
	wsample(ttt_moves, view(status.valid_moves, d4_inverted[isym]))
end

# ╔═╡ 7c5234d2-7a19-46cd-81cd-daeb15327594
get_random_move(board) = get_random_move(BoardTTT(board))

# ╔═╡ 875d894d-055d-4f48-91e5-19871f4ee370
#clean up possible issues in softmax caused by infinite and undefined values
function clean_output!(v::AbstractVector{T}) where T <: AbstractFloat
	for (i, x) in enumerate(v)
		if isnan(x) || isinf(x)
			v[i] = zero(T)
		end
	end
	return v
end

# ╔═╡ 23f8b2d3-7f76-4dd5-a301-95afe719ec30
#move on a board but return the symmetric version
symmetric_move(board, m) = first(symmetric_board_lookup[ttt_environment.move(board, m)])

# ╔═╡ c8885191-44a7-448c-a546-d5fb50254616
#take a step but map boards to symmetric versions and any inactive board maps to the terminal state. defaults to calculating rewards from the x player perspective
function ttt_step(board, m; reward_func = get_reward_x)
	newboard = symmetric_move(board, m)
	(status, isym) = lookup_board_status(newboard)
	r = reward_func(status)
	finalboard = status.is_active ? newboard : ttt_environment.term_board
	(finalboard, r, status.is_active)
end

# ╔═╡ f29d1813-fa74-46ce-b3ac-3a06a5cd104e
#define step for a player against an opponent
function ttt_step(board::BoardTTT, m::UInt8, get_opponent_action::Function; kwargs...)
	(newboard, r, is_active) = ttt_step(board, m; kwargs...)
	!is_active && return (newboard, r)
	m2 = get_opponent_action(newboard)
	ttt_step(newboard, m2; kwargs...)[[1, 2]]
end

# ╔═╡ 3f927561-7b13-4f19-946a-67c310c60255
md"""
### Actor-Critic Agents vs Fixed Opponent
"""

# ╔═╡ 04d93927-6206-4d32-91fb-81b73568f1f7
struct ActorCriticTTTAgent{Vest, Vgrad, Pfunc, Pgrad}
	v̂::Vest
	∇v̂::Vgrad
	π!::Pfunc
	∇lnπ!::Pgrad
	θ::Matrix{Float64}
	w::Vector{Float64}
	πoutput::Vector{Float64}
	∇output::Matrix{Float64}
end

# ╔═╡ 2da0cccb-5e1a-43a7-b485-8b62a8d70d10
#setup estimation functions for a player given a set of valid playable states for that player.  For example to create an X player, only valid X states should be selected and the corresponding step function should only produce those states
function setup_ttt_player(states::AbstractVector{T}) where T <: BoardTTT
	#convert states to index
	statelookup = Dict(zip(states, eachindex(states)))
	statelookup[ttt_environment.term_board] = lastindex(states) + 1

	#create state feature vectors, leave the terminal state at all zeros
	xs = [zeros(lastindex(states)+1) for i in 1:(lastindex(states)+1)]
	for i in eachindex(states)
		xs[i][i] = 1.0
	end

	#value function and gradient
	v̂(s::BoardTTT, w) = w[statelookup[s]]
	∇v̂(s::BoardTTT, w) = xs[statelookup[s]]
	#allocations for outputs
	πoutput = zeros(lastindex(ttt_moves))
	∇output = zeros(lastindex(states)+1, lastindex(ttt_moves))

	#policy function and gradient
	function π!(s::BoardTTT, θ::Matrix)
		πoutput .= view(θ, statelookup[s], :)
		πoutput .+= ((s .!= 0x00) .* -Inf) #set output preference to -Inf for occupied cells
		soft_max!(πoutput)
		clean_output!(πoutput)
	end

	#under the convension that we always use the x player reward for the value estimate, to get a valid policy for the o player we can reverse the gradient direction for board states on which the o player is taking a turn.  That way both players can use the same value function
	function ∇lnπ!(a::UInt8, s::BoardTTT, θ::Matrix)
		π!(s, θ)
		i = statelookup[s]
		f = 1.0 - (2.0 * lookup_board_status(s).status.is_o_move) #reverse policy gradient for o player
		for n in ttt_moves
			@inbounds @simd for m in eachindex(states)
				#apply gradient for soft-max but noticing all values are 0 for i != m which corresponds to other states
				∇output[m, n] = f * (i == m) * ((n == a) - πoutput[n])
			end
		end
		return ∇output
	end

	∇lnπ!(a, s, θ) = ∇lnπ!(UInt8(a), BoardTTT(s), θ)

	#parameters
	θ = zeros(lastindex(states)+1, lastindex(ttt_moves))
	w = zeros(lastindex(states)+1)

	#note that because there are internal allocated outputs for the policy and the gradient a new instance of this should be generated each time a learning procedure is done.  it may be better design to explicitely pass these holders into any running function so there's always a new copy
	ActorCriticTTTAgent(v̂, ∇v̂, π!, ∇lnπ!, θ, w, πoutput, ∇output)
end	

# ╔═╡ 626940b4-eeb9-4ee6-9f27-b6446f014572
struct PolicyResultsTTT{T}
	rewards::Vector{Float64} #rewards per episode of training
	θ::Matrix{Float64} #parameters for policy function
	w::Vector{Float64} #parameters for value function
	eval_board::T #function to evaluate a board
end

# ╔═╡ 11d113f1-c1f0-4a58-a3b2-44c70b21cdac
function execute_ttt_actor_critic(states, step, get_s0, αθ, αw; kwargs...)
	agent = setup_ttt_player(states)
	s0 = ttt_environment.init_board
	sterm = ttt_environment.term_board
	actions = ttt_moves

	# reinforce_monte_carlo_control(π!, ∇lnπ!, length(θ), s0, αθ, step, sterm, actions; θ = θ, kwargs...)
	(rewards, θout, wout) = actor_critic_eligibility(agent.π!, agent.∇lnπ!, agent.v̂, agent.∇v̂, length(agent.θ), length(agent.w), s0, αθ, αw, step, sterm, actions; θ = agent.θ, w = agent.w, get_s0=get_s0, kwargs...)
	# one_step_actor_critic(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)

	function eval_board(b)
		(symboard, isym) = symmetric_board_lookup[b]
		prbs = agent.π!(symboard, θout)[d4_inverted[isym]]
		v = agent.v̂(symboard, wout)
		(prbs, v)
	end
	PolicyResultsTTT(rewards, θout, wout, eval_board)
end

# ╔═╡ 959e4a18-fe6e-4c9c-b9bf-f752108fd2dd
x_step_vs_random(board, move) = ttt_step(board, move, get_random_move)

# ╔═╡ b6f3d5b6-74b7-4211-b236-203881a97c38
x_step_vs_random_results = execute_ttt_actor_critic(active_x_boards, x_step_vs_random, () -> rand() < 0.1 ? ttt_environment.init_board : rand(active_x_boards), 0.5, 0.5; λθ = 0.5, λw = 0.5, max_episodes = 100_000, showprogress=true)

# ╔═╡ 8731821b-d82a-4697-be21-522583d7dbab
@bind avgeps Slider(100:10000, show_value=true)

# ╔═╡ 3728a916-a502-48ec-9c84-5b2e7e4df61c
#train O-player vs the first X policy
o_step_vs_x1(board, move) = ttt_step(board, move, b -> select_action(x_step_vs_random_results.eval_board(b)[1]))

# ╔═╡ e6cd6459-6e50-4c5c-b6d3-a55706bbb257
o_vs_x1_results = execute_ttt_actor_critic(active_o_boards, o_step_vs_x1, () -> rand(active_o_boards), 0.5, 0.5; λθ = 0.5, λw = 0.5, max_episodes = 100_000, showprogress=true)

# ╔═╡ 4a999b16-1427-4b30-a2be-1919b0ad2caf
x_vs_o1(board, move) = ttt_step(board, move, b -> select_action(o_vs_x1_results.eval_board(b)[1]))

# ╔═╡ aea8317c-fb5f-4817-b927-1c6d48072ea7
x_vs_o1_results = execute_ttt_actor_critic(active_x_boards, x_vs_o1, () -> rand(active_x_boards), 0.5, 0.5; λθ = 0.5, λw = 0.5, max_episodes = 100_000, showprogress=true)

# ╔═╡ 9e32d0d4-bdbb-46a7-ad3c-34184cea0b92
md"""
Compare these three policies on a single board state
"""

# ╔═╡ 9e9d1b3a-d8a5-45f2-87b1-20f7edf56793
run_ttt_game(πx, πo) = run_ttt_game(πx, πo, [ttt_environment.init_board], Vector{UInt8}(), Vector{UInt8}())

# ╔═╡ f271a2a6-1720-4cb1-99e0-9aff3fab171c
#play a game between two different policies for the x and o player
function run_ttt_game(πx::Function, πo::Function, board_history::Vector{BoardTTT}, xturns::Vector{UInt8}, oturns::Vector{UInt8})
	board = last(board_history)
	status = lookup_board_status(board)
	#if the board is no longer active then end the game
	!status.status.is_active && return (board_history, status, xturns, oturns)
	xmove = πx(board) #select move for x player
	push!(xturns, xmove)
	board′ = ttt_environment.move(board, xmove)
	push!(board_history, board′)
	status′ = lookup_board_status(board′)
	#if the board is no longer active then end the game
	!status′.status.is_active && return (board_history, status′, xturns, oturns)
	omove = πo(board′) #select move for o player
	push!(oturns, omove)
	board′′ = ttt_environment.move(board′, omove)
	push!(board_history, board′′)
	run_ttt_game(πx, πo, board_history, xturns, oturns)
end

# ╔═╡ 75f13e3d-90a5-461e-9f64-479a01465fab
function get_ttt_matchup_statistics(πx, πo; trials = 100_000)
	wld = 1:trials |> Map(n -> run_ttt_game(πx, πo)[2].status[(:x_win, :o_win, :is_draw)]) |> collect
	NamedTuple(outcome => count(a[outcome] for a in wld)/trials for outcome in (:x_win, :o_win, :is_draw))
end

# ╔═╡ 938e33dd-c129-40d0-a72e-b7d1f3f770ff
get_ttt_move(results::PolicyResultsTTT) = b -> select_action(results.eval_board(b) |> first)

# ╔═╡ a278e854-e230-42aa-97a2-0f5b7d1815af
function compare_ttt_policies(results1::PolicyResultsTTT, results2::PolicyResultsTTT; kwargs...)
	p1 = get_ttt_move(results1)
	p2 = get_ttt_move(results2)
	get_ttt_matchup_statistics(p1, p2; kwargs...)
end

# ╔═╡ 702f39a7-f921-4f20-90d2-9b7ec493230e
compare_ttt_policies(results::PolicyResultsTTT, p::Function; kwargs...) = get_ttt_matchup_statistics(get_ttt_move(results), p; kwargs...)

# ╔═╡ 088c2166-17ab-4c22-b621-6421316ebd52
compare_ttt_policies(p::Function, results::PolicyResultsTTT; kwargs...) = get_ttt_matchup_statistics(p, get_ttt_move(results); kwargs...)

# ╔═╡ 75377f64-9b4b-47ec-b25e-b17d42407fad
#modify this so that it uses the new functions and plots progress per round by showing the victory rate over the previous opponent
function execute_actor_critic_selfplay(αθ, αw, rounds; kwargs...)
	form_opponent(results) = (board, move) -> ttt_step(board, move, b -> select_action(results.eval_board(b)[1]))
	train_player(active_boards, opponent) = execute_ttt_actor_critic(active_boards, opponent, () -> rand(active_boards), αθ, αw; kwargs...)

	x_results = Vector{PolicyResultsTTT}(undef, rounds)
	o_results = Vector{PolicyResultsTTT}(undef, rounds)

	x_results[1] = train_player(active_x_boards, (board, move) -> ttt_step(board, move, get_random_move))
	o_results[1] = train_player(active_o_boards, form_opponent(x_results[1]))
	
	@progress for i in 2:rounds
		x_results[i] = train_player(active_x_boards, form_opponent(o_results[i-1]))
		o_results[i] = train_player(active_o_boards, form_opponent(x_results[i]))
	end
	
	return x_results, o_results
end

# ╔═╡ 1d0fe433-0bca-4083-842b-dc209298af13
nrounds = 10

# ╔═╡ a0740d6d-d034-4037-b410-f31f76b207f5
ttt_rounds_results = execute_actor_critic_selfplay(0.5, 0.5, nrounds; λθ = 0.5, λw = 0.5, max_episodes = 30_000)

# ╔═╡ 124a38c0-dd7a-43b2-9f86-5a41261736e0
md"""
Round:
$(@bind roundcount Slider(1:nrounds, show_value=true))

Player:
$(@bind playerselect Select([1 => "X", 2 => "O"]))
"""

# ╔═╡ 3c6243f6-973c-4521-9881-c66f94de83a0
function plot_ttt_rounds(round_results; trials = 1000)
	xrounds = first(ttt_rounds_results) |> Map(x_results -> compare_ttt_policies(x_results, get_random_move, trials = trials)) |> tcollect
	x_traces = [scatter(x = eachindex(round_results[1]), y = [a[sym] for a in xrounds], name = String(sym)) for sym in (:x_win, :o_win, :is_draw)] 
	p1 = Plot(x_traces, Layout(title = "X Player vs Random Policy", xaxis_title = "Rounds"))
	orounds = last(ttt_rounds_results) |> Map(o_results -> compare_ttt_policies(get_random_move, o_results, trials = trials)) |> tcollect
	o_traces = [scatter(x = eachindex(round_results[1]), y = [a[sym] for a in orounds], name = String(sym)) for sym in (:x_win, :o_win, :is_draw)] 
	p2 = Plot(o_traces, Layout(title = "O Player vs Random Policy", xaxis_title = "Rounds"))
	plot([p1 p2])
end

# ╔═╡ 87fd6b09-fd43-454c-a589-38dab5ccf71a
plot_ttt_rounds(ttt_rounds_results; trials = 10_000)

# ╔═╡ a45949bc-878b-47fc-a239-cb8bb110046b
compare_ttt_policies(x_step_vs_random_results, get_random_move)

# ╔═╡ afb19bfb-0e0d-4d3b-8db5-c9f1a91b61ae
compare_ttt_policies(get_random_move, o_vs_x1_results)

# ╔═╡ 72025689-c50d-4f74-8ddb-5709b43b39ed
compare_ttt_policies(x_vs_o1_results, get_random_move)

# ╔═╡ 2616cfe3-c66a-4d00-8caa-1b92e8bcfa6d
get_ttt_matchup_statistics(get_ttt_move(x_step_vs_random_results), get_ttt_move(o_vs_x1_results))

# ╔═╡ eef60b59-8595-454c-89a3-f02729fbd1d5
get_ttt_matchup_statistics(get_ttt_move(x_vs_o1_results), get_ttt_move(o_vs_x1_results))

# ╔═╡ 2900dc4e-eed2-4a5c-a026-d1d1bdaf62b9
get_ttt_matchup_statistics(get_random_move, get_random_move)

# ╔═╡ 2ec47c25-ec71-4cd9-b1b7-14ae8ee3492a
ttt_selfplay_results = execute_ttt_actor_critic(active_ttt_boards, ttt_step, () -> rand() < 0.75 ? ttt_environment.init_board : rand(active_ttt_boards), 0.5, 0.1; λθ = 0.5, λw = 0.5, γ = 0.9, max_episodes = 100_000, showprogress=true)

# ╔═╡ 46277863-5e64-4e23-87e3-7980110a8742
compare_ttt_policies(ttt_selfplay_results, o_vs_x1_results)

# ╔═╡ fe2d0874-ac60-421d-9632-9310af0b8d1f
compare_ttt_policies(ttt_selfplay_results, ttt_rounds_results[end][2])

# ╔═╡ cf7dd9c3-6c51-40c9-bbea-08ccf4d3a8b1
compare_ttt_policies(ttt_selfplay_results, get_random_move)

# ╔═╡ f17cc08e-e0e8-4bad-8f81-08eb1d03d827
compare_ttt_policies(x_vs_o1_results, o_vs_x1_results)

# ╔═╡ f70dcbbd-e871-4f6d-9287-b468d511dc7b
compare_ttt_policies(x_vs_o1_results, get_random_move)

# ╔═╡ c5502e6e-751a-4e24-851d-6cc1ed119c3f
compare_ttt_policies(x_step_vs_random_results, get_random_move)

# ╔═╡ b72f2485-e9a9-4b2c-a126-d7a42a3d6ba6
compare_ttt_policies(ttt_selfplay_results, ttt_selfplay_results)

# ╔═╡ 83ccd36d-96c8-4665-9148-bdf95eb8dda1
function plot_tttresults(ttt_results::PolicyResultsTTT, avgeps = 100)
	plot([mean(ttt_results.rewards[i:avgeps+i-1]) for i in 1:lastindex(ttt_results.rewards)-avgeps])
end

# ╔═╡ 892df402-df32-4344-9201-0458b90fed26
plot_tttresults(x_step_vs_random_results, avgeps)

# ╔═╡ f88a0889-f3d5-4d75-a745-c734e4420802
plot_tttresults(o_vs_x1_results, avgeps)

# ╔═╡ 73693dd6-f07c-4625-9d84-f356c91f5735
plot_tttresults(x_vs_o1_results, avgeps)

# ╔═╡ 90385599-9db0-4463-8063-81a41266712f
plot_tttresults(ttt_rounds_results[playerselect][roundcount], 100)

# ╔═╡ 8464adca-a780-4da1-bb1c-05db6277634c
plot_tttresults(ttt_selfplay_results, avgeps)

# ╔═╡ 0d234b25-994f-4649-ac05-0df2dcf12264
function optimize_λ(αθlist, αwlist, opt_setup; epavg = 100, nruns = nthreads(), λlist = [0.0, 0.1, 0.2, 0.4, 0.8, .9], kwargs...)
	function maketrace(αθ, αw) 
		@info "running for αθ = $αθ and αw = $αw"
		@progress rewards = [begin
			out = average_runs((;kwargs...) -> execute_actor_critic(opt_setup, αθ, αw; kwargs...), nruns; λθ = λ, λw = λ, kwargs...) 
			mean(out[max(1, end-epavg):end])
		end
		for λ in λlist]
		scatter(x = λlist, y = rewards, name = "αθ = $αθ, αw = $αw")
	end

	params = [(a, b) for a in αθlist for b in αwlist]
	@progress traces = [maketrace(p...) for p in params]
	plot(traces, Layout(xaxis_title = "λ", yaxis_title = "Average Reward Last $epavg Episodes", width = 900, height = 600))
end

# ╔═╡ 506a7c77-0d48-47a1-b3fd-d203101b9106
function showboard(board::AbstractVector)
	function f(n::Integer)
		n == 0 && return '-'
		n == 1 && return 'X'
		return 'O'
	end
	mapreduce(inds -> f.(board[inds]), vcat, [[1 2 3], [4 5 6], [7 8 9]])
end		

# ╔═╡ 60652571-4e4e-4d68-bec2-3b3fb6db0b1d
showboard(boardstate::UInt16) = boardstate == typemax(UInt16) ? "Terminal State" : showboard(mapstate(boardstate))

# ╔═╡ d7976b1a-41a7-4d3d-9b0d-7b5a7d87da54
md"""
### Using Value Iteration

For the previous two environments, value iteration was not feasible because defining the probability transition function was very inconvenient or impossible.  However for the tic tac toe game it may be possible assuming that the opponent is pursuing the same greedy policy as the player.  Alternatively we can train value iteration against the random policy which could very well find the same optimal strategy as playing against an optimal opponent.  To make the problem more tractable we will only consider states that are unique in terms of symmetries and use the mapping functions to enforce every state in our lookup is a symmetry mapped version.
"""

# ╔═╡ f1d6e558-6e7c-4238-983a-b756d4ea9450
function make_ttt_ptf(boards, π_opponent)
	function get_opponent_transitions(board, s, a)
		prbs = π_opponent(board)
		inds = findall(!=(0), prbs)
		#add up probabilities for each transition accumulating them if the ending state is equivalent
		mapreduce(mergewith(+), inds) do i
			(s′, r, active) = ttt_step(board, i)
			Dict((s′, r, s, a) => prbs[i])
		end
	end

	function get_transitions(board, a)
		(newboard, r, active) = ttt_step(board, a)
		!active && return Dict((newboard, r, board, a) => 1.)
		get_opponent_transitions(newboard, board, a) #if game isn't over get the transition from the subsequent move
	end

	function get_transitions(board::S) where S
		moves = findall(==(0), board)
		isempty(moves) && return Dict{Tuple{S, Float64, S, UInt8}, Float64}()
		mapreduce(mergewith(+), moves) do move
			get_transitions(board, move)
		end
	end

	#only calculate transitions from valid states for x player
	ptf = mapreduce(get_transitions, mergewith(+), boards)
	sa_keys = get_sa_keys(ptf)

	return (ptr = ptf, sa_keys = sa_keys)
end

# ╔═╡ f4e8f556-b131-4c86-b245-0f7a09760353
#takes a policy that only is defined for the unique symmetrical boards and applies it to any board
function apply_sym_π(π, board)
	(newplayboard, inds) = state_symmetry_lookup[mapboard(board)]
	invinds = [findfirst(inds .== i) for i in 1:9]
	π[newplayboard][invinds]
end

# ╔═╡ 8a9bbf5b-18f3-4cbe-ac15-d2d88b68f8bd
function value_policy_output(value_policy, board)
	(newplayboard, isym) = symmetric_board_lookup[board]
	!haskey(value_policy[3], newplayboard) && return (zeros(9), "Invalid State")
	πs = convertπs(value_policy[3][newplayboard])
	board_value = (value_policy |> first |> last)[newplayboard]
	prbs = [haskey(πs, a) ? πs[a] : 0.0 for a in UInt8.(1:9)][d4_inverted[isym]]
	return (prbs, board_value)
end

# ╔═╡ f0a358b5-9733-4593-8174-fa44c87d93b7
function eval_value_policy(board, results, name)
	(newplayboard, inds) = state_symmetry_lookup[mapboard(board)]
	invertinds = [findfirst(inds .== i) for i in 1:9]
	!haskey(results[3], newplayboard) && return (value = "Not a valid state for first player", actions = heatmap_board(name, board, zeros(9))) 
	πs = convertπs(results[3][newplayboard])
	(value = results[1][end][newplayboard], actions = heatmap_board(name, board, [haskey(πs, UInt8(a)) ? πs[UInt8(a)] : 0.0 for a in 1:9][invertinds])) 
end

# ╔═╡ 73f0ca1b-b331-4682-9741-3399a0ac3d46
oplayer_value_results = begin_value_iteration_v(ttt_mdp_oplayer, term_board, 1.0; θ = 0.0)

# ╔═╡ 3bea1145-2387-4674-9ac4-cad212694e72
#can alternate this as well until each player's policy is identical for every state similar to how the value iteration stops running

# ╔═╡ 07e29c7d-ec52-4abd-9d60-46bdfc51ba15
# ╠═╡ disabled = true
#=╠═╡
board2 = UInt8.([1, 0, 0, 1, 2, 0, 0, 0, 0])
  ╠═╡ =#

# ╔═╡ 55636bc9-0500-4327-894a-a6b2a67f4ef9
#=╠═╡
eval_value_policy(board2, oplayer_value_results, "value_o_vs_x1")
  ╠═╡ =#

# ╔═╡ 514e48df-9fdd-41d8-bf7b-d3562531c91c
ttt_mdp_xplayer = make_ttt_mdp_xplayer(convertπ(oplayer_value_results[3]))

# ╔═╡ 19fbbb78-7e8d-4f04-b0fb-dd940dc316b7
xplayer_value_results = begin_value_iteration_v(ttt_mdp_xplayer, term_board, 1.0; θ = 0.0)

# ╔═╡ bd9b454c-5038-469c-97af-595c6c91cf4b
ttt_mdp_oplayer2 = make_ttt_mdp_oplayer(convertπ(xplayer_value_results[3]))

# ╔═╡ 91bbd3df-8a06-4e16-b8b4-46887f8b7e8c
oplayer_value_results2 = begin_value_iteration_v(ttt_mdp_oplayer2, term_board, 1.0; θ = 0.0)

# ╔═╡ 72b5a805-a908-4f16-ba29-212f5c464baf
compactions = [k => (oplayer_value_results[3][k], oplayer_value_results2[3][k]) for k in keys(oplayer_value_results[3])]

# ╔═╡ 148360dc-b9eb-484b-a8c0-6cec9edaca24
filter(a -> a[2][1] != a[2][2], compactions)

# ╔═╡ a7d3ac39-4317-428b-8a66-6a353a8a1ca5
board3 = UInt8.([0, 0, 0, 0, 0, 0, 0, 0, 0])

# ╔═╡ f726ddb2-e33f-40e0-8c91-b3a2163e7db1
eval_value_policy(board3, xplayer_value_results, "value_x_vs_o")

# ╔═╡ be0ab5f8-a89f-4127-96ee-3d9a52f6887a
function make_ttt_ptf()
	function get_transitions(board::S) where S
		moves = findall(==(0), board)
		isempty(moves) && return Dict{Tuple{S, Float64, S, UInt8}, Float64}()
		mapreduce(mergewith(+), moves) do move
			(newboard, r, active) = ttt_step(board, move)
			Dict((newboard, r, board, move) => 1.)
		end
	end

	#only calculate transitions from valid states for x player
	ptf = mapreduce(get_transitions, mergewith(+), active_ttt_boards)
	sa_keys = get_sa_keys(ptf)

	return (ptr = ptf, sa_keys = sa_keys)
end

# ╔═╡ 8568dd44-ad15-42a6-9aff-62c41d2ff739
const x_vs_random_ptf = make_ttt_ptf(active_x_boards, b -> (b .== 0) ./ count(==(0), b))

# ╔═╡ 9b726b74-0e54-4031-b48b-f99248363962
ttt_value_results = begin_value_iteration_v(x_vs_random_ptf, ttt_environment.term_board, 0.9; θ = 0.0, nmax=Inf, Vinit = 0.0)

# ╔═╡ 16c09b54-23cf-46bc-8dfe-77c78065e8cb
eval_value_policy(board3, ttt_value_results, "value_x_vs_random")

# ╔═╡ 47492b1f-2ff4-4f98-9489-68b2d8bc45ac
const o_vs_random_ptf = make_ttt_ptf(active_o_boards, b -> (b .== 0) ./ count(==(0), b))

# ╔═╡ 19fbb0b8-bc03-4203-a65d-0b1516b73174
o_vs_random_value_results = begin_value_iteration_v(o_vs_random_ptf, ttt_environment.term_board, 1.0; θ = 0.0, nmax=Inf, Vinit = 0.0, invert_state = s -> -1.0)

# ╔═╡ f9063856-b2bf-4b01-90cf-2420d53405d2
o_vs_x1_ptf = make_ttt_ptf(active_o_boards, b -> convertπ(ttt_value_results[3])[b])

# ╔═╡ 540af2b5-9f16-4c9c-8134-d5b6ccdd7d40
o_vs_x1_value_results = begin_value_iteration_v(o_vs_x1_ptf, ttt_environment.term_board, 1.0; θ = 0.0, nmax=Inf, Vinit = 0.0, invert_state = s -> -1.0)

# ╔═╡ 28729f3c-2f68-4399-afe6-2c56a76cb3cc
const selfplay_ptf = make_ttt_ptf()

# ╔═╡ 1539ff60-3082-4e5c-ad52-dbb93299bac2
selfplay_value_results = begin_value_iteration_v(selfplay_ptf, ttt_environment.term_board, 1.0; θ = 0.0, nmax=Inf, Vinit = 0.0, invert_state = s -> is_o_move(s) ? -1.0 : 1.0)

# ╔═╡ 93ca1d37-1f88-4c9d-958d-8ee30c9ee079
selfplay_value_π(b) = select_action(value_policy_output(selfplay_value_results, b)[1])

# ╔═╡ df2f8ceb-e231-4580-8faf-0e73209d8d4b
x_vs_random_value_π(b) = select_action(value_policy_output(ttt_value_results, b)[1])

# ╔═╡ a7e03990-4cb3-4d1a-9cbe-a362ec8870cd
o_vs_random_value_π(b) = select_action(value_policy_output(o_vs_random_value_results, b)[1])

# ╔═╡ cb707369-245d-455d-a6b0-8b62b4f13635
get_ttt_matchup_statistics(selfplay_value_π, b -> select_action(o_vs_x1_results.eval_board(b)[1]))

# ╔═╡ 88faed7e-9e0d-48a2-8992-72f20854157f
get_ttt_matchup_statistics(selfplay_value_π, get_random_move)

# ╔═╡ cdf467f0-1dec-46cb-a354-8fde5eb22e09
get_ttt_matchup_statistics(x_vs_random_value_π, get_random_move)

# ╔═╡ d2abed56-7b34-4885-96ed-9c295870d061
get_ttt_matchup_statistics(x_vs_random_value_π, selfplay_value_π)

# ╔═╡ b771489e-7bd8-4977-bc26-f667bb036b82
get_ttt_matchup_statistics(selfplay_value_π, selfplay_value_π)

# ╔═╡ 49de73c3-dcbe-4012-a997-924e06e6f912
get_ttt_matchup_statistics(get_random_move, selfplay_value_π)

# ╔═╡ 737d4566-a737-46d1-87f0-c691c7a12525
get_ttt_matchup_statistics(get_random_move, o_vs_random_value_π)

# ╔═╡ 3b403f52-c12e-4477-9597-b1ba89096738
const boardnodes = Dict(begin
		moves = findall(==(0), b)
		nextboards = if isempty(moves) 
			Set{SVector{9, UInt8}}()
		else
			Set(symmetric_move(b, a) for a in moves)
		end
		b => nextboards
	
	end
	for b in active_ttt_boards)

# ╔═╡ 9373e86e-2bdf-4d71-ab48-181be977f8ba
# ╠═╡ disabled = true
#=╠═╡
@bind board4raw heatmap_board("fjehjkwio6786fe", zeros(9), ones(9))
  ╠═╡ =#

# ╔═╡ 64b56556-2c3e-4f6f-b874-50c48ac4b439
#=╠═╡
board4 = UInt8.(board4raw)
  ╠═╡ =#

# ╔═╡ b19df237-4158-4168-9736-280f05c29a2e
#=╠═╡
checkboard(state_symmetry_lookup[mapboard(board4)][1])
  ╠═╡ =#

# ╔═╡ a21a92d2-cd52-47ad-9043-78f2e1f59ab3
#should address this problem of having values for states that should be terminal.  The value of every terminal state should be 0.0 and the symmetry map should turn every such state into the terminal state.  Also states where more than one player has 3 in a row should be eliminated from the MDP

# ╔═╡ da67b5bb-3b44-462a-86b5-3e536545b0fa
#=╠═╡
eval_value_policy(board4, selfplay_ttt_value_results, "value_selfplay")
  ╠═╡ =#

# ╔═╡ c6781d81-6497-41b0-ad4b-1248b7212d21
#next step is to implement the HTML program for adding moves to the state and updating a board object.  Ideally we could recompute the policy as well but another cell could actually update the style for these grid elements which would change the appearance.  Yeah so I can make the HTML where the bound variable is the board and then another cell styles that board with the correct policy.  But then I would need to just stick with one policy per board.  Also wanna implement the reset button.

# ╔═╡ 262c8cad-ff83-42ea-a6fc-b763611d8688
#=╠═╡
(value = minimaxvalues[state_symmetry_lookup[mapboard(board4)][1]], actions =  show_policy(board4, s -> apply_sym_π(minimax_policy, board4)))
  ╠═╡ =#

# ╔═╡ 44d6a906-2966-4342-8b24-48682dfc4db7
show_policy(board, f) = heatmap_board(hash(f), board, f(board))

# ╔═╡ 9b1ec178-bebf-489b-8376-58b574d3c9dd
show_policy(ttt_selfplay[2][end], board3, "self_play_actor_critic")

# ╔═╡ de982a01-2d17-40fc-a005-a1d500ae38bf
function get_minimax_policy(minimaxvalues, board)
	moves = check_available_moves(board)
	c = is_o_move(board) ? -1.0 : 1.0
	prefs = [begin
		newboard = first(move(board, a))
		if haskey(minimaxvalues, newboard)
			c*minimaxvalues[newboard]
		else
			-Inf
		end
	end
	for a in UInt8.(1:9)]

	v = soft_max(1e2*prefs)
end

# ╔═╡ fac24b16-ca02-4255-bd5f-ac8995e2b52f
function minimax(board, o_max_player::Bool, boardvalues)
	c = o_max_player ? -1.0 : 1.0
	nextboards = boardnodes[board]
	if any(checkboard(board)) || isempty(nextboards)
		r = c*get_reward_x(board)
		boardvalues[board] = r
		return r
	end
	
	(value, f) = if (is_o_move(board) == o_max_player) #maximizing player
		(-Inf, max)
	else
		(Inf, min)
	end

	for newboard in nextboards
		value = f(value, minimax(newboard, o_max_player, boardvalues))
	end
	boardvalues[board] = value
	return value
end

# ╔═╡ 40500856-73f6-47ab-97d2-afd69eaf6d95
function run_minimax(startboard)
	boardvalues = Dict{SVector{9, UInt8}, Float64}()
	v = minimax(startboard, is_o_move(startboard), boardvalues)
	π = Dict(board => get_minimax_policy(boardvalues, board) for board in keys(boardvalues))
	return (v, boardvalues, π)
end

# ╔═╡ 811fcaed-fcbb-4109-bf79-05cf1bfec645
(baseval, minimaxvalues, minimax_policy) = run_minimax(ttt_environment.init_board)

# ╔═╡ f7ede764-5ad8-426b-a805-cc21b622d977
md"""
# Results Caching
"""

# ╔═╡ 2e2435bc-ca24-4b1f-87bb-4d20e7a346d8
racetrack_optimize_λ_plots = Dict()

# ╔═╡ 8afb8301-d2b9-4719-9337-3e6de5e2a535
eval_racetrack_plots = Dict()

# ╔═╡ 805b6220-0a14-4f2a-bbb1-7ba13ac1749b
blackjack_optimize_λ_plots = Dict()

# ╔═╡ 3ea08816-705e-4be7-a175-dbd3f3e4c17d
md"""
# Misc Utilities/Functions
"""

# ╔═╡ 5d50a5d0-8fe2-4c6e-b76c-d5614e4fd884
#for displaying plots that do not load by default when the notebook first runs.  Displays a placeholder markdown and then if the counter is more than 0 runs the function f with the provided arguments and caches the result in the appropriate dictionary
function show_or_lookup_plot(buttoncounter::Integer, args::Tuple, kwargs::NamedTuple, dict::Dict, f::Function, name::AbstractString)
	buttoncounter == 0 && return md"""
								 #### Placeholder for $name plot.  Click above button to run
								 """
	haskey(dict, (args, kwargs)) && return dict[(args, kwargs)]

	p = f(args...; kwargs...)
	dict[(args, kwargs)] = p
end

# ╔═╡ 617dba19-2819-4317-a652-e39235030aa9
show_or_lookup_plot(run_eval_racetrack, (track1,), (max_episodes = 1000, maxsteps = 10_000, termination_threshold = (episode = 100, reward = -500), λlist = [0.2, 0.4, 0.5, 0.6, 0.7, 0.8]), eval_racetrack_plots, eval_racetrack, "racetrack episode progress")

# ╔═╡ 8e10be80-6902-46df-ab72-1a999dd44d2e
show_or_lookup_plot(run_racetrack_optimize, (track1, [0.3, 0.5, 0.8], [0.3, 0.5]), (max_episodes = 1000, maxsteps = 10_000, termination_threshold = (episode = 100, reward = -500), λlist = [0.2, 0.4, 0.5, 0.6, 0.7, 0.8]), racetrack_optimize_λ_plots, racetrack_optimize_λ, "racetrack optimize λ plot")

# ╔═╡ 6046893f-2f7a-40cc-8844-22c62f2e2660
show_or_lookup_plot(blackjackruncount, (2. .^ (-3:-1), 2. .^ (-3:-1)), (max_episodes = 1_000_000, λlist = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]), blackjack_optimize_λ_plots, blackjack_optimize_λ, "blackjack optimize λ plot")

# ╔═╡ 1227cfdb-19ea-4df8-80ae-724ef403d5c9
md"""
## Tic Tac Toe Board Visualization
"""

# ╔═╡ 544ee0c2-6ebd-4878-b5fd-799f489e9171
md"""
### Style and JavaScript
"""

# ╔═╡ fac4c6d1-44b2-408b-bea5-1f11baae2e82
const base_cell_style = HTML("""
		<style>
		.grid-container {
			margin: 10px;
			display: grid;
			justify-content: center;
			align-content: center;
			grid-template-columns: repeat(3, auto);
			background-color: rgb(31, 31, 31);
		}

		.grid-container .gridcell.x::before,
		.grid-container .gridcell.x::after,
		.grid-container.x .gridcell:hover:not(.x):not(.o)::before,
		.grid-container.x .gridcell:hover:not(.x):not(.o)::after {
			content: '';
			position: absolute;
			background-color: black;
			width: 10%;
			height: 90%;
		}

		.grid-container .gridcell.x::before,
		.grid-container.x .gridcell:hover::before {
			transform: rotate(45deg);
		}

		.grid-container .gridcell.x::after,
		.grid-container.x .gridcell:hover::after {
			transform: rotate(-45deg);
		}

		.grid-container .gridcell.o::before, 
		.grid-container.o .gridcell:hover:not(.x):not(.o)::before
		{
			content: '';
			background-color: rgba(1, 1, 1, 0);
			border: 10px solid black;
			border-radius:50%;
			width: 65%;
			height: 65%;
		}

		.grid-container.x .gridcell:hover:not(.x):not(.o)::before,
		.grid-container.x .gridcell:hover:not(.x):not(.o)::after {
			background-color: gray;
		}

		.grid-container.o .gridcell:hover:not(.x):not(.o)::before {
			border-color: gray;
		}
		
		.gridcell {
			border: 1px solid black;
			display: flex;
			justify-content: center;
			align-items: center;
			position: relative;
			cursor: pointer;
			width: vw/10;
			height: vw/10;
		}

		.gridcell.x, .gridcell.o {
			cursor: not-allowed;
		}

		.gridcell:first-child,
		.gridcell:nth-child(2),
		.gridcell:nth-child(3) {
			border-top: none;
		}

		.gridcell:nth-child(3),
		.gridcell:nth-child(6),
		.gridcell:nth-child(9) {
			border-right: none;
		}

		.gridcell:nth-child(7),
		.gridcell:nth-child(8),
		.gridcell:nth-child(9) {
			border-bottom: none;
		}

		.gridcell:nth-child(1),
		.gridcell:nth-child(4),
		.gridcell:nth-child(7) {
			border-left: none;
		}
	</style>
""")

# ╔═╡ 26e388a2-b715-428b-96e2-64bd49b936de
function make_board_script(name) 
	"""
<script>
	const resetButton = document.querySelector(".$name .resetButton");
	console.log("got button")
	console.log(resetButton)
	resetButton.addEventListener("click", resetClick);
	resetButton.onclick = console.log("clicked");
	
	const X_CLASS = 'x'
	const CIRCLE_CLASS = 'o'
	const span = currentScript.parentElement
	const board = document.querySelector('.grid-container.$name')
	const cells = [...board.children];
	
	let circleTurn 

	span.value = [$(zeros(Int64, 9)), '$name']
	span.dispatchEvent(new CustomEvent('input'))

	cells.forEach ((child) => {
		child.addEventListener('click', handleClick, {once: true});    
	})

	function resetClick(e) {
		console.log('button pushed')
		restart()
	}

	function restart() {
		circleTurn = false
		cells.forEach((cell) => {
			var index = cells.indexOf(cell);
			cell.classList.remove(X_CLASS);
			cell.classList.remove(CIRCLE_CLASS);
			cell.removeEventListener('click', handleClick);
			cell.addEventListener('click', handleClick, {once: true});
			span.value[0][index] = 0;
		})
		setBoardHoverClass()
		span.dispatchEvent(new CustomEvent('input'))
	}

	function handleClick(e) {
		const cell = e.target;
		const index = cells.indexOf(cell);
		console.log('cell ', index, ' clicked');
		const currentClass = circleTurn ? CIRCLE_CLASS : X_CLASS;
		const fillValue = circleTurn ? 2 : 1;
		placeMark(cell, currentClass);
		swapTurns();
		setBoardHoverClass();
		span.value[0][index] = fillValue;
		span.dispatchEvent(new CustomEvent('input'));
	}

	function placeMark(cell, currentClass) {
		cell.classList.add(currentClass)
	}

	function setBoardHoverClass() {
		board.classList.remove(X_CLASS)
		board.classList.remove(CIRCLE_CLASS)
		if (circleTurn) {
			board.classList.add(CIRCLE_CLASS)
		} else {
			board.classList.add(X_CLASS)
		}
				
	}

	function swapTurns() {
		circleTurn = !circleTurn
	}
	
</script>
"""
end

# ╔═╡ b45b9df1-c1ae-440f-829b-312178d55b94
md"""
### Board Display and Control
"""

# ╔═╡ 7558d7f1-d8a0-4e7c-b411-8801021f2a25
md"""
### Restyling Utilities
"""

# ╔═╡ c7b74124-c448-466f-905c-d78e44370590
const no_color = "rgba(0, 0, 0, 0)"

# ╔═╡ 3db231d5-dc5f-434a-ac83-d3fb5cd125ee
joinelements(a, b) =  """$a \n $b"""

# ╔═╡ 20e27028-cdd7-433f-ace3-a053b14e22f7
make_elems(f, iter) = mapreduce(f, joinelements, iter)

# ╔═╡ 49608c2c-b66f-4dbf-a99f-d589e0143f8a
function colorcell(name, i, c)
	"""
	.grid-container.$name .gridcell:nth-child($i) {
		background-color: $c;
	}
	"""
end

# ╔═╡ 284df137-a066-4fd1-a7ac-32b319f65e75
function colorboard(name::AbstractString, colors::AbstractVector{T}) where T <: AbstractString
	HTML("""
	<style>
	$(make_elems(i -> colorcell(name, i, colors[i]), 1:9))
	</style>
	""")
end

# ╔═╡ 2baab643-1b70-442b-96e8-1eb0ee0090ad
#option to just make every cell the same color
colorboard(name, color) = colorboard(name, fill(color, 9))

# ╔═╡ a8520c73-60f6-4d9f-9949-9f75e7345c58
#display boards in rows that wrap to the next line
function displayboards(boards)
	HTML("""
	<span class=multiboard>
	$(reduce(joinelements, boards))
	</span>
	<style>
		.multiboard {
			display: flex;
			flex-wrap: wrap;
		}
	</style>
""")
end

# ╔═╡ 905c92e5-9130-4353-8bc1-69d80b8f7735
function resize_board(name, cellsize)
	HTML("""
	<style>
	.grid-container.$name .gridcell {
			width: $(cellsize)px;
			height: $(cellsize)px;
		}
	.grid-container.$name .gridcell.o::before, 
	.grid-container.$name.o .gridcell:hover:not(.x):not(.o)::before
	{
		border: $(cellsize/10)px solid black;
	}
	.grid-container.$name.o .gridcell:hover:not(.x):not(.o)::before {
			border-color: gray;
		}
	.$name .resetButton {
		font-size: $(min(20, cellsize/3))px;
	}
	.$name .board-value {
		font-size: $(min(20, cellsize/4))px;
	}
	</style>
""")
end

# ╔═╡ 081139f2-a2be-4a84-bb73-cb3a8c3f7974
resize_boards(boardnames::Union{AbstractVector{T}, Base.Generator}, size) where T <: AbstractString = HTML(reduce(joinelements, (resize_board(b, size).content for b in boardnames)))

# ╔═╡ 45c3e544-6cc9-4694-b0ff-c7d876fac5de
function annotate_value(name, str)
	"""
	<style>
	.$name .board-value::after {
		content: '$str';
		background-color: "rgba(0, 0, 0, 0)";
		font-weight: normal;
		color: rgb(180, 180, 180);
		font-family: Arial;
		text-shadow: 1px 2px 1px black;
	}
	</style>
"""
end

# ╔═╡ b3846537-df26-4e3d-b336-0990a544c2f9
value_board(name, v) = annotate_value(name, "Value Est: $v")

# ╔═╡ 9e9c655b-035e-4de7-bb67-7f8c5f8d76a3
prb_to_color(p::AbstractFloat) = "rgb(40, $(max(40, .9*round(Int64, 255*(p .^(1/2))))), 40)"

# ╔═╡ 55267cb3-1089-4146-9325-b8eb0ad38f4f
makecolors(prbs::AbstractVector{T}) where T <: AbstractFloat = prb_to_color.(prbs)

# ╔═╡ 6b72f9a0-41ab-4245-a6a2-83b9d19154d1
colorboard(name::AbstractString, prbs::AbstractVector{T}) where T <: AbstractFloat = colorboard(name, makecolors(prbs)) 

# ╔═╡ abe86494-c43c-4999-9d0e-4d11f6e6292d
#color a TTT board with action probabilities based on a policy function
function style_value_policy(get_value_policy, board, boardname)
	(prbs, v) = try get_value_policy(board) catch; (zeros(9), "Invalid State") end
	c = colorboard(boardname, prbs).content
	htmlstr = if isa(v, Real)
		joinelements(c, value_board(boardname, round(v, sigdigits = 2)))
	else
		joinelements(c, value_board(boardname, v))
	end
	HTML(htmlstr)
end

# ╔═╡ d3abee1c-21ec-4e24-be88-996324991d2e
randomclassname(n = 20) = string(rand('a':'z'), String(rand(['a':'z'; '0':'9'; '_'; '-'], 20)))

# ╔═╡ 957d0392-d627-4d47-95bf-ef927129279a
function make_ttt_board_raw(board; colors = ["rgba(0, 0, 0, 0)" for _ in 1:9], cellsize = 100, name = randomclassname(), boardtitle = "")
	function makehtmlcell(v)
		str = if v == 1
			" x"
		elseif v == 2
			" o"
		else
			""
		end
		"""<div class = "gridcell$str"></div>"""
	end
	gridstr(board) = is_o_move(board) ? "o" : "x"
	function makecontainer(board, name)
		"""
		<div class = "grid-container $name $(gridstr(board))">
			$(makecells(board))
		</div>
		"""
	end
	
	makecells(board) = make_elems(makehtmlcell, board)

	board = """
	<span class = $name>
	<div>$boardtitle</div>
	<div class = "board-value"></div>
	$(makecontainer(board, name))
	</span>
	$(colorboard(name, colors).content)
	$(resize_board(name, cellsize).content)
	<style>
		$name {
			display: flex;
			flex-direction: column;
		}
	</style>
	"""
	(board = board, id = name)
end

# ╔═╡ 4cd527c7-6e6e-47bf-971e-6256801005e8
function displayexamplegame(xplayer::PolicyResultsTTT, oselect::Function; cellsize = 50)
	game = run_ttt_game(b -> select_action(xplayer.eval_board(b)[1]), oselect)
	gameboards = [(board, make_ttt_board_raw(board, cellsize = cellsize)) for board in game[1]]
	style = mapreduce(joinelements, gameboards[1:end-1]) do board
		if !is_o_move(board[1])
			style_value_policy(xplayer.eval_board, board[1], board[2][2]).content
		else
			""""""
		end
	end
	base = joinelements(displayboards(a[2][1] for a in gameboards).content, style)
	outcomestr = game[2].status.x_win ? "X Wins" : game[2].status.o_win ? "O Wins" : "Draw"
	joinelements(base, annotate_value(gameboards[end][2][2], outcomestr)) |> HTML
end

# ╔═╡ 2b8a3cf6-0eef-4fd8-9704-dcfd1bd858f9
function displayexamplegame(xselect::Function, oplayer::PolicyResultsTTT; cellsize = 50)
	game = run_ttt_game(xselect, b -> select_action(oplayer.eval_board(b)[1]))
	gameboards = [(board, make_ttt_board_raw(board, cellsize = cellsize)) for board in game[1]]
	style = mapreduce(joinelements, gameboards[1:end-1]) do board
		if is_o_move(board[1])
			style_value_policy(oplayer.eval_board, board[1], board[2][2]).content
		else
			""""""
		end
	end
	base = joinelements(displayboards(a[2][1] for a in gameboards[2:end]).content, style)
	outcomestr = game[2].status.x_win ? "X Wins" : game[2].status.o_win ? "O Wins" : "Draw"
	joinelements(base, annotate_value(gameboards[end][2][2], outcomestr)) |> HTML
end

# ╔═╡ a70e9d2d-f964-4825-a66e-006d489c0538
function displayexamplegame(xplayer::PolicyResultsTTT, oplayer::PolicyResultsTTT; cellsize = 50)
	game = run_ttt_game(b -> select_action(xplayer.eval_board(b)[1]), b -> select_action(oplayer.eval_board(b)[1]))
	gameboards = [(board, make_ttt_board_raw(board, cellsize = cellsize)) for board in game[1]]
	style = mapreduce(joinelements, gameboards[1:end-1]) do board
	result = if is_o_move(board[1])
		oplayer
	else
		xplayer
	end
	style_value_policy(result.eval_board, board[1], board[2][2]).content
	end
	base = joinelements(displayboards(a[2][1] for a in gameboards).content, style)
	outcomestr = game[2].status.x_win ? "X Wins" : game[2].status.o_win ? "O Wins" : "Draw"
	joinelements(base, annotate_value(gameboards[end][2][2], outcomestr)) |> HTML
end

# ╔═╡ b8612417-77da-489c-bf66-fb99a3e0ab25
displayexamplegame(x_step_vs_random_results, get_random_move)

# ╔═╡ 0f18d16f-bfd3-4fb6-b8cf-34e76fe5ee0a
displayexamplegame(x_step_vs_random_results, o_vs_x1_results)

# ╔═╡ 09768139-1c6c-4c69-99a1-b40f35505302
displayexamplegame(x_vs_o1_results, o_vs_x1_results)

# ╔═╡ 2dd430db-1bfd-4f39-876b-0983b1c0fada
displayexamplegame(get_random_move, o_vs_x1_results)

# ╔═╡ e431002a-e31a-42ed-9f98-2e766d8e3fa8
displayexamplegame(ttt_selfplay_results, o_vs_x1_results)

# ╔═╡ 3153b4fc-1c2c-47d0-84ab-f40344df4794
displayexamplegame(ttt_selfplay_results, ttt_selfplay_results)

# ╔═╡ 9a7619e4-b6a6-4285-b7dc-0172b8fdafb1
displayexamplegame(ttt_selfplay_results, get_random_move)

# ╔═╡ 2ec09938-55c8-4259-adb7-0d35ef6a6b42
#create interactive board that works with @bind
function TTTBoard(;cellsize = 100, alignment = "flex-start")
	(board, id) = make_ttt_board_raw(zeros(9); cellsize = cellsize) #make empty board
	js = make_board_script(id)
	HTML(
		"""
		<span class = $id>
			<button class="resetButton">Reset Board</button>
			$board
			$js
		</span>
		<style>
			.$id {
				display: flex;
				flex-direction: column;
				align-items: $alignment;
			}
		</style>
		"""
	)
end

# ╔═╡ 46ce0c68-19c4-4c84-bddf-5a19542aa26b
@bind testboard TTTBoard()

# ╔═╡ ae46c33d-0119-4d3c-8a6d-bf8c58835445
get_board_status(testboard[1]), get_reward(testboard[1]), isvalid(testboard[1])

# ╔═╡ 6c9a1063-29d7-45ab-84d0-475d806ccec7
@bind xplayboard TTTBoard()

# ╔═╡ f2fc13ac-6eff-43a0-bec9-f1d14f89cf91
style_value_policy(x_step_vs_random_results.eval_board, xplayboard...)

# ╔═╡ a9efdd1c-fb11-45f4-9ef1-da5a7298b504
@bind oplayboard TTTBoard()

# ╔═╡ d219a48b-a491-44cc-b746-6c5282537855
style_value_policy(o_vs_x1_results.eval_board, oplayboard...)

# ╔═╡ f2e33f78-d61c-4337-9430-f75ab01e2d36
@bind xplayboard2 TTTBoard()

# ╔═╡ 30090262-67a1-430a-b1fc-74fb59432def
style_value_policy(x_vs_o1_results.eval_board, xplayboard2...)

# ╔═╡ 21a726ef-48f3-4e69-870c-549add227181
@bind compboard1 TTTBoard(cellsize = 70)

# ╔═╡ 28c068b4-53a7-4d35-93f8-d3a3da81d208
#generate multiple boards with custom coloring
comp1displayboards = [make_ttt_board_raw(compboard1[1], cellsize=70, boardtitle = title) for title in ["x vs random", "o vs x1", "x vs o1"]]

# ╔═╡ 5ece8d19-ac1c-4414-920a-24d833d8c7fd
displayboards([a[1] for a in comp1displayboards])

# ╔═╡ 48e5056c-192b-433a-b0bf-5deba31db223
#style each board with the appropriate policy
reduce(joinelements, [style_value_policy(result.eval_board, compboard1[1], board[2]).content for (result, board) in zip([x_step_vs_random_results, o_vs_x1_results, x_vs_o1_results], comp1displayboards)]) |> HTML

# ╔═╡ 2224d20f-c8dc-4ef6-af81-d1f832bee5ea
@bind selfplayboard TTTBoard()

# ╔═╡ 063e0ba3-69b0-4c77-8ecd-e8b70c64f7ba
style_value_policy(ttt_selfplay_results.eval_board, selfplayboard...)

# ╔═╡ f27dbf3c-df30-453c-8764-879df3b93694
md"""
#### Visualize Learned X-Player Policy Against Random  

Higher probability moves appear more green.  Click on board to change state by adding moves.  The value estimate will be 1.0 for an expected win, -0.5 for a draw, and -1.0 for a loss.

$(@bind base_board1 TTTBoard())
"""

# ╔═╡ a4261098-17d6-47e4-9649-42e09d21d1ad
style_value_policy(b -> value_policy_output(ttt_value_results, b), base_board1...)

# ╔═╡ 3efbbb22-1e34-4924-8a16-7289210437af
@bind o_vs_random_value_board TTTBoard()

# ╔═╡ c772ae36-3023-444f-a6f6-3b4c159541b8
style_value_policy(b -> value_policy_output(o_vs_random_value_results, b), o_vs_random_value_board...)

# ╔═╡ 3f305df4-8419-42a0-b4c8-3990248aa0ce
@bind o_vs_x1_value_board TTTBoard()

# ╔═╡ e7a2e7df-f7fd-49db-8339-95fe96376ab6
style_value_policy(b -> value_policy_output(o_vs_x1_value_results, b), o_vs_x1_value_board...)

# ╔═╡ 3b466d93-fb32-4081-87db-e69d8e580af4
@bind selfplay_value_board TTTBoard()

# ╔═╡ 3afd97de-fa10-4458-a272-ede2fea04118
style_value_policy(b -> value_policy_output(selfplay_value_results, b), selfplay_value_board...)

# ╔═╡ 0ab70fc3-6188-42eb-aba2-d808f319be9f
md"""
# Dependencies and Settings
"""

# ╔═╡ 16ae3aa6-8f28-4cb0-a15f-7a96c01cdaeb
import HypertextLiteral.@htl

# ╔═╡ 92b62688-2cff-4286-958f-9f4e32de52ee
function makeboardselector() 
	PlutoUI.combine() do Child
		makechild() = @htl("""<div>$(Child(Select([0x00 => "", 0x01 => "X", 0x02 => "O", ])))</div>""")
		makechildren() = mapreduce(a -> makechild(), (a, b) -> @htl("""$a \n $b"""), 1:9)
		children = makechildren()
		@htl("""
		<div class = "button-grid">
			$(children)
		</div>
		<style>
			.button-grid {
				display: grid;
				grid-template-columns: repeat(3, auto);
				width: 100px;
				height: 100px;
			}
		</style>
		""")
	end
end

# ╔═╡ f59a5dcd-9f4a-4336-a391-e64af35ef799
html"""
	<style>
		main {
			margin: 0 auto;
			max-width: 2000px;
	    	padding-left: max(80px, 10%);
	    	padding-right: max(80px, 15%);
		}
	</style>
	"""

# ╔═╡ 00000000-0000-0000-0000-000000000001
PLUTO_PROJECT_TOML_CONTENTS = """
[deps]
BenchmarkTools = "6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf"
Distributions = "31c24e10-a181-5473-b8eb-7969acd0382f"
HypertextLiteral = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
LaTeXStrings = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
LinearAlgebra = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"
PlutoPlotly = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
PlutoUI = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
ProfileCanvas = "efd6af41-a80b-495e-886c-e51b0c7d77a3"
ProgressLogging = "33c8b6b6-d38a-422a-b730-caa89a2f386c"
Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"
StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"
StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
Transducers = "28d57a85-8fef-5791-bfe6-a80928e7c999"

[compat]
BenchmarkTools = "~1.3.2"
Distributions = "~0.25.87"
HypertextLiteral = "~0.9.4"
LaTeXStrings = "~1.3.0"
PlutoPlotly = "~0.3.6"
PlutoUI = "~0.7.50"
ProfileCanvas = "~0.1.6"
ProgressLogging = "~0.1.4"
StaticArrays = "~1.5.21"
StatsBase = "~0.33.21"
Transducers = "~0.4.75"
"""

# ╔═╡ 00000000-0000-0000-0000-000000000002
PLUTO_MANIFEST_TOML_CONTENTS = """
# This file is machine-generated - editing it directly is not advised

julia_version = "1.9.0"
manifest_format = "2.0"
project_hash = "6263ef7cb0fcc213303fc01e0ce38eda42ef5f78"

[[deps.AbstractPlutoDingetjes]]
deps = ["Pkg"]
git-tree-sha1 = "8eaf9f1b4921132a4cff3f36a1d9ba923b14a481"
uuid = "6e696c72-6542-2067-7265-42206c756150"
version = "1.1.4"

[[deps.Adapt]]
deps = ["LinearAlgebra", "Requires"]
git-tree-sha1 = "cc37d689f599e8df4f464b2fa3870ff7db7492ef"
uuid = "79e6a3ab-5dfb-504d-930d-738a2a938a0e"
version = "3.6.1"
weakdeps = ["StaticArrays"]

    [deps.Adapt.extensions]
    AdaptStaticArraysExt = "StaticArrays"

[[deps.ArgCheck]]
git-tree-sha1 = "a3a402a35a2f7e0b87828ccabbd5ebfbebe356b4"
uuid = "dce04be8-c92d-5529-be00-80e4d2c0e197"
version = "2.3.0"

[[deps.ArgTools]]
uuid = "0dad84c5-d112-42e6-8d28-ef12dabb789f"
version = "1.1.1"

[[deps.Artifacts]]
uuid = "56f22d72-fd6d-98f1-02f0-08ddc0907c33"

[[deps.BangBang]]
deps = ["Compat", "ConstructionBase", "Future", "InitialValues", "LinearAlgebra", "Requires", "Setfield", "Tables", "ZygoteRules"]
git-tree-sha1 = "7fe6d92c4f281cf4ca6f2fba0ce7b299742da7ca"
uuid = "198e06fe-97b7-11e9-32a5-e1d131e6ad66"
version = "0.3.37"

[[deps.Base64]]
uuid = "2a0f44e3-6c83-55bd-87e4-b1978d98bd5f"

[[deps.Baselet]]
git-tree-sha1 = "aebf55e6d7795e02ca500a689d326ac979aaf89e"
uuid = "9718e550-a3fa-408a-8086-8db961cd8217"
version = "0.1.1"

[[deps.BenchmarkTools]]
deps = ["JSON", "Logging", "Printf", "Profile", "Statistics", "UUIDs"]
git-tree-sha1 = "d9a9701b899b30332bbcb3e1679c41cce81fb0e8"
uuid = "6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf"
version = "1.3.2"

[[deps.Calculus]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "f641eb0a4f00c343bbc32346e1217b86f3ce9dad"
uuid = "49dc2e85-a5d0-5ad3-a950-438e2897f1b9"
version = "0.5.1"

[[deps.ChainRulesCore]]
deps = ["Compat", "LinearAlgebra", "SparseArrays"]
git-tree-sha1 = "c6d890a52d2c4d55d326439580c3b8d0875a77d9"
uuid = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
version = "1.15.7"

[[deps.ColorSchemes]]
deps = ["ColorTypes", "ColorVectorSpace", "Colors", "FixedPointNumbers", "Random", "SnoopPrecompile"]
git-tree-sha1 = "aa3edc8f8dea6cbfa176ee12f7c2fc82f0608ed3"
uuid = "35d6a980-a343-548e-a6ea-1d62b119f2f4"
version = "3.20.0"

[[deps.ColorTypes]]
deps = ["FixedPointNumbers", "Random"]
git-tree-sha1 = "eb7f0f8307f71fac7c606984ea5fb2817275d6e4"
uuid = "3da002f7-5984-5a60-b8a6-cbb66c0b333f"
version = "0.11.4"

[[deps.ColorVectorSpace]]
deps = ["ColorTypes", "FixedPointNumbers", "LinearAlgebra", "SpecialFunctions", "Statistics", "TensorCore"]
git-tree-sha1 = "600cc5508d66b78aae350f7accdb58763ac18589"
uuid = "c3611d14-8923-5661-9e6a-0046d554d3a4"
version = "0.9.10"

[[deps.Colors]]
deps = ["ColorTypes", "FixedPointNumbers", "Reexport"]
git-tree-sha1 = "fc08e5930ee9a4e03f84bfb5211cb54e7769758a"
uuid = "5ae59095-9a9b-59fe-a467-6f913c188581"
version = "0.12.10"

[[deps.Compat]]
deps = ["UUIDs"]
git-tree-sha1 = "7a60c856b9fa189eb34f5f8a6f6b5529b7942957"
uuid = "34da2185-b29b-5c13-b0c7-acf172513d20"
version = "4.6.1"
weakdeps = ["Dates", "LinearAlgebra"]

    [deps.Compat.extensions]
    CompatLinearAlgebraExt = "LinearAlgebra"

[[deps.CompilerSupportLibraries_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "e66e0078-7015-5450-92f7-15fbd957f2ae"
version = "1.0.2+0"

[[deps.CompositionsBase]]
git-tree-sha1 = "455419f7e328a1a2493cabc6428d79e951349769"
uuid = "a33af91c-f02d-484b-be07-31d278c5ca2b"
version = "0.1.1"

[[deps.ConstructionBase]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "89a9db8d28102b094992472d333674bd1a83ce2a"
uuid = "187b0558-2788-49d3-abe0-74a17ed4e7c9"
version = "1.5.1"

    [deps.ConstructionBase.extensions]
    IntervalSetsExt = "IntervalSets"
    StaticArraysExt = "StaticArrays"

    [deps.ConstructionBase.weakdeps]
    IntervalSets = "8197267c-284f-5f27-9208-e0e47529a953"
    StaticArrays = "90137ffa-7385-5640-81b9-e52037218182"

[[deps.DataAPI]]
git-tree-sha1 = "e8119c1a33d267e16108be441a287a6981ba1630"
uuid = "9a962f9c-6df0-11e9-0e5d-c546b8b5ee8a"
version = "1.14.0"

[[deps.DataStructures]]
deps = ["Compat", "InteractiveUtils", "OrderedCollections"]
git-tree-sha1 = "d1fff3a548102f48987a52a2e0d114fa97d730f0"
uuid = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
version = "0.18.13"

[[deps.DataValueInterfaces]]
git-tree-sha1 = "bfc1187b79289637fa0ef6d4436ebdfe6905cbd6"
uuid = "e2d170a0-9d28-54be-80f0-106bbe20a464"
version = "1.0.0"

[[deps.Dates]]
deps = ["Printf"]
uuid = "ade2ca70-3891-5945-98fb-dc099432e06a"

[[deps.DefineSingletons]]
git-tree-sha1 = "0fba8b706d0178b4dc7fd44a96a92382c9065c2c"
uuid = "244e2a9f-e319-4986-a169-4d1fe445cd52"
version = "0.1.2"

[[deps.DelimitedFiles]]
deps = ["Mmap"]
git-tree-sha1 = "9e2f36d3c96a820c678f2f1f1782582fcf685bae"
uuid = "8bb1440f-4735-579b-a4ab-409b98df4dab"
version = "1.9.1"

[[deps.Distributed]]
deps = ["Random", "Serialization", "Sockets"]
uuid = "8ba89e20-285c-5b6f-9357-94700520ee1b"

[[deps.Distributions]]
deps = ["FillArrays", "LinearAlgebra", "PDMats", "Printf", "QuadGK", "Random", "SparseArrays", "SpecialFunctions", "Statistics", "StatsBase", "StatsFuns", "Test"]
git-tree-sha1 = "13027f188d26206b9e7b863036f87d2f2e7d013a"
uuid = "31c24e10-a181-5473-b8eb-7969acd0382f"
version = "0.25.87"

    [deps.Distributions.extensions]
    DistributionsChainRulesCoreExt = "ChainRulesCore"
    DistributionsDensityInterfaceExt = "DensityInterface"

    [deps.Distributions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    DensityInterface = "b429d917-457f-4dbc-8f4c-0cc954292b1d"

[[deps.DocStringExtensions]]
deps = ["LibGit2"]
git-tree-sha1 = "2fb1e02f2b635d0845df5d7c167fec4dd739b00d"
uuid = "ffbed154-4ef7-542d-bbb7-c09d3a79fcae"
version = "0.9.3"

[[deps.Downloads]]
deps = ["ArgTools", "FileWatching", "LibCURL", "NetworkOptions"]
uuid = "f43a241f-c20a-4ad4-852c-f6b1247861c6"
version = "1.6.0"

[[deps.DualNumbers]]
deps = ["Calculus", "NaNMath", "SpecialFunctions"]
git-tree-sha1 = "5837a837389fccf076445fce071c8ddaea35a566"
uuid = "fa6b7ba4-c1ee-5f82-b5fc-ecf0adba8f74"
version = "0.6.8"

[[deps.FileWatching]]
uuid = "7b1f6079-737a-58dc-b8bc-7a2ca5c1b5ee"

[[deps.FillArrays]]
deps = ["LinearAlgebra", "Random", "SparseArrays", "Statistics"]
git-tree-sha1 = "fc86b4fd3eff76c3ce4f5e96e2fdfa6282722885"
uuid = "1a297f60-69ca-5386-bcde-b61e274b549b"
version = "1.0.0"

[[deps.FixedPointNumbers]]
deps = ["Statistics"]
git-tree-sha1 = "335bfdceacc84c5cdf16aadc768aa5ddfc5383cc"
uuid = "53c48c17-4a7d-5ca2-90c5-79b7896eea93"
version = "0.8.4"

[[deps.Future]]
deps = ["Random"]
uuid = "9fa8497b-333b-5362-9e8d-4d0656e87820"

[[deps.HypergeometricFunctions]]
deps = ["DualNumbers", "LinearAlgebra", "OpenLibm_jll", "SpecialFunctions"]
git-tree-sha1 = "432b5b03176f8182bd6841fbfc42c718506a2d5f"
uuid = "34004b35-14d8-5ef3-9330-4cdb6864b03a"
version = "0.3.15"

[[deps.Hyperscript]]
deps = ["Test"]
git-tree-sha1 = "8d511d5b81240fc8e6802386302675bdf47737b9"
uuid = "47d2ed2b-36de-50cf-bf87-49c2cf4b8b91"
version = "0.0.4"

[[deps.HypertextLiteral]]
deps = ["Tricks"]
git-tree-sha1 = "c47c5fa4c5308f27ccaac35504858d8914e102f9"
uuid = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
version = "0.9.4"

[[deps.IOCapture]]
deps = ["Logging", "Random"]
git-tree-sha1 = "f7be53659ab06ddc986428d3a9dcc95f6fa6705a"
uuid = "b5f81e59-6552-4d32-b1f0-c071b021bf89"
version = "0.2.2"

[[deps.InitialValues]]
git-tree-sha1 = "4da0f88e9a39111c2fa3add390ab15f3a44f3ca3"
uuid = "22cec73e-a1b8-11e9-2c92-598750a2cf9c"
version = "0.3.1"

[[deps.InteractiveUtils]]
deps = ["Markdown"]
uuid = "b77e0a4c-d291-57a0-90e8-8db25a27a240"

[[deps.IrrationalConstants]]
git-tree-sha1 = "630b497eafcc20001bba38a4651b327dcfc491d2"
uuid = "92d709cd-6900-40b7-9082-c6be49f344b6"
version = "0.2.2"

[[deps.IteratorInterfaceExtensions]]
git-tree-sha1 = "a3f24677c21f5bbe9d2a714f95dcd58337fb2856"
uuid = "82899510-4779-5014-852e-03e436cf321d"
version = "1.0.0"

[[deps.JLLWrappers]]
deps = ["Preferences"]
git-tree-sha1 = "abc9885a7ca2052a736a600f7fa66209f96506e1"
uuid = "692b3bcd-3c85-4b1f-b108-f13ce0eb3210"
version = "1.4.1"

[[deps.JSON]]
deps = ["Dates", "Mmap", "Parsers", "Unicode"]
git-tree-sha1 = "3c837543ddb02250ef42f4738347454f95079d4e"
uuid = "682c06a0-de6a-54ab-a142-c8b1cf79cde6"
version = "0.21.3"

[[deps.LaTeXStrings]]
git-tree-sha1 = "f2355693d6778a178ade15952b7ac47a4ff97996"
uuid = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
version = "1.3.0"

[[deps.LibCURL]]
deps = ["LibCURL_jll", "MozillaCACerts_jll"]
uuid = "b27032c2-a3e7-50c8-80cd-2d36dbcbfd21"
version = "0.6.3"

[[deps.LibCURL_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll", "Zlib_jll", "nghttp2_jll"]
uuid = "deac9b47-8bc7-5906-a0fe-35ac56dc84c0"
version = "7.84.0+0"

[[deps.LibGit2]]
deps = ["Base64", "NetworkOptions", "Printf", "SHA"]
uuid = "76f85450-5226-5b5a-8eaa-529ad045b433"

[[deps.LibSSH2_jll]]
deps = ["Artifacts", "Libdl", "MbedTLS_jll"]
uuid = "29816b5a-b9ab-546f-933c-edad1886dfa8"
version = "1.10.2+0"

[[deps.Libdl]]
uuid = "8f399da3-3557-5675-b5ff-fb832c97cbdb"

[[deps.LinearAlgebra]]
deps = ["Libdl", "OpenBLAS_jll", "libblastrampoline_jll"]
uuid = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"

[[deps.LogExpFunctions]]
deps = ["DocStringExtensions", "IrrationalConstants", "LinearAlgebra"]
git-tree-sha1 = "0a1b7c2863e44523180fdb3146534e265a91870b"
uuid = "2ab3a3ac-af41-5b50-aa03-7779005ae688"
version = "0.3.23"

    [deps.LogExpFunctions.extensions]
    LogExpFunctionsChainRulesCoreExt = "ChainRulesCore"
    LogExpFunctionsChangesOfVariablesExt = "ChangesOfVariables"
    LogExpFunctionsInverseFunctionsExt = "InverseFunctions"

    [deps.LogExpFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    ChangesOfVariables = "9e997f8a-9a97-42d5-a9f1-ce6bfc15e2c0"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.Logging]]
uuid = "56ddb016-857b-54e1-b83d-db4d58db5568"

[[deps.MIMEs]]
git-tree-sha1 = "65f28ad4b594aebe22157d6fac869786a255b7eb"
uuid = "6c6e2e6c-3030-632d-7369-2d6c69616d65"
version = "0.1.4"

[[deps.MacroTools]]
deps = ["Markdown", "Random"]
git-tree-sha1 = "42324d08725e200c23d4dfb549e0d5d89dede2d2"
uuid = "1914dd2f-81c6-5fcd-8719-6d5c9610ff09"
version = "0.5.10"

[[deps.Markdown]]
deps = ["Base64"]
uuid = "d6f4376e-aef5-505a-96c1-9c027394607a"

[[deps.MbedTLS_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "c8ffd9c3-330d-5841-b78e-0817d7145fa1"
version = "2.28.2+0"

[[deps.MicroCollections]]
deps = ["BangBang", "InitialValues", "Setfield"]
git-tree-sha1 = "629afd7d10dbc6935ec59b32daeb33bc4460a42e"
uuid = "128add7d-3638-4c79-886c-908ea0c25c34"
version = "0.1.4"

[[deps.Missings]]
deps = ["DataAPI"]
git-tree-sha1 = "f66bdc5de519e8f8ae43bdc598782d35a25b1272"
uuid = "e1d29d7a-bbdc-5cf2-9ac0-f12de2c33e28"
version = "1.1.0"

[[deps.Mmap]]
uuid = "a63ad114-7e13-5084-954f-fe012c677804"

[[deps.MozillaCACerts_jll]]
uuid = "14a3606d-f60d-562e-9121-12d972cd8159"
version = "2022.10.11"

[[deps.NaNMath]]
deps = ["OpenLibm_jll"]
git-tree-sha1 = "0877504529a3e5c3343c6f8b4c0381e57e4387e4"
uuid = "77ba4419-2d1f-58cd-9bb1-8ffee604a2e3"
version = "1.0.2"

[[deps.NetworkOptions]]
uuid = "ca575930-c2e3-43a9-ace4-1e988b2c1908"
version = "1.2.0"

[[deps.OpenBLAS_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "Libdl"]
uuid = "4536629a-c528-5b80-bd46-f80d51c5b363"
version = "0.3.21+4"

[[deps.OpenLibm_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "05823500-19ac-5b8b-9628-191a04bc5112"
version = "0.8.1+0"

[[deps.OpenSpecFun_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "JLLWrappers", "Libdl", "Pkg"]
git-tree-sha1 = "13652491f6856acfd2db29360e1bbcd4565d04f1"
uuid = "efe28fd5-8261-553b-a9e1-b2916fc3738e"
version = "0.5.5+0"

[[deps.OrderedCollections]]
git-tree-sha1 = "d321bf2de576bf25ec4d3e4360faca399afca282"
uuid = "bac558e1-5e72-5ebc-8fee-abe8a469f55d"
version = "1.6.0"

[[deps.PDMats]]
deps = ["LinearAlgebra", "SparseArrays", "SuiteSparse"]
git-tree-sha1 = "67eae2738d63117a196f497d7db789821bce61d1"
uuid = "90014a1f-27ba-587c-ab20-58faa44d9150"
version = "0.11.17"

[[deps.Parameters]]
deps = ["OrderedCollections", "UnPack"]
git-tree-sha1 = "34c0e9ad262e5f7fc75b10a9952ca7692cfc5fbe"
uuid = "d96e819e-fc66-5662-9728-84c9c7592b0a"
version = "0.12.3"

[[deps.Parsers]]
deps = ["Dates", "SnoopPrecompile"]
git-tree-sha1 = "478ac6c952fddd4399e71d4779797c538d0ff2bf"
uuid = "69de0a69-1ddd-5017-9359-2bf0b02dc9f0"
version = "2.5.8"

[[deps.Pkg]]
deps = ["Artifacts", "Dates", "Downloads", "FileWatching", "LibGit2", "Libdl", "Logging", "Markdown", "Printf", "REPL", "Random", "SHA", "Serialization", "TOML", "Tar", "UUIDs", "p7zip_jll"]
uuid = "44cfe95a-1eb2-52ea-b672-e2afdf69b78f"
version = "1.9.0"

[[deps.PlotlyBase]]
deps = ["ColorSchemes", "Dates", "DelimitedFiles", "DocStringExtensions", "JSON", "LaTeXStrings", "Logging", "Parameters", "Pkg", "REPL", "Requires", "Statistics", "UUIDs"]
git-tree-sha1 = "56baf69781fc5e61607c3e46227ab17f7040ffa2"
uuid = "a03496cd-edff-5a9b-9e67-9cda94a718b5"
version = "0.8.19"

[[deps.PlutoPlotly]]
deps = ["AbstractPlutoDingetjes", "Colors", "Dates", "HypertextLiteral", "InteractiveUtils", "LaTeXStrings", "Markdown", "PlotlyBase", "PlutoUI", "Reexport"]
git-tree-sha1 = "dec81dcd52748ffc59ce3582e709414ff78d947f"
uuid = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
version = "0.3.6"

[[deps.PlutoUI]]
deps = ["AbstractPlutoDingetjes", "Base64", "ColorTypes", "Dates", "FixedPointNumbers", "Hyperscript", "HypertextLiteral", "IOCapture", "InteractiveUtils", "JSON", "Logging", "MIMEs", "Markdown", "Random", "Reexport", "URIs", "UUIDs"]
git-tree-sha1 = "5bb5129fdd62a2bbbe17c2756932259acf467386"
uuid = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
version = "0.7.50"

[[deps.Preferences]]
deps = ["TOML"]
git-tree-sha1 = "47e5f437cc0e7ef2ce8406ce1e7e24d44915f88d"
uuid = "21216c6a-2e73-6563-6e65-726566657250"
version = "1.3.0"

[[deps.Printf]]
deps = ["Unicode"]
uuid = "de0858da-6303-5e67-8744-51eddeeeb8d7"

[[deps.Profile]]
deps = ["Printf"]
uuid = "9abbd945-dff8-562f-b5e8-e1ebf5ef1b79"

[[deps.ProfileCanvas]]
deps = ["Base64", "JSON", "Pkg", "Profile", "REPL"]
git-tree-sha1 = "e42571ce9a614c2fbebcaa8aab23bbf8865c624e"
uuid = "efd6af41-a80b-495e-886c-e51b0c7d77a3"
version = "0.1.6"

[[deps.ProgressLogging]]
deps = ["Logging", "SHA", "UUIDs"]
git-tree-sha1 = "80d919dee55b9c50e8d9e2da5eeafff3fe58b539"
uuid = "33c8b6b6-d38a-422a-b730-caa89a2f386c"
version = "0.1.4"

[[deps.QuadGK]]
deps = ["DataStructures", "LinearAlgebra"]
git-tree-sha1 = "6ec7ac8412e83d57e313393220879ede1740f9ee"
uuid = "1fd47b50-473d-5c70-9696-f719f8f3bcdc"
version = "2.8.2"

[[deps.REPL]]
deps = ["InteractiveUtils", "Markdown", "Sockets", "Unicode"]
uuid = "3fa0cd96-eef1-5676-8a61-b3b8758bbffb"

[[deps.Random]]
deps = ["SHA", "Serialization"]
uuid = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"

[[deps.Reexport]]
git-tree-sha1 = "45e428421666073eab6f2da5c9d310d99bb12f9b"
uuid = "189a3867-3050-52da-a836-e630ba90ab69"
version = "1.2.2"

[[deps.Requires]]
deps = ["UUIDs"]
git-tree-sha1 = "838a3a4188e2ded87a4f9f184b4b0d78a1e91cb7"
uuid = "ae029012-a4dd-5104-9daa-d747884805df"
version = "1.3.0"

[[deps.Rmath]]
deps = ["Random", "Rmath_jll"]
git-tree-sha1 = "f65dcb5fa46aee0cf9ed6274ccbd597adc49aa7b"
uuid = "79098fc4-a85e-5d69-aa6a-4863f24498fa"
version = "0.7.1"

[[deps.Rmath_jll]]
deps = ["Artifacts", "JLLWrappers", "Libdl", "Pkg"]
git-tree-sha1 = "6ed52fdd3382cf21947b15e8870ac0ddbff736da"
uuid = "f50d1b31-88e8-58de-be2c-1cc44531875f"
version = "0.4.0+0"

[[deps.SHA]]
uuid = "ea8e919c-243c-51af-8825-aaa63cd721ce"
version = "0.7.0"

[[deps.Serialization]]
uuid = "9e88b42a-f829-5b0c-bbe9-9e923198166b"

[[deps.Setfield]]
deps = ["ConstructionBase", "Future", "MacroTools", "StaticArraysCore"]
git-tree-sha1 = "e2cc6d8c88613c05e1defb55170bf5ff211fbeac"
uuid = "efcf1570-3423-57d1-acb7-fd33fddbac46"
version = "1.1.1"

[[deps.SnoopPrecompile]]
deps = ["Preferences"]
git-tree-sha1 = "e760a70afdcd461cf01a575947738d359234665c"
uuid = "66db9d55-30c0-4569-8b51-7e840670fc0c"
version = "1.0.3"

[[deps.Sockets]]
uuid = "6462fe0b-24de-5631-8697-dd941f90decc"

[[deps.SortingAlgorithms]]
deps = ["DataStructures"]
git-tree-sha1 = "a4ada03f999bd01b3a25dcaa30b2d929fe537e00"
uuid = "a2af1166-a08f-5f64-846c-94a0d3cef48c"
version = "1.1.0"

[[deps.SparseArrays]]
deps = ["Libdl", "LinearAlgebra", "Random", "Serialization", "SuiteSparse_jll"]
uuid = "2f01184e-e22b-5df5-ae63-d93ebab69eaf"

[[deps.SpecialFunctions]]
deps = ["IrrationalConstants", "LogExpFunctions", "OpenLibm_jll", "OpenSpecFun_jll"]
git-tree-sha1 = "ef28127915f4229c971eb43f3fc075dd3fe91880"
uuid = "276daf66-3868-5448-9aa4-cd146d93841b"
version = "2.2.0"
weakdeps = ["ChainRulesCore"]

    [deps.SpecialFunctions.extensions]
    SpecialFunctionsChainRulesCoreExt = "ChainRulesCore"

[[deps.SplittablesBase]]
deps = ["Setfield", "Test"]
git-tree-sha1 = "e08a62abc517eb79667d0a29dc08a3b589516bb5"
uuid = "171d559e-b47b-412a-8079-5efa626c420e"
version = "0.1.15"

[[deps.StaticArrays]]
deps = ["LinearAlgebra", "Random", "StaticArraysCore", "Statistics"]
git-tree-sha1 = "63e84b7fdf5021026d0f17f76af7c57772313d99"
uuid = "90137ffa-7385-5640-81b9-e52037218182"
version = "1.5.21"

[[deps.StaticArraysCore]]
git-tree-sha1 = "6b7ba252635a5eff6a0b0664a41ee140a1c9e72a"
uuid = "1e83bf80-4336-4d27-bf5d-d5a4f845583c"
version = "1.4.0"

[[deps.Statistics]]
deps = ["LinearAlgebra", "SparseArrays"]
uuid = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
version = "1.9.0"

[[deps.StatsAPI]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "45a7769a04a3cf80da1c1c7c60caf932e6f4c9f7"
uuid = "82ae8749-77ed-4fe6-ae5f-f523153014b0"
version = "1.6.0"

[[deps.StatsBase]]
deps = ["DataAPI", "DataStructures", "LinearAlgebra", "LogExpFunctions", "Missings", "Printf", "Random", "SortingAlgorithms", "SparseArrays", "Statistics", "StatsAPI"]
git-tree-sha1 = "d1bf48bfcc554a3761a133fe3a9bb01488e06916"
uuid = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
version = "0.33.21"

[[deps.StatsFuns]]
deps = ["HypergeometricFunctions", "IrrationalConstants", "LogExpFunctions", "Reexport", "Rmath", "SpecialFunctions"]
git-tree-sha1 = "f625d686d5a88bcd2b15cd81f18f98186fdc0c9a"
uuid = "4c63d2b9-4356-54db-8cca-17b64c39e42c"
version = "1.3.0"

    [deps.StatsFuns.extensions]
    StatsFunsChainRulesCoreExt = "ChainRulesCore"
    StatsFunsInverseFunctionsExt = "InverseFunctions"

    [deps.StatsFuns.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.SuiteSparse]]
deps = ["Libdl", "LinearAlgebra", "Serialization", "SparseArrays"]
uuid = "4607b0f0-06f3-5cda-b6b1-a6196a1729e9"

[[deps.SuiteSparse_jll]]
deps = ["Artifacts", "Libdl", "Pkg", "libblastrampoline_jll"]
uuid = "bea87d4a-7f5b-5778-9afe-8cc45184846c"
version = "5.10.1+6"

[[deps.TOML]]
deps = ["Dates"]
uuid = "fa267f1f-6049-4f14-aa54-33bafae1ed76"
version = "1.0.3"

[[deps.TableTraits]]
deps = ["IteratorInterfaceExtensions"]
git-tree-sha1 = "c06b2f539df1c6efa794486abfb6ed2022561a39"
uuid = "3783bdb8-4a98-5b6b-af9a-565f29a5fe9c"
version = "1.0.1"

[[deps.Tables]]
deps = ["DataAPI", "DataValueInterfaces", "IteratorInterfaceExtensions", "LinearAlgebra", "OrderedCollections", "TableTraits", "Test"]
git-tree-sha1 = "1544b926975372da01227b382066ab70e574a3ec"
uuid = "bd369af6-aec1-5ad0-b16a-f7cc5008161c"
version = "1.10.1"

[[deps.Tar]]
deps = ["ArgTools", "SHA"]
uuid = "a4e569a6-e804-4fa4-b0f3-eef7a1d5b13e"
version = "1.10.0"

[[deps.TensorCore]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1feb45f88d133a655e001435632f019a9a1bcdb6"
uuid = "62fd8b95-f654-4bbd-a8a5-9c27f68ccd50"
version = "0.1.1"

[[deps.Test]]
deps = ["InteractiveUtils", "Logging", "Random", "Serialization"]
uuid = "8dfed614-e22c-5e08-85e1-65c5234f0b40"

[[deps.Transducers]]
deps = ["Adapt", "ArgCheck", "BangBang", "Baselet", "CompositionsBase", "DefineSingletons", "Distributed", "InitialValues", "Logging", "Markdown", "MicroCollections", "Requires", "Setfield", "SplittablesBase", "Tables"]
git-tree-sha1 = "c42fa452a60f022e9e087823b47e5a5f8adc53d5"
uuid = "28d57a85-8fef-5791-bfe6-a80928e7c999"
version = "0.4.75"

[[deps.Tricks]]
git-tree-sha1 = "aadb748be58b492045b4f56166b5188aa63ce549"
uuid = "410a4b4d-49e4-4fbc-ab6d-cb71b17b3775"
version = "0.1.7"

[[deps.URIs]]
git-tree-sha1 = "074f993b0ca030848b897beff716d93aca60f06a"
uuid = "5c2747f8-b7ea-4ff2-ba2e-563bfd36b1d4"
version = "1.4.2"

[[deps.UUIDs]]
deps = ["Random", "SHA"]
uuid = "cf7118a7-6976-5b1a-9a39-7adc72f591a4"

[[deps.UnPack]]
git-tree-sha1 = "387c1f73762231e86e0c9c5443ce3b4a0a9a0c2b"
uuid = "3a884ed6-31ef-47d7-9d2a-63182c4928ed"
version = "1.0.2"

[[deps.Unicode]]
uuid = "4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5"

[[deps.Zlib_jll]]
deps = ["Libdl"]
uuid = "83775a58-1f1d-513f-b197-d71354ab007a"
version = "1.2.13+0"

[[deps.ZygoteRules]]
deps = ["ChainRulesCore", "MacroTools"]
git-tree-sha1 = "977aed5d006b840e2e40c0b48984f7463109046d"
uuid = "700de1a5-db45-46bc-99cf-38207098b444"
version = "0.2.3"

[[deps.libblastrampoline_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850b90-86db-534c-a0d3-1478176c7d93"
version = "5.7.0+0"

[[deps.nghttp2_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850ede-7688-5339-a07c-302acd2aaf8d"
version = "1.48.0+0"

[[deps.p7zip_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "3f19e933-33d8-53b3-aaab-bd5110c3b7a0"
version = "17.4.0+0"
"""

# ╔═╡ Cell order:
# ╟─36a6e43f-6bcf-4c27-bfbb-047760e77ada
# ╟─2501cbc0-9772-4b2f-ab01-ef7903e62950
# ╟─7a6fb1f0-fc3c-4c29-a6d9-769d32ca98a9
# ╠═1b89a5be-d4f6-43b6-b778-0895d77d0962
# ╠═759afa53-2b01-4d9b-b398-80120626634f
# ╠═97046258-7753-4edb-b0c9-0981d587ad35
# ╠═423321cc-1c8c-44a0-bd8e-a4d3cb68962b
# ╠═980af3e7-2f1c-49be-8f6b-fc61271dff52
# ╠═edb145d7-95e0-44c9-a60f-57d517edb0c7
# ╠═23291878-b49d-4626-8313-1e7b2d1f8d44
# ╟─9d815d9c-6e5a-473e-a395-6f92d504dbf3
# ╟─9c342958-1971-48ec-b919-5dfdcbc915a4
# ╟─e5faaa1b-88cb-43e2-8d04-8972b58b4bda
# ╟─406638af-1e08-44d2-9ee4-97aa9294a94b
# ╟─aa450da4-fe84-4eea-b6c4-9820b7982437
# ╟─f924eb30-d1cc-4941-8fb5-ff70ad425ab9
# ╠═b406577a-5478-42fd-8ed0-e36b5574cfc6
# ╠═2b11ef08-288f-4110-b741-ba580782b6a7
# ╠═71973c41-5fbb-40bf-8cc9-e063c7372a1c
# ╠═cb83e57f-3b3b-44ae-8c75-69b9b12ec6f5
# ╟─49a1d508-b491-4d3a-8415-f5def06884e9
# ╟─c6b61679-8a06-47ae-abab-6997ad5cbfea
# ╠═c2d8a622-b8f9-454b-9fd1-dc940280624c
# ╠═5f91ce14-c9d4-4818-8955-8e7381b4943b
# ╠═a45c1930-ad70-44f4-a6bc-10ccb03f65ab
# ╟─71c8d422-8177-4324-b048-98dd39198fee
# ╟─a206c759-3f6e-4003-8cba-5f6ce6742646
# ╟─2e6d0374-1c93-48c8-b8ba-dd1a0c682d01
# ╟─cc45091e-b889-4d5a-9eef-84d80f792046
# ╠═5bebef34-e266-4c18-95c3-28e1f1cb4b64
# ╠═d26cd4cb-9a62-4a03-8b71-b415c9be79f6
# ╠═5f042c7e-45ad-4f8f-94d8-9133e67dd0f6
# ╟─b72e030f-7d52-481f-b4f7-2b16b227e547
# ╟─87876de4-ca40-4736-81a8-bb26bc273d89
# ╟─ce33f710-fd9d-4dfa-acda-40204e54d518
# ╟─f4b6f10b-4cd0-4be6-98ec-4d4ffb696392
# ╠═bf656d55-19cb-4052-baaa-0896ab6d23a0
# ╟─4cbdb082-22ba-49e9-a6ed-4380917625ac
# ╠═58ad84b0-f9c9-424e-8c05-0b15fbe7b349
# ╠═8f11b8dc-2c3e-41a5-8dbb-9af06235fe85
# ╟─70d4e199-2941-46dd-99c0-0f0520bf976b
# ╠═72900e88-98f4-4879-b005-d79ef6c7ee7f
# ╟─511a847f-234c-465e-8f4a-688e79d9b975
# ╠═533cbf4b-ac14-47eb-98cf-e569f32cc215
# ╟─735b548a-88f5-4a30-ab8f-dfb3d6401b2b
# ╟─79c85707-ea09-4f6b-ad51-a2683c3923c0
# ╟─7ccadf01-fbba-4dfd-a5ad-770dab9946f9
# ╟─beb01fb8-c77d-4b5c-a66d-3812415e04a3
# ╟─68e6f17e-8c87-40f0-a673-1115ecd1b71d
# ╟─692c1043-4eaf-491e-b8fe-368618867f99
# ╟─4c34640f-efa2-4e1d-8a70-0acd2ce45428
# ╟─273e7735-91a6-45cd-81ad-49d0da665143
# ╠═8a4e2b43-15fe-49c4-a487-497875246f82
# ╠═ac43b613-5c74-45bd-a49e-5b30bb19f52d
# ╠═1683b216-d310-4c66-81ba-0329898d90dd
# ╠═d538939d-df32-4766-b3c7-f9fc5af564df
# ╠═461e27bb-c38b-4dc6-aa68-d5d76ff79cbf
# ╠═ee23064b-499a-4061-bfed-242ccbcbf25e
# ╠═3825159e-a5db-45c8-b2bc-193b4494b53d
# ╠═ed2785a8-0fed-4052-8371-0e34982e8800
# ╠═f5745c5a-8dd9-4827-a222-df4036498a0e
# ╠═a81d2380-b853-432e-9592-d5461daad7b2
# ╠═fc68dd3e-e42d-4642-a5ba-bac9ba1b432d
# ╠═a7316ca6-28ae-4ee0-b0be-e8d451beb17f
# ╟─5ce1af6b-847c-47f0-a6ca-867c35948caa
# ╠═ff60f48e-2055-4bb6-8cf4-fac1da45200b
# ╠═a79ed238-a6d3-40e6-9bf3-351b7494b446
# ╠═76af787d-7a3d-4c65-ab6a-898fba148705
# ╠═0538aaf4-716b-4f3c-aa7e-dcb1dd456172
# ╠═d37e21ac-b82a-423f-8719-b513bddf433d
# ╠═d314361e-4d4f-413b-b935-1e88c1112fa0
# ╠═9040a58b-afd7-49cd-a253-054a5b26c603
# ╠═e5a0a3fc-2eb3-4f31-8ab6-4a3130c70932
# ╠═85fc29c9-e5ca-4bc8-b607-51d75906a1f2
# ╟─6e2e9c99-8664-40f2-a1df-bd182db9859e
# ╟─617dba19-2819-4317-a652-e39235030aa9
# ╠═b50282ed-e599-4687-bfbc-0ac9c4f30c84
# ╟─aeffb168-06d2-484e-beea-b507f329e4b8
# ╟─8e10be80-6902-46df-ab72-1a999dd44d2e
# ╟─80e40d2b-a67b-46eb-86fd-294c0a87a80f
# ╠═8edb3337-0902-45fa-a5b0-c7cc3d40f97f
# ╠═37dc5518-d378-41fd-b0ef-bc5e3b1b3687
# ╠═0ac08421-20d2-4e56-bce8-1bc47b36fe2e
# ╠═064b06ae-903b-4430-b925-534925bca733
# ╠═9be279fa-9325-4eb1-8c73-7742c066664d
# ╠═6353a374-9eba-4184-a528-f8ca9f32dfe5
# ╠═b265b8e6-994a-4be2-a7c9-05adef570fda
# ╠═460d9e76-9841-4fb8-8e35-0efbbf6f9f08
# ╠═8ea91577-57eb-4afc-8919-95bd16ae6865
# ╠═6324046e-c766-444f-8a74-f6e3569154fa
# ╠═7f8ea283-8b42-4bb7-8d49-a54855a98c5d
# ╠═8f133852-12da-41b3-8071-51a12211f432
# ╠═ebeabff8-4779-49e6-a04f-16a76e0b9b04
# ╠═097b8fc1-b4a4-4b93-bc08-2ceebd5d759a
# ╠═519e6da0-efbf-4b0a-a61c-5849ba403389
# ╠═4c4ba58e-e3b7-4d02-81ae-b8d753487caa
# ╠═06d508ea-640d-4e55-b3b6-05c929f82c3b
# ╟─0c3714fd-821a-4dae-8d1e-1db35ebef315
# ╟─6046893f-2f7a-40cc-8844-22c62f2e2660
# ╠═8cb58177-cc29-4bf0-af2f-704bebb9871f
# ╠═7550213b-8174-4623-9abc-9dcbdc0351a8
# ╠═0b6fb5bf-c21e-4727-aafb-65fc3f7b76fb
# ╟─2b964c13-c961-4ed9-8b66-a6715ff7d0ef
# ╟─078a1739-911c-4673-821b-488a878bab37
# ╠═205fcf67-e79c-4f20-bb8b-ddb6b980ed9d
# ╠═5f917c40-c1b2-4dd8-ac81-8e955d6af7af
# ╠═7f4b6d93-53dd-466e-8401-24c1a59c32d9
# ╠═eadea57c-b3b6-44c0-bf5d-ed57fec3ff7c
# ╠═15ac8e2e-9ec6-4723-a0ba-0bd29a37f64e
# ╠═9fe44113-8232-4579-85b3-65725d30fd46
# ╠═a0ddc362-cfeb-4585-86bd-0ad003e3d61d
# ╠═cfdc3298-118e-4b87-b0c0-f46afe573a12
# ╠═bd53c7c0-e77a-46a5-be88-0fd97e80c02c
# ╠═2deb0cf0-d83d-4a51-b76b-093c53f09c77
# ╠═62eb2abe-e418-4826-9c99-7d3b1df500eb
# ╠═727ea9ce-a670-4e8c-b2f2-8a19477d9a33
# ╠═9b8187b5-aba6-404c-b232-faf4ed200c89
# ╠═57f7fddd-01cb-489f-98cf-f1ce07977eb1
# ╠═83988884-e42e-44aa-9ead-0b81258160cb
# ╠═76e7a54d-8db6-43be-a994-e8469fce6760
# ╠═f53f464b-f9f6-4f34-b35f-e7e8cabc3600
# ╠═ef4b5d88-cae0-466b-baac-90c3cf8f65df
# ╠═ec5bb245-051a-4c80-aa98-76b618ec65c6
# ╠═a12c0d95-8e64-4a82-9c31-62604a1a03ce
# ╠═d7850e01-34ff-48aa-b366-ee33584372a7
# ╠═58668067-0efe-4f23-94f4-d010016f568d
# ╠═d4f866a4-7e40-4e60-8382-cd78b2fe0a86
# ╠═475099d3-d3a4-4757-865d-1e1b4e7da10e
# ╠═e43b5edb-9a3b-4b88-9ef4-87ca1f267b2a
# ╠═bbb82a7d-8c54-4cdd-95fe-f6719ecaa5fd
# ╠═c9ea915e-2bab-4648-8388-658ebf796d78
# ╠═97d7bfbd-f821-4201-9d68-9d1654d2a86b
# ╠═6ce26626-1c4c-41ea-b23b-cf6d5bac230b
# ╠═3b3973d7-e26e-4e0d-b767-a4247304b9a0
# ╠═52fb1724-3e09-4514-b315-ffc83ba88ebe
# ╠═2d811d01-d2c4-477d-8a06-fcf94e5ad798
# ╠═a7be257c-d2d9-4d55-a498-3e4db491e644
# ╠═2f147876-144d-4c7f-9c5a-affc3476753c
# ╠═90170d3b-25d9-4fbc-a131-f8def2187435
# ╟─46ce0c68-19c4-4c84-bddf-5a19542aa26b
# ╠═ae46c33d-0119-4d3c-8a6d-bf8c58835445
# ╠═08f28c09-708c-40e0-ba16-71135fb438b4
# ╠═85a07db1-5135-472e-95e6-9d4e85928350
# ╠═3e44c2aa-2d59-4dba-ba3c-0db868c6e460
# ╠═e59efa67-9f52-4493-ba47-84f3ad8a87a2
# ╠═f9a2136c-7b8d-4427-9fd9-040084dc96fe
# ╠═762bed69-bdd8-443b-9526-bf10442eef65
# ╠═ffb0962d-bf78-40ea-aa56-7eb198ef5234
# ╠═5481a261-45b3-4afd-9c28-03c11f884e69
# ╠═9ea87c25-9acb-4ae9-8f39-6c684dbc6b19
# ╠═b89107cf-45e7-44da-9b73-40b5c995eb8e
# ╠═6e556453-694d-48f7-8c8c-adc04f2d80df
# ╠═a8137758-4ff3-42d2-a163-8e1daffdc869
# ╠═52f0411c-ac13-41c0-bb7b-8fc755849a35
# ╠═5bdcfe04-f343-414a-87cf-a2a7170cf9c4
# ╠═70e2766d-bc8f-42e6-89fd-7036a0020177
# ╠═ea0e22de-a9c1-4a02-8a01-97990348d571
# ╠═2d7651cb-0226-44c4-ac3c-5de1b98c513a
# ╠═d856f23f-72ef-44e5-aaf6-0213997f783f
# ╠═a8f7db69-4a50-4e0d-9be6-cf4eef5233c6
# ╠═4d67501b-1f27-40e3-8d76-f406c0de9e1c
# ╠═a3e91897-ebc0-4b1f-bec2-c99338c92fb0
# ╠═7c5234d2-7a19-46cd-81cd-daeb15327594
# ╠═875d894d-055d-4f48-91e5-19871f4ee370
# ╠═23f8b2d3-7f76-4dd5-a301-95afe719ec30
# ╠═c8885191-44a7-448c-a546-d5fb50254616
# ╠═f29d1813-fa74-46ce-b3ac-3a06a5cd104e
# ╟─3f927561-7b13-4f19-946a-67c310c60255
# ╠═04d93927-6206-4d32-91fb-81b73568f1f7
# ╠═2da0cccb-5e1a-43a7-b485-8b62a8d70d10
# ╠═626940b4-eeb9-4ee6-9f27-b6446f014572
# ╠═11d113f1-c1f0-4a58-a3b2-44c70b21cdac
# ╠═959e4a18-fe6e-4c9c-b9bf-f752108fd2dd
# ╠═b6f3d5b6-74b7-4211-b236-203881a97c38
# ╟─8731821b-d82a-4697-be21-522583d7dbab
# ╠═892df402-df32-4344-9201-0458b90fed26
# ╠═6c9a1063-29d7-45ab-84d0-475d806ccec7
# ╠═f2fc13ac-6eff-43a0-bec9-f1d14f89cf91
# ╠═abe86494-c43c-4999-9d0e-4d11f6e6292d
# ╠═3728a916-a502-48ec-9c84-5b2e7e4df61c
# ╠═e6cd6459-6e50-4c5c-b6d3-a55706bbb257
# ╠═f88a0889-f3d5-4d75-a745-c734e4420802
# ╠═a9efdd1c-fb11-45f4-9ef1-da5a7298b504
# ╠═d219a48b-a491-44cc-b746-6c5282537855
# ╠═4a999b16-1427-4b30-a2be-1919b0ad2caf
# ╠═aea8317c-fb5f-4817-b927-1c6d48072ea7
# ╠═73693dd6-f07c-4625-9d84-f356c91f5735
# ╟─f2e33f78-d61c-4337-9430-f75ab01e2d36
# ╠═30090262-67a1-430a-b1fc-74fb59432def
# ╟─9e32d0d4-bdbb-46a7-ad3c-34184cea0b92
# ╟─21a726ef-48f3-4e69-870c-549add227181
# ╟─5ece8d19-ac1c-4414-920a-24d833d8c7fd
# ╠═28c068b4-53a7-4d35-93f8-d3a3da81d208
# ╠═48e5056c-192b-433a-b0bf-5deba31db223
# ╠═4cd527c7-6e6e-47bf-971e-6256801005e8
# ╠═2b8a3cf6-0eef-4fd8-9704-dcfd1bd858f9
# ╠═a70e9d2d-f964-4825-a66e-006d489c0538
# ╠═b8612417-77da-489c-bf66-fb99a3e0ab25
# ╠═0f18d16f-bfd3-4fb6-b8cf-34e76fe5ee0a
# ╠═09768139-1c6c-4c69-99a1-b40f35505302
# ╠═2dd430db-1bfd-4f39-876b-0983b1c0fada
# ╠═9e9d1b3a-d8a5-45f2-87b1-20f7edf56793
# ╠═f271a2a6-1720-4cb1-99e0-9aff3fab171c
# ╠═75f13e3d-90a5-461e-9f64-479a01465fab
# ╠═938e33dd-c129-40d0-a72e-b7d1f3f770ff
# ╠═a278e854-e230-42aa-97a2-0f5b7d1815af
# ╠═702f39a7-f921-4f20-90d2-9b7ec493230e
# ╠═088c2166-17ab-4c22-b621-6421316ebd52
# ╠═75377f64-9b4b-47ec-b25e-b17d42407fad
# ╠═1d0fe433-0bca-4083-842b-dc209298af13
# ╠═a0740d6d-d034-4037-b410-f31f76b207f5
# ╠═87fd6b09-fd43-454c-a589-38dab5ccf71a
# ╟─124a38c0-dd7a-43b2-9f86-5a41261736e0
# ╠═90385599-9db0-4463-8063-81a41266712f
# ╠═3c6243f6-973c-4521-9881-c66f94de83a0
# ╠═a45949bc-878b-47fc-a239-cb8bb110046b
# ╠═afb19bfb-0e0d-4d3b-8db5-c9f1a91b61ae
# ╠═72025689-c50d-4f74-8ddb-5709b43b39ed
# ╠═2616cfe3-c66a-4d00-8caa-1b92e8bcfa6d
# ╠═eef60b59-8595-454c-89a3-f02729fbd1d5
# ╠═2900dc4e-eed2-4a5c-a026-d1d1bdaf62b9
# ╠═2ec47c25-ec71-4cd9-b1b7-14ae8ee3492a
# ╠═8464adca-a780-4da1-bb1c-05db6277634c
# ╟─2224d20f-c8dc-4ef6-af81-d1f832bee5ea
# ╠═063e0ba3-69b0-4c77-8ecd-e8b70c64f7ba
# ╠═e431002a-e31a-42ed-9f98-2e766d8e3fa8
# ╠═3153b4fc-1c2c-47d0-84ab-f40344df4794
# ╠═46277863-5e64-4e23-87e3-7980110a8742
# ╠═fe2d0874-ac60-421d-9632-9310af0b8d1f
# ╠═9a7619e4-b6a6-4285-b7dc-0172b8fdafb1
# ╠═cf7dd9c3-6c51-40c9-bbea-08ccf4d3a8b1
# ╠═f17cc08e-e0e8-4bad-8f81-08eb1d03d827
# ╠═f70dcbbd-e871-4f6d-9287-b468d511dc7b
# ╠═c5502e6e-751a-4e24-851d-6cc1ed119c3f
# ╠═b72f2485-e9a9-4b2c-a126-d7a42a3d6ba6
# ╠═83ccd36d-96c8-4665-9148-bdf95eb8dda1
# ╠═0d234b25-994f-4649-ac05-0df2dcf12264
# ╠═506a7c77-0d48-47a1-b3fd-d203101b9106
# ╠═60652571-4e4e-4d68-bec2-3b3fb6db0b1d
# ╟─d7976b1a-41a7-4d3d-9b0d-7b5a7d87da54
# ╠═f1d6e558-6e7c-4238-983a-b756d4ea9450
# ╠═8568dd44-ad15-42a6-9aff-62c41d2ff739
# ╠═9b726b74-0e54-4031-b48b-f99248363962
# ╠═f4e8f556-b131-4c86-b245-0f7a09760353
# ╠═8a9bbf5b-18f3-4cbe-ac15-d2d88b68f8bd
# ╠═f0a358b5-9733-4593-8174-fa44c87d93b7
# ╟─f27dbf3c-df30-453c-8764-879df3b93694
# ╠═a4261098-17d6-47e4-9649-42e09d21d1ad
# ╠═47492b1f-2ff4-4f98-9489-68b2d8bc45ac
# ╠═19fbb0b8-bc03-4203-a65d-0b1516b73174
# ╟─3efbbb22-1e34-4924-8a16-7289210437af
# ╠═c772ae36-3023-444f-a6f6-3b4c159541b8
# ╠═f9063856-b2bf-4b01-90cf-2420d53405d2
# ╠═540af2b5-9f16-4c9c-8134-d5b6ccdd7d40
# ╠═3f305df4-8419-42a0-b4c8-3990248aa0ce
# ╠═e7a2e7df-f7fd-49db-8339-95fe96376ab6
# ╠═bd9b454c-5038-469c-97af-595c6c91cf4b
# ╠═73f0ca1b-b331-4682-9741-3399a0ac3d46
# ╠═91bbd3df-8a06-4e16-b8b4-46887f8b7e8c
# ╠═72b5a805-a908-4f16-ba29-212f5c464baf
# ╠═3bea1145-2387-4674-9ac4-cad212694e72
# ╠═148360dc-b9eb-484b-a8c0-6cec9edaca24
# ╠═07e29c7d-ec52-4abd-9d60-46bdfc51ba15
# ╠═55636bc9-0500-4327-894a-a6b2a67f4ef9
# ╠═514e48df-9fdd-41d8-bf7b-d3562531c91c
# ╠═19fbbb78-7e8d-4f04-b0fb-dd940dc316b7
# ╠═a7d3ac39-4317-428b-8a66-6a353a8a1ca5
# ╠═f726ddb2-e33f-40e0-8c91-b3a2163e7db1
# ╠═16c09b54-23cf-46bc-8dfe-77c78065e8cb
# ╠═9b1ec178-bebf-489b-8376-58b574d3c9dd
# ╠═be0ab5f8-a89f-4127-96ee-3d9a52f6887a
# ╠═28729f3c-2f68-4399-afe6-2c56a76cb3cc
# ╠═1539ff60-3082-4e5c-ad52-dbb93299bac2
# ╠═3b466d93-fb32-4081-87db-e69d8e580af4
# ╠═3afd97de-fa10-4458-a272-ede2fea04118
# ╠═93ca1d37-1f88-4c9d-958d-8ee30c9ee079
# ╠═df2f8ceb-e231-4580-8faf-0e73209d8d4b
# ╠═a7e03990-4cb3-4d1a-9cbe-a362ec8870cd
# ╠═cb707369-245d-455d-a6b0-8b62b4f13635
# ╠═88faed7e-9e0d-48a2-8992-72f20854157f
# ╠═cdf467f0-1dec-46cb-a354-8fde5eb22e09
# ╠═d2abed56-7b34-4885-96ed-9c295870d061
# ╠═b771489e-7bd8-4977-bc26-f667bb036b82
# ╠═49de73c3-dcbe-4012-a997-924e06e6f912
# ╠═737d4566-a737-46d1-87f0-c691c7a12525
# ╠═3b403f52-c12e-4477-9597-b1ba89096738
# ╠═9373e86e-2bdf-4d71-ab48-181be977f8ba
# ╠═64b56556-2c3e-4f6f-b874-50c48ac4b439
# ╠═b19df237-4158-4168-9736-280f05c29a2e
# ╠═a21a92d2-cd52-47ad-9043-78f2e1f59ab3
# ╠═da67b5bb-3b44-462a-86b5-3e536545b0fa
# ╠═c6781d81-6497-41b0-ad4b-1248b7212d21
# ╠═262c8cad-ff83-42ea-a6fc-b763611d8688
# ╠═811fcaed-fcbb-4109-bf79-05cf1bfec645
# ╠═44d6a906-2966-4342-8b24-48682dfc4db7
# ╠═de982a01-2d17-40fc-a005-a1d500ae38bf
# ╠═40500856-73f6-47ab-97d2-afd69eaf6d95
# ╠═fac24b16-ca02-4255-bd5f-ac8995e2b52f
# ╟─f7ede764-5ad8-426b-a805-cc21b622d977
# ╠═2e2435bc-ca24-4b1f-87bb-4d20e7a346d8
# ╠═8afb8301-d2b9-4719-9337-3e6de5e2a535
# ╠═805b6220-0a14-4f2a-bbb1-7ba13ac1749b
# ╟─3ea08816-705e-4be7-a175-dbd3f3e4c17d
# ╠═5d50a5d0-8fe2-4c6e-b76c-d5614e4fd884
# ╟─1227cfdb-19ea-4df8-80ae-724ef403d5c9
# ╟─544ee0c2-6ebd-4878-b5fd-799f489e9171
# ╠═fac4c6d1-44b2-408b-bea5-1f11baae2e82
# ╠═26e388a2-b715-428b-96e2-64bd49b936de
# ╟─b45b9df1-c1ae-440f-829b-312178d55b94
# ╠═957d0392-d627-4d47-95bf-ef927129279a
# ╠═2ec09938-55c8-4259-adb7-0d35ef6a6b42
# ╟─7558d7f1-d8a0-4e7c-b411-8801021f2a25
# ╠═c7b74124-c448-466f-905c-d78e44370590
# ╠═3db231d5-dc5f-434a-ac83-d3fb5cd125ee
# ╠═20e27028-cdd7-433f-ace3-a053b14e22f7
# ╠═49608c2c-b66f-4dbf-a99f-d589e0143f8a
# ╠═284df137-a066-4fd1-a7ac-32b319f65e75
# ╠═2baab643-1b70-442b-96e8-1eb0ee0090ad
# ╠═a8520c73-60f6-4d9f-9949-9f75e7345c58
# ╠═905c92e5-9130-4353-8bc1-69d80b8f7735
# ╠═081139f2-a2be-4a84-bb73-cb3a8c3f7974
# ╠═b3846537-df26-4e3d-b336-0990a544c2f9
# ╠═45c3e544-6cc9-4694-b0ff-c7d876fac5de
# ╠═9e9c655b-035e-4de7-bb67-7f8c5f8d76a3
# ╠═55267cb3-1089-4146-9325-b8eb0ad38f4f
# ╠═6b72f9a0-41ab-4245-a6a2-83b9d19154d1
# ╠═d3abee1c-21ec-4e24-be88-996324991d2e
# ╠═92b62688-2cff-4286-958f-9f4e32de52ee
# ╟─0ab70fc3-6188-42eb-aba2-d808f319be9f
# ╠═d04d4234-d97f-11ed-2ea3-85ee0fc3bd70
# ╠═16ae3aa6-8f28-4cb0-a15f-7a96c01cdaeb
# ╠═f59a5dcd-9f4a-4336-a391-e64af35ef799
# ╟─00000000-0000-0000-0000-000000000001
# ╟─00000000-0000-0000-0000-000000000002
