### A Pluto.jl notebook ###
# v0.19.25

using Markdown
using InteractiveUtils

# ╔═╡ d04d4234-d97f-11ed-2ea3-85ee0fc3bd70
using PlutoUI, PlutoPlotly, Random, Distributions, StatsBase, LinearAlgebra, LaTeXStrings, Base.Threads, ProfileCanvas

# ╔═╡ 36a6e43f-6bcf-4c27-bfbb-047760e77ada
md"""
# Chapter 13 Policy Gradient Methods Introduction
Instead of selection actions based on *action-value estimates* we learn a *parameterized policy* with parameters θ.  $\pi(a|s, \mathbf{\theta}) = \text{Pr}\{A_t=a|S_t=s, \mathbf{\theta}_t=\mathbf{\theta\}}$ denotes the probability that action *a* is taken at time *t* given that the environment is in state *s* at time *t* with parameter **θ**.  

We consider methods that improve the policy parameter using the gradient of some scalar performance measure $J(\mathbf{\theta})$ with respect to the policy parameters.  We follow gradient ascent since we are trying to maximize this value and methods that use this approach are called *policy gradient methods*.  Methods that learn approximations to both policy and value functions are often called *actor-critic methods*, where 'actor' is a reference to the learned policy, and 'critic' refers to the learned value function, usually a state-value function.
# 13.1 Policy Approximation and its Advantages
"""

# ╔═╡ 2501cbc0-9772-4b2f-ab01-ef7903e62950
md"""
If the state/action space is discrete and not too large then we can have numerical preferences for each state/action pair parameterized by θ.  $h(s, a, \mathbf{\theta})$ and the corresponding policy can be to select actions according to the probability distribution generated by the soft-max.  $\pi(a|s, \mathbf{\theta}) \dot = \frac{\exp{h(s, a, \mathbf{\theta})}}{\sum_b \exp{h(s, b, \mathbf{\theta})}}$.  One advantage of using the soft-max is that the optimal policy can be stochastic or we can approach a deterministic policy by selecting the action with the highest probability.  If we include a temperature parameter in the soft-max then we can vary the same policy to be more or less stochastic as needed.

Another advantage is that for some problems the policy may be easier to approximate than the action-value function.  We can also inject some prior knowledge of the environment into how the policy is parametrized.
"""

# ╔═╡ 7a6fb1f0-fc3c-4c29-a6d9-769d32ca98a9
md"""
## Example 13.1 Short corridor gridworld
"""

# ╔═╡ 1b89a5be-d4f6-43b6-b778-0895d77d0962
abstract type LinearMove end

# ╔═╡ 759afa53-2b01-4d9b-b398-80120626634f
struct Left <: LinearMove end

# ╔═╡ 97046258-7753-4edb-b0c9-0981d587ad35
struct Right <: LinearMove end

# ╔═╡ 423321cc-1c8c-44a0-bd8e-a4d3cb68962b
function make_corridor()
	function step(s::Integer, a::LinearMove)
		f(s) = ifelse(s == 2, -1, 1) #reverse actions for 2nd state
		move(s::Integer, ::Left) = s - f(s)
		move(s::Integer, ::Right) = s + f(s) 
		s′ = max(1, move(s, a))
		(s′, -1.0)
	end
	(states = 1:3, sterm = 4, actions = [Left(), Right()], step = step)
end	

# ╔═╡ 980af3e7-2f1c-49be-8f6b-fc61271dff52
function run_corridor_episode(π; cor = make_corridor())
	s = first(cor.states)
	G = 0.0
	state_history = [s]
	rewardhistory = Vector{Float64}()
	select_action(vec) = sample(eachindex(vec), pweights(vec))
	while s != cor.sterm
		a = cor.actions[select_action(π(s))]
		(s, r) = cor.step(s, a)
		push!(state_history, s)
		push!(rewardhistory, r)
	end
	return state_history, rewardhistory
end

# ╔═╡ edb145d7-95e0-44c9-a60f-57d517edb0c7
run_corridor_episode(s -> [0.5, 0.5]) #this policy chooses randomly between both actions

# ╔═╡ 9d815d9c-6e5a-473e-a395-6f92d504dbf3
md"""
### Exercise 13.1
>*Exercise 13.1* Use your knowledge of the gridworld and its dynamics to determine an *exact* symbolic expression for the optimal probability of selecting the right action in Example 13.1

Example 13.1 is a gridworld with 3 non-terminal states and a terminal state at the far right.  The reward is -1 per step.  States 1 and 3 have actions left/right that move in the expected directions but state 2 reverses the directions.  We use a performance measure $J(\mathbf{\theta}) = v_{\pi_\theta}(S)$.  Given our feature representations of $\mathbf{x}(s, \text{right}) = [1, 0]^{\top}$ and $\mathbf{x}(s, \text{left}) = [0, 1]^{\top}$, we can only learn policies that are stochastic in terms of left/right action selection but do not vary between states.  Also observe that due to probability constraints $p_{\text{right}} = 1 - p_{\text{left}}$. For simplicity, we will use the notation $p \dot = p_{\text{left}}$.

$\begin{flalign}
v(S_1) &= p \times v(S_1) + (1-p) \times v(S_2) - 1 \tag{1} \\
v(S_2) &= p \times v(S_3) +(1-p) \times v(S_1) - 1 \tag{2} \\
v(S_3) &= p\times v(S_2) - 1 \tag{3}\\
v(S_2) &= p \times [p\times v(S_2) - 1] +(1-p) \times v(S_1) - 1  \tag{substituting 3 into 2} \\
v(S_2) &= \frac{(1-p) \times v(S_1) - (1+p)}{(1+p)(1-p)} \tag{collecting terms} \\
v(S_1) &= p \times v(S_1) + (1-p) \times \left [ \frac{(1-p) \times v(S_1) - (1+p)}{(1+p)(1-p)} \right ] - 1 \tag{substituting above into 1} \\
v(S_1) &= p \times v(S_1) + \left [ \frac{(1-p) \times v(S_1) - (1+p)}{1+p} \right ] - 1 \tag{simplifying} \\
2 &= v(S_1) (-1 + p + \frac{1-p}{1+p}) \tag{collecting v terms} \\
2 &= v(S_1) \frac{(p - 1)(1+p) + 1 - p}{1+p} \tag{combining fractions} \\
2 &= v(S_1) \frac{p^2 - p}{1+p} \tag{simpifying} \\
v(S_1) &= \frac{2(1+p)}{p^2 - p} \\
v(S_2) &= \frac{(1-p) \times [\frac{2(1+p)}{p^2 - p} ] - (1+p)}{(1+p)(1-p)} =  \frac{-\frac{2(1+p)}{p} - (1+p)}{(1+p)(1-p)} = \frac{-2(1+p) - p(1+p)}{p(1+p)(1-p)}\\
&=-\frac{2 + 3p + p^2}{p(1+p)(1-p)} = \frac{(p+2)(p+1)}{p(1+p)(p - 1)} = \frac{p+2}{p(p - 1)} \\
v(S_3) &= p\times \frac{p+2}{p(p - 1)} - 1 = \frac{p+2}{p - 1} - \frac{p-1}{p-1} = \frac{3}{p-1}\\
\end{flalign}$
In order to find the p that maximizes the expected value for state 1, we should differentiate by p and set the result to 0

$\begin{flalign}
0 &= \frac{p(p - 1) - (1+p)(2p - 1)}{(p(p-1))^2} \\
(1+p)(2p - 1) &= p(p - 1) \\
2p - 1 + 2p^2 - p &= p^2 - p \\
2p - 1 + p^2 &= 0 \\
\end{flalign}$

Using the quadratic equation, there are two solutions but since we know p has to be positive we only take that one.

$p = \frac{-2 \pm \sqrt{4 + 4}}{2} = -1 \pm \sqrt{2} \implies p = \sqrt{2} - 1 \approx 0.414$

So, in order to maximize the value at state 1, we have $p_{\text{left}} \approx 0.414$ and $p_{\text{right}} \approx 0.586$.  That also implies that $v(S_1) = \frac{2(1+p)}{p^2 - p} = \frac{2\sqrt{2}}{2 - 2 \sqrt{2} + 1 + 1 - \sqrt{2}} = \frac{2\sqrt{2}}{4 - 3\sqrt{2}} = \frac{2\sqrt{2}(4 + 3\sqrt{2})}{16 - 18}=\frac{8\sqrt{2}+12}{-2} = -6-4\sqrt{2}\approx-11.657$

If we solve the same problem at state 2 we get:

$\begin{flalign}
0 &= \frac{p^2 + 4p - 2}{(p-1)^2p^2} \\
0 &= p^2 + 4p - 2 \\
\end{flalign}$

Using the quadratic equation and keeping only the positive solution gives:
$p = \frac{-4 + \sqrt{16 + 8}}{2} = \frac{-4 + 2\sqrt{6}}{2} = \sqrt{6} - 2 \approx 0.4495$.

So, in order to maximize the value at state 1, we have $p_{\text{left}} \approx 0.4495$ and $p_{\text{right}} \approx 0.55$. Which is different from the value we got for state 1.  So There is a different optimal policy depending on the starting state.  It should be obvious for example that starting in the third state results in an optimial policy of choosing the right action every time.  The value functions for each state are plotted below.  The behavior of V(S3) is not well defined at $p=0$ because for any finite V(S2) it should be 0 but the limit approaching from the right side is -3.  This is because for $p=0$ both V(S1) and V(S2) are not finite and the episode never terminates.   
"""

# ╔═╡ e5faaa1b-88cb-43e2-8d04-8972b58b4bda
begin
	v1(p) = (2*(1+p))/(p^2 - p)
	v2(p) = (p+2)/(p*(p - 1))
	v3(p) = 3/(p-1)
	plist = 0.:0.001:1.
	traces = [scatter(x = plist, y = f.(plist), name = n) for (f, n) in zip([v1, v2, v3], ["V(S1)", "V(S2)", "V(S3)"])]
	plot(traces, Layout(yaxis_range = [-100, 0], xaxis_title = "probability of right action", yaxis_title = "State Value"))
end

# ╔═╡ 406638af-1e08-44d2-9ee4-97aa9294a94b
md"""
# 13.2 The Policy Gradient Theorem
"""

# ╔═╡ aa450da4-fe84-4eea-b6c4-9820b7982437
md"""
With continuous policy parametrization, we can smoothly very action selection probabilities by arbitrarily small amounts, something that was not possible with ϵ-greedy action selection.  Therefore stronger convergence guarantees are possible for policy-gradient methods than for action-value methods.

In the episodic case, assuming some particular non-random starting state $s_0$, we define the performance of a policy parametrized by *θ* as:

$\begin{align}
J(\mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} v_{\pi_\mathbf{\theta}}(s_0) \tag{13.4}
\end{align}$

where $v_{\pi_\mathbf{\theta}}$ is the true value function for $\pi_\mathbf{\theta}$, the policy determined by $\mathbf{\theta}$.

The *policy gradient theorem* provides an analytic expression for the gradient of performance with respect to the policy parameter that does *not* involve the derivative of the state distribution:

$\begin{align}
\nabla J(\mathbf{\theta}) \propto \sum_s \mu (s) \sum_a q_\pi (s, a) \nabla \pi (a|s,\mathbf{\theta}) \tag{13.5}
\end{align}$

where the gradients are column vectors of partial derivatives with respect to the components of $\mathbf{\theta}$.  In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1.  The distribution here $\mu$ is the on-policy distribution under $\pi$.
"""


# ╔═╡ f924eb30-d1cc-4941-8fb5-ff70ad425ab9
md"""
# 13.3 REINFORCE: Monte Carlo Policy Gradient

If we replace the true action-value function in (13.5) with a learned approximation $\hat q_\pi$, then we have a method called the *all-actions* method because the update involves the sum over all actions.  For the REINFORCE algorithm, we instead sample this value using the actual return and the policy distribution.

We can re-write (13.5) using an expected value under the policy and continue from there:

$\begin{flalign}
\nabla J(\mathbf{\theta}) & \propto \mathbb{E}_\pi \left [ \sum_a q_\pi (S_t, a) \nabla \pi(a|S_t, \mathbf{\theta}) \right ] \tag{13.6}\\
 &= \mathbb{E}_\pi \left [ \sum_a \pi(a|S_t, \mathbf{\theta}) q_\pi (S_t, a) \frac{\nabla \pi(a|S_t, \mathbf{\theta})}{\pi(a|S_t, \mathbf{\theta})} \right ] \tag{multiply and divide by policy} \\
 &= \mathbb{E}_\pi \left [ q_\pi (S_t, A_t) \frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \right ] \tag{replace a with sample under policy} \\
 &= \mathbb{E}_\pi \left [ G_t \frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \right ] \tag{replace value with sample return} \\
\end{flalign}$

Using the expression in the brackets we can write down an update rule for the parameters that can be sampled on each time step.  This is the **REINFORCE update**:

$\begin{align}
\mathbf{\theta}_{t+1} \hspace{5px} \dot = \hspace{5px} \mathbf{\theta}_t + \alpha G_t \frac{\nabla \pi(A_t|S_t, \mathbf{\theta}_t)}{\pi(A_t|S_t, \mathbf{\theta}_t)} \tag{13.8}
\end{align}$

Because it uses all future returns after step t, REINFORCE is a Monte Carlo algorithm and is well defined only for the episodic case.  For implementation purposes we can replace $\frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})}$ with $\nabla \ln \pi(A_t|S_t, \mathbf{\theta}_t)$ which is usually refered to as the *eligibility vector*.
"""

# ╔═╡ b406577a-5478-42fd-8ed0-e36b5574cfc6
select_action(vec) = wsample(eachindex(vec), vec)

# ╔═╡ 2b11ef08-288f-4110-b741-ba580782b6a7
"""
	reinforce_monte_carlo_control(π, ∇lnπ, d, s0, α, step, sterm, actions; 
                             	γ = 1.0, max_episodes = 1000, maxsteps = Inf,
								baseline = 0.0, θ = zeros(d))

Implements the REINFORCE algorithm for Monte Carlo control, which is a policy gradient method for reinforcement learning. Given a function π that maps states to probability distributions over actions, and a function ∇lnπ that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ, this function learns the optimal policy for a given Markov decision process (MDP).

Required arguments:
- π: A function that maps a state to a probability distribution over actions. This function must take two arguments: the current state and the policy parameters θ.
- ∇lnπ: A function that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ. This function must take three arguments: the action, the current state, and the policy parameters θ.
- d: An integer representing the number of policy parameters to be learned.
- s0: The initial state of the MDP.
- α: The learning rate for the policy gradient update.
- step: A function that takes a state and an action and returns the next state and the reward received. This function must take two arguments: the current state and the chosen action.
- sterm: A state representing the terminal state of the MDP.
- actions: A collection of all possible actions in the MDP.

Optional keyword arguments:
- γ: The discount factor for future rewards. Default value is 1.0.
- max_episodes: The maximum number of episodes to run the algorithm. Default value is 1000.
- maxsteps: The maximum number of steps to take in each episode. Default value is Inf.
- baseline: The baseline value for the policy gradient update. Default value is 0.0.
- θ: The initial policy parameters. Default value is a vector of zeros with length d.
"""
function reinforce_monte_carlo_control(π::Function, ∇lnπ::Function, d::Int64, s0, α, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), maxsteps = Inf, baseline = 0.0)
	rewards = zeros(max_episodes)
	
	function run_episode(maxsteps)
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s, r) = step(s0, actions[a])
		reward_history = [r]
		while s != sterm && length(state_history) < maxsteps
			a = select_action(π(s, θ))
			push!(action_history, a)
			(s, r) = step(s, actions[a])
			push!(reward_history, r)
			push!(state_history, s)
		end
		return state_history, action_history, reward_history
	end	
	for i in eachindex(rewards)
		state_history, action_history, reward_history = run_episode(maxsteps)
		G = 0.0
		#iterate through episode beginning at the end
		for i in reverse(eachindex(reward_history))
			G = (γ * G) + reward_history[i]
			θ .+= α * γ^(i-1) * (G - baseline) .* ∇lnπ(action_history[i], state_history[i], θ)
		end
		rewards[i] = sum(reward_history)
	end
	return rewards, θ
end

# ╔═╡ 71973c41-5fbb-40bf-8cc9-e063c7372a1c
"""
    soft_max!(v::AbstractVector, out::AbstractVector)

Calculate the softmax of a vector `v` and store the result in another vector `out` of equal length.

# Arguments
- `v::AbstractVector`: The input vector.
- `out::AbstractVector`: The output vector of equal length as `v`.

# Output
- `out` is modified in-place to contain the softmax of `v`.

# Examples
```julia
julia> v = [1.0, 2.0, 3.0]
3-element Vector{Float64}:
 1.0
 2.0
 3.0

julia> out = similar(v)
3-element Vector{Float64}:
 0.0
 0.0
 0.0

julia> soft_max!(v, out)
3-element Vector{Float64}:
 0.09003057317038046
 0.24472847105479767
 0.6652409557748219
```
"""
function soft_max!(v::AbstractVector, out::AbstractVector)
	out .= exp.(v)
	s = sum(out)
	out .= out ./ s
end

# ╔═╡ 49a1d508-b491-4d3a-8415-f5def06884e9
"""
    soft_max(v::AbstractVector) -> AbstractVector

Calculate the softmax of a vector `v` and return the result in a new vector.

# Arguments
- `v::AbstractVector`: The input vector.

# Output
- A new vector of the same length as `v`, containing the softmax of `v`.

# Examples
```julia
julia> v = [1.0, 2.0, 3.0]
3-element Vector{Float64}:
 1.0
 2.0
 3.0

julia> out = soft_max(v)
3-element Vector{Float64}:
 0.09003057317038046
 0.24472847105479767
 0.6652409557748219
```
"""
soft_max(v::AbstractVector) = soft_max!(v, similar(v))

# ╔═╡ c6b61679-8a06-47ae-abab-6997ad5cbfea
md"""
Using one hot encoding feature vectors each parameter θ simply represents each state preference, so the policy just takes a softmax of the feature vector to get the action distribution.

$\pi(s, \theta) = \sigma(\mathbf{\theta}) = \frac{e^{\theta_i}}{\sum_{j} e^{\theta_j}} \forall i$

The components of the gradient of the softmax function σ is given by:

$\frac{\partial \sigma(\theta)_i}{\partial \theta_j} = \sigma(\theta)_i(\delta_{ij} - \sigma(\theta)_j)$ 

We can use this expression to get the gradient of the policy output with respect to the parameters:

$\nabla\pi(a| s, \theta) = \sigma(\theta)_a(\delta_{a
j} - \sigma(\theta)_j)$ where we overload the notation for a to also be the index of the selected action

Combining these the *eligibility vector* for one hot encoding and action a is:
$\frac{\nabla \pi(a|s, \mathbf{\theta})}{\pi(a|s, \mathbf{\theta})} = (\delta_{aj} - \sigma(\theta)_j) \forall j$

To calculate this practically, we can use the columns of a one hot matrix who's dimension is dxd and subtract from that the policy vector for state s.
"""

# ╔═╡ c2d8a622-b8f9-454b-9fd1-dc940280624c
function run_corridor_reinforce(;α = 0.0002, θ_0 = [0.0, 0.0], kwargs...)
	features = [1.0 0.0; 0.0 1.0] #feature vectors of length 2 for each action
	avec = zeros(2) #vector to store action output distribution
	e_vec = zeros(2) #storage for eligibility vector

	corridor = make_corridor()

	#we have one parameter for each action
	d = length(corridor.actions)

	#starting state is always 1
	s0 = 1
	
	#policy does not distinguish between states and updates the distribution vector
	π!(s, θ) = soft_max!(θ, avec)
	function ∇lnπ!(a, s, θ)
		π!(s, θ) #fill avec with the appropriate softmax
		#softmax derivative
		for i in eachindex(e_vec)
			e_vec[i] = features[a, i] - avec[i]
		end
		return e_vec
	end

	reinforce_monte_carlo_control(π!, ∇lnπ!, d, s0, α, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), kwargs...)
end

# ╔═╡ 5f91ce14-c9d4-4818-8955-8e7381b4943b
function average_runs(f, n; kwargs...) 
	runs = Vector{Any}(undef, n)
	# for i in 1:n
	@threads for i in 1:n
		runs[i] = f(;kwargs...)[1]
	end

	[begin
		v = [r[i] for r in runs]
		cleanv = filter(x -> !isnan(x) && !isinf(x), v)
		if isempty(v)
			-Inf
		else
			mean(cleanv)
		end
	end
	for i in eachindex(first(runs))]
	
	# reduce(+, runs) ./ n
end

# ╔═╡ a45c1930-ad70-44f4-a6bc-10ccb03f65ab
function figure_13_1(αlist; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	traces = [begin
		v = average_runs(run_corridor_reinforce, nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("α = 2^{$(log2(α))}"))
	end
	for α in αlist]

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([traces; baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld",  height = 500))
end

# ╔═╡ 71c8d422-8177-4324-b048-98dd39198fee
#in the source code used to generate this for the book found here: http://incompleteideas.net/book/code/figure_13_1.py-remove the episodes start with poor performace because the parameter vector is initialized to prefer left with 95% probability
figure_13_1(2.0 .^ [-12, -13, -14]; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ a206c759-3f6e-4003-8cba-5f6ce6742646
md"""
## Figure 13.1
"""

# ╔═╡ 2e6d0374-1c93-48c8-b8ba-dd1a0c682d01
md"""
### Exercise 13.3
> *Exercise 13.3* In Section 13.1 we considered policy parameterizations using the soft-max in action preferences (13.2) with linear action preferences (13.3).  For this parameterization, prove that the eligibility vector is
 
$\begin{flalign}
	\nabla \ln \pi(a|s, \mathbf{\theta}) = \mathbf{x}(s, a) - \sum_b \pi(b|s, \mathbf{\theta}) \mathbf{x}(s, b) \tag{13.9}
\end{flalign}$

> using the definitions and elementary calculus.

$\begin{flalign}
\pi(a|s, \mathbf{\theta}) \hspace{5 px} &\dot = \hspace{5 px} \frac{e^{h(s, a, \mathbf{\theta})}}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{13.2} \\
h(s, a, \mathbf{\theta}) &= \mathbf{\theta}^\top \mathbf{x}(s, a) \tag{13.3}
\end{flalign}$

Working from these definitions we can derive the following:

$\begin{flalign}
\nabla h(s, a, \mathbf{\theta}) &= \mathbf{x}(s, a) \tag{by linearity of h}\\
\ln \pi(a|s, \mathbf{\theta}) &= h(s, a, \mathbf{\theta}) - \ln{\sum_b e^{h(s, b, \mathbf{\theta})}} \\
\nabla \ln \pi(a|s, \mathbf{\theta}) &= \nabla h(s, a, \mathbf{\theta}) - \nabla \ln{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{distributing gradient} \\
&= \mathbf{x}(s, a) - \frac{\sum_b \nabla  e^{h(s, b, \mathbf{\theta})}}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{using chain rule} \\
&= \mathbf{x}(s, a) - \frac{\sum_b e^{h(s, b, \mathbf{\theta})} \mathbf{x}(s, b)}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{using chain rule} \\
&= \mathbf{x}(s, a) - \sum_i \frac{e^{h(s, i, \mathbf{\theta})} \mathbf{x}(s, i)}{\sum_t e^{h(s, t, \mathbf{\theta})}} \forall i \tag{separating fractions} \\
&= \mathbf{x}(s, a) - \sum_i \pi(i|s, \mathbf{\theta}) \mathbf{x}(s, i) \tag{definition of π} \\
\square\\
\end{flalign}$
"""

# ╔═╡ cc45091e-b889-4d5a-9eef-84d80f792046
md"""
# 13.4 REINFORCE with Baseline

The policy gradient theorem (13.5) can be generalized to include a comparison of teh action value to an arbitrary *baseline* b(s):

$\nabla J(\mathbf{\theta}) \propto \sum_s \mu(s)\sum_a\left( q_\pi(s,a)-b(s) \right ) \nabla\pi(a|s,\mathbf{\theta}) \tag{13.10}$

The baseline can be any function, even a random variable, as long as it does not vary with $a$; the euation remains valid because the subtracted quantity is zero:

$\sum_ab(s)\nabla\pi(a|s,\mathbf{\theta})=b(s)\nabla\sum_a\pi(a|s,\mathbf{\theta})=b(s)\nabla1=0$

The policy gradient theorem with baseline (13.10) can be used to derive an update rule using similar steps as in the previous section.  The update rule that we end up with is a new version of REINFORCE that includes a general baseline:

$\mathbf{\theta}_{t+1} \dot = \mathbf{\theta}_t+\alpha(G_t-b(S_t))\frac{\nabla\pi(A_t|S_t,\mathbf{\theta}_t)}{\pi(A_t|S_t,\mathbf{\theta}_t)} \tag{13.11}$

Since the baseline could be uniformly zero, this is a strict generalization of REINFORCE.  To have an effective baseline that depends on state we can use a state value estimate that is also updated with gradient steps: $\hat v(S_t, \mathbf{w})$.  Using such an estimate we can revise the previous REINFORCE algorithm.
"""

# ╔═╡ 5bebef34-e266-4c18-95c3-28e1f1cb4b64
"""
	reinforce_with_baseline_MC_control(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)

Implements the REINFORCE algorithm for Monte Carlo control, which is a policy gradient method for reinforcement learning. Given a function π that maps states to probability distributions over actions, and a function ∇lnπ that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ, this function learns the optimal policy for a given Markov decision process (MDP).  The baseline value is determined by a state value estimator function parametrized by w.

Required arguments:
- π: A function that maps a state to a probability distribution over actions. This function must take two arguments: the current state and the policy parameters θ.
- ∇lnπ: A function that computes the gradient of the log-probability of an action under π with respect to the policy parameters θ. This function must take three arguments: the action, the current state, and the policy parameters θ.
- d: An integer representing the number of policy parameters to be learned.
- s0: The initial state of the MDP.
- α: The learning rate for the policy gradient update.
- step: A function that takes a state and an action and returns the next state and the reward received. This function must take two arguments: the current state and the chosen action.
- sterm: A state representing the terminal state of the MDP.
- actions: A collection of all possible actions in the MDP.

Optional keyword arguments:
- γ: The discount factor for future rewards. Default value is 1.0.
- max_episodes: The maximum number of episodes to run the algorithm. Default value is 1000.
- maxsteps: The maximum number of steps to take in each episode. Default value is Inf.
- baseline: The baseline value for the policy gradient update. Default value is 0.0.
- θ: The initial policy parameters. Default value is a vector of zeros with length d.
"""
function reinforce_with_baseline_MC_control(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)
	rewards = zeros(max_episodes)	
	function run_episode(maxsteps)
		s = s0
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s′, r) = step(s0, actions[a])
		reward_history = [r]
		while s′ != sterm && length(state_history) < maxsteps
			s = s′
			a = select_action(π(s, θ))
			(s′, r) = step(s, actions[a])
			push!(reward_history, r)
			push!(state_history, s)
			push!(action_history, a)
		end
		return state_history, action_history, reward_history
	end	
	for i in eachindex(rewards)
		state_history, action_history, reward_history = run_episode(maxsteps)
		#iterate through episode beginning at the end
		G = 0.0
		for i in reverse(eachindex(reward_history))
			G = (γ * G) + reward_history[i]
			s = state_history[i]
			δ = G - v̂(s, w)
			w .+= αw * δ .* ∇v̂(s, w)
			θ .+= αθ * γ^(i-1) * δ .* ∇lnπ(action_history[i], s, θ)		
		end
		rewards[i] = sum(reward_history)
	end
	return rewards, θ, w
end

# ╔═╡ d26cd4cb-9a62-4a03-8b71-b415c9be79f6
function run_corridor_critic(;αθ = 0.0002, αw = 0.0002, θ_0 = [0.0, 0.0], w_0 = [0.0, 0.0, 0.0, 0.0], f = reinforce_with_baseline_MC_control, kwargs...)
	features = [1.0 0.0; 0.0 1.0] #feature vectors of length 2 for each action
	avec = zeros(2) #vector to store action output distribution
	e_vec = zeros(2) #storage for eligibility vector

	#one hot vectors for each state including 0 for terminal state
	state_features = [[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 0.]]
	
	corridor = make_corridor()

	#we have one parameter for each action
	d = length(corridor.actions)
	d′ = 4

	#starting state is always 1
	s0 = 1
	
	#policy does not distinguish between states and updates the distribution vector
	π!(s, θ) = soft_max!(θ, avec)
	function ∇lnπ!(a, s, θ)
		π!(s, θ) #fill avec with the appropriate softmax
		#softmax derivative
		for i in eachindex(e_vec)
			e_vec[i] = features[a, i] - avec[i]
		end
		return e_vec
	end

	#linear model for state values simply returns the corresponding weight
	v̂(s::Int64, w::AbstractVector) = w[s]
	
	#gradient with respect to weights is just the state vector
	∇v̂(s::Int64, w::AbstractVector) = state_features[s]
	

	# reinforce_monte_carlo_control(π!, ∇lnπ!, d, s0, α, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), kwargs...)

	f(π!, ∇lnπ!, v̂, ∇v̂, d, d′, s0, αθ, αw, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), w = copy(w_0), kwargs...)
end

# ╔═╡ 5f042c7e-45ad-4f8f-94d8-9133e67dd0f6
function figure_13_2(α, αθ, αw; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	trace1 = begin
		v = average_runs(run_corridor_critic, nruns; αθ = αθ, αw = αw, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{with baseline } α^θ = 2^{$(log2(αθ))}, α^w = 2^{$(log2(αw))}"))
	end
	Random.seed!(seed)
	trace2 = begin
		v = average_runs(run_corridor_reinforce, nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{no baseline } α = 2^{$(log2(α))}"))
	end

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([trace1, trace2, baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld",  height = 500))
end

# ╔═╡ b72e030f-7d52-481f-b4f7-2b16b227e547
md"""
### Figure 13.2
"""

# ╔═╡ 87876de4-ca40-4736-81a8-bb26bc273d89
figure_13_2(2.0 ^-13, 2.0 .^ -9, 2.0 ^-3; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ ce33f710-fd9d-4dfa-acda-40204e54d518
md"""
# 13.5 Actor-Critic Methods

Here we also use the value function estimator to calculate the the return estimate using the one step bootstrap return.  When the state value function is used in this way we call it the *critic*.  In general we can use this function with n-step returns and eligibility traces.

The one-step actor-critic method is the analog of the one step methods such as TD(0), Sarsa(0), and Q learning.  These methods replace the full return of REINFORCE with the one step return as follows:

$\begin{flalign}
\mathbf{\theta}_{t+1} &\hspace{5px}   \dot = \hspace{5px} \mathbf{\theta}_t + \alpha(G_{t:t+1} - \hat v(S_t, \mathbf{w}))\ln\nabla\pi(A_t|S_t, \mathbf{\theta_t}) \tag{13.12} \\
& = \mathbf{\theta}_t + \alpha(R_{t+1} + \gamma \hat v(S_{t+1}, \mathbf{w}) - \hat v(S_t, \mathbf{w}))\ln\nabla\pi(A_t|S_t, \mathbf{\theta_t}) \tag{13.13} \\
& = \mathbf{\theta}_t + \delta_t\ln\nabla\pi(A_t|S_t, \mathbf{\theta_t}) \tag{13.14} \\
\end{flalign}$

This can be implemented as a fully online algorithm because we do not have to wait until the end of an episode to calculate return estimates.
"""

# ╔═╡ f4b6f10b-4cd0-4be6-98ec-4d4ffb696392
md"""
## One-step Actor-Critic
"""

# ╔═╡ bf656d55-19cb-4052-baaa-0896ab6d23a0
"""
    one_step_actor_critic(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; 
        γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)

Run the one-step actor-critic algorithm to learn a policy and state-value function for a given environment.

# Arguments
- `π::Function`: a function that maps a state `s` and a parameter vector `θ` to a probability distribution over actions.
- `∇lnπ::Function`: a function that maps an action `a`, a state `s`, and a parameter vector `θ` to the gradient of the log-probability of the action under the policy.
- `v̂::Function`: a function that maps a state `s` and a parameter vector `w` to an estimate of the state-value function.
- `∇v̂::Function`: a function that maps a state `s` and a parameter vector `w` to the gradient of the state-value function estimate.
- `d::Int64`: the dimension of the parameter vector `θ`.
- `d′::Int64`: the dimension of the parameter vector `w`.
- `s0`: the initial state of the environment.
- `αθ`: the step size parameter for updating the policy parameters `θ`.
- `αw`: the step size parameter for updating the state-value function parameters `w`.
- `step`: a function that maps a state `s` and an action `a` to a tuple `(s′, r)` representing the next state and reward.
- `sterm`: the terminal state of the environment.
- `actions`: an array of possible actions in the environment.
- `γ`: the discount factor for future rewards (default: `1.0`).
- `max_episodes`: the maximum number of episodes to run (default: `1000`).
- `θ`: the initial policy parameters (default: `zeros(d)`).
- `w`: the initial state-value function parameters (default: `zeros(d′)`).
- `maxsteps`: the maximum number of steps per episode (default: `Inf`).

# Returns
- `rewards`: an array of length `max_episodes` containing the total rewards obtained in each episode.
- `θ`: the learned policy parameters.
- `w`: the learned state-value function parameters.

# Notes
This function implements the one-step actor-critic algorithm, which updates the policy and state-value function estimates in an online fashion using the gradients of the log-probability and state-value function estimates, respectively, with respect to their parameters. The algorithm uses the eligibility trace method to update the state-value function estimates.
"""
function one_step_actor_critic(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf)
	rewards = zeros(max_episodes)	
	function run_episode!(maxsteps)
		I = 1.0
		s = s0w
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s′, r) = step(s0, actions[a])
		reward_history = [r]
		δ = r + γ*v̂(s′, w) - v̂(s, w)
		w .+= αw * δ .* ∇v̂(s, w)
		θ .+= αθ * I * δ .* ∇lnπ(a, s, θ)		
		
		while s′ != sterm && length(state_history) < maxsteps
			I = γ*I
			s = s′
			a = select_action(π(s, θ))
			(s′, r) = step(s, actions[a])
			push!(state_history, s)
			push!(action_history, a)
			push!(reward_history, r)
			δ = r + γ*v̂(s′, w) - v̂(s, w)
			w .+= αw * δ .* ∇v̂(s, w)
			θ .+= αθ * I * δ .* ∇lnπ(a, s, θ)		
		end
		return state_history, action_history, reward_history
	end	

	for i in eachindex(rewards)		
		state_history, action_history, reward_history = run_episode!(maxsteps)
		rewards[i] = sum(reward_history)
	end
	return rewards, θ, w
end

# ╔═╡ 4cbdb082-22ba-49e9-a6ed-4380917625ac
md"""
## Actor-Critic with Eligibility Traces
"""

# ╔═╡ 58ad84b0-f9c9-424e-8c05-0b15fbe7b349
function actor_critic_eligibility(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0, αθ, αw, step, sterm, actions; λθ = 0.0, λw = 0.0, γ = 1.0, max_episodes = 1000, θ = zeros(d), w = zeros(d′), maxsteps = Inf, termination_threshold = (episode = Inf, reward = -Inf), zθ = zeros(size(θ)...), zw = zeros(size(w)...))
	rewards = zeros(max_episodes)
	#initialize trace vectors
	zθ = zeros(size(θ)...)
	zw = zeros(size(w)...)
	
	function run_episode!(maxsteps)
		I = 1.0
		zθ .= 0.0
		zw .= 0.0
		s = s0
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s′, r) = step(s0, actions[a])
		reward_history = [r]
		δ = r + γ*v̂(s′, w) - v̂(s, w)
		zw .= γ*λw .* zw .+ ∇v̂(s, w)
		zθ .= γ*λθ .* zθ .+ I .* ∇lnπ(a, s, θ)
		w .+= αw * δ .* zw
		θ .+= αθ * δ .* zθ		
		
		while s′ != sterm && length(state_history) < maxsteps
			I = γ*I
			s = s′
			a = select_action(π(s, θ))
			(s′, r) = step(s, actions[a])
			push!(state_history, s)
			push!(action_history, a)
			push!(reward_history, r)
			δ = r + γ*v̂(s′, w) - v̂(s, w)
			zw .= γ*λw .* zw .+ ∇v̂(s, w)
			zθ .= γ*λθ .* zθ .+ I .* ∇lnπ(a, s, θ)
			w .+= αw * δ .* zw
			θ .+= αθ * δ .* zθ			
		end
		return state_history, action_history, reward_history
	end	

	for i in eachindex(rewards)		
		state_history, action_history, reward_history = run_episode!(maxsteps)
		rewards[i] = sum(reward_history)
		#end execution early if results don't meet criteria
		if i > termination_threshold.episode && rewards[i] < termination_threshold.reward
			rewards[i+1:end] .= -Inf
			break
		end
	end
	return rewards, θ, w
end

# ╔═╡ 8f11b8dc-2c3e-41a5-8dbb-9af06235fe85
function corridor_actor_critic(α, αθ, αw; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	trace1 = begin
		v = average_runs(run_corridor_critic, nruns; αθ = αθ, αw = αw, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{with baseline } α^θ = 2^{$(log2(αθ))}, α^w = 2^{$(log2(αw))}"))
	end
	Random.seed!(seed)
	trace2 = begin
		# v = average_runs((args...; kwargs...) -> run_corridor_critic(args...; f = one_step_actor_critic, kwargs...), nruns; αθ = αθ, αw = αw, θ_0 = θ_0, kwargs...)
		v = average_runs(run_corridor_critic, nruns; αθ = αθ, αw = αw, θ_0 = θ_0, f = one_step_actor_critic, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{actor-critic } α^θ = 2^{$(log2(αθ))}, α^w = 2^{$(log2(αw))}"))
	end
	Random.seed!(seed)
	trace3 = begin
		v = average_runs(run_corridor_reinforce, nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("\\text{no baseline } α = 2^{$(log2(α))}"))
	end

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([trace1, trace2, trace3, baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld",  height = 500))
end

# ╔═╡ 70d4e199-2941-46dd-99c0-0f0520bf976b
corridor_actor_critic(2.0 ^ -13, 2.0 ^ -9, 2.0 ^-3; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ 72900e88-98f4-4879-b005-d79ef6c7ee7f
function corridor_actor_critic_λ(αθ, αw, λlist; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	traces = [begin
		name = latexstring("\\lambda^θ = $λθ, \\lambda^w = $λw")
		v = average_runs(run_corridor_critic, nruns; f = actor_critic_eligibility, θ_0 = θ_0, λθ = λθ, λw = λw, αθ = αθ, αw = αw,  kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = name)
	end
	for λθ in λlist for λw in λlist]

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([traces; baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "Actor-Critic λ Comparison short-Corridor gridworld",  height = 500))
end

# ╔═╡ 225ab967-7c7a-44ea-925b-5fa786382d62
corridor_actor_critic_λ(2.0 ^ -6, 2.0 ^-3, [0.0, 0.1, 0.93]; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ 511a847f-234c-465e-8f4a-688e79d9b975
md"""
# 13.6 Policy Gradient for Continuing Problems

In the continuing case we need to define the average reward per time step as discussed in Section 10.3.  In the update procedure the δ is calculated differently in terms of the reward compared to this long running average.  The value functions in this case will also learn the reward difference from the average which is assumed to have a well defined expected value under the stationary state distribution for the policy.  This shift in the value function will not affect performance since shifting the value function up and down by a constant does not affect the learned policy.  To implement this we need a new learning rate αr which controls how quickly the reward average updates.  This replaces γ in a sense since we no longer discount rewards of future time steps.
"""

# ╔═╡ 533cbf4b-ac14-47eb-98cf-e569f32cc215
function actor_critic_eligibility_continuing(π::Function, ∇lnπ::Function, v̂::Function, ∇v̂::Function, d::Int64, d′::Int64, s0::S, αθ, αw, αR, step, actions::AbstractVector{A}; λθ = 0.0, λw = 0.0, maxsteps = 1000, θ = zeros(d), w = zeros(d′)) where {S, A}
	state_history = Vector{S}(undef, maxsteps+1)
	action_history = Vector{A}(undef, maxsteps+1)
	reward_history = zeros(maxsteps+1)
	
	#initialize trace vectors
	zθ = zeros(d)
	zw = zeros(d′)

	#fill in first step history
	state_history[1] = s0

	#initialize reward average
	r̄ = 0.0
	
	s = s0
	state_history = [s0]
	a = select_action(π(s0, θ))
	action_history = [a]
	(s′, r) = step(s0, actions[a])
	reward_history = [r]
	δ = r + γ*v̂(s′, w) - v̂(s, w)
	zw .= γ*λw .* zw .+ ∇v̂(s, w)
	zθ .= γ*λθ .* zθ .+ I .* ∇lnπ(a, s, θ)
	w .+= αw * δ .* zw
	θ .+= αθ * δ .* zθ		
	
	for i in 2:maxsteps+1
		s = state_history[i-1]
		a = select_action(π(s, θ))
		action_history[i-1] = a
		(s′, r) = step(s, actions[a])
		reward_history[i-1] = r
		state_history[i] = s′
		
		δ = r - r̄ + v̂(s′, w) - v̂(s, w)
		r̄ += αR * δ
		zw .= γ*λw .* zw .+ ∇v̂(s, w)
		zθ .= γ*λθ .* zθ .+ ∇lnπ(a, s, θ)
		w .+= αw * δ .* zw
		θ .+= αθ * δ .* zθ			
	end
	
	return reward_history, state_history, action_history, θ, w
end

# ╔═╡ 735b548a-88f5-4a30-ab8f-dfb3d6401b2b
md"""
# 13.7 Policy Parameterization for Continuous Actions

With a parameterized policy we are to learn statistics of teh distribution that selects actions.  As a foundation consider the normal distribution:

$p(x) \hspace{5px} \dot = \hspace{5px} \frac{1}{\sigma \sqrt{2\pi}} \exp \left ( - \frac{(x-\mu)^2}{2\sigma^2} \right ) \tag{13.18}$
"""

# ╔═╡ 79c85707-ea09-4f6b-ad51-a2683c3923c0
let x = LinRange(-5, 5, 10_000)
	traces = [scatter(x = x, y = pdf.(Normal(0.0, σ), x), name = latexstring("\\sigma^2 = $(round(σ^2, sigdigits = 2))")) for σ in sqrt.([0.2, 1.0, 0.5, 5.0])]
	plot(traces, Layout(xaxis_title = "x", title = "Normal Distribution N(μ, σ)"))
end

# ╔═╡ 7ccadf01-fbba-4dfd-a5ad-770dab9946f9
md"""
We can define our policy as a normal distribution function over actions for a given state and parameter vector.

$\pi(a|s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \frac{1}{\sigma(s, \mathbf{\theta}) \sqrt{2\pi}} \exp \left ( - \frac{(a-\mu(s, \mathbf{\theta}))^2}{2\sigma(s, \mathbf{\theta})^2} \right ) \tag{13.19}$

This policy requires μ and σ to be parameterized by the parameter vector.  To make a linear model for both parameters we can use the following formulas:

$\mu(s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \mathbf{\theta}_\mu ^\top \mathbf{x}_\mu(s) \text{ and } \sigma(s, \mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} \exp{( \mathbf{\theta}_\sigma ^ \top \mathbf{x}_\sigma (s))} \tag{13.20}$

where $\mathbf{x}_\mu(s)$ and $\mathbf{x}_\sigma(s)$ are state feature vectors.  With these formulas we can apply the previous algorithms to solve environments with real-valued actions. 
"""

# ╔═╡ beb01fb8-c77d-4b5c-a66d-3812415e04a3
md"""
### Exercise 13.4
> *Exercise 13.4* For the Gaussian policy parameterization, derive the formula for the eligibility vector $\nabla \ln{\pi(a|s, \mathbf{\theta})}$

Starting with our expression for the parameter function, we can calculate the gradient: 

$\nabla \pi(a|s, \mathbf{\theta}) = \nabla \left ( \frac{1}{\sigma(s, \mathbf{\theta}) \sqrt{2\pi}} \exp \left ( - \frac{(a-\mu(s, \mathbf{\theta}))^2}{2\sigma(s, \mathbf{\theta})^2} \right ) \right )$

We will eventually need $\nabla \mu$ and $\nabla \sigma$ so let's calculate them now.

$\nabla (\sigma(s, \mathbf{\theta})) = \nabla \exp{( \mathbf{\theta}_\sigma ^ \top \mathbf{x}_\sigma (s))} = \sigma(s, \mathbf{\theta})\mathbf{x}_\sigma (s)$

$\nabla(\mu(s, \mathbf{\theta})) = \nabla ( \mathbf{\theta}_\mu ^\top \mathbf{x}_\mu(s)) = \mathbf{x}_\mu (s)$


The first application of the quotient rule is trivial, I will omit the input arguments to μ and σ keeping in mind that these are functions of the parameters.  Also let $\left ( - \frac{(a-\mu)^2}{2\sigma^2} \right ) = f(\mu, \sigma)$ which results in $\pi(a|s, \mathbf{\theta}) =  \frac{1}{\sigma \sqrt{2\pi}} \exp{(f(\mu, \sigma))}$.  Therefore:

$\begin{flalign}
\nabla \pi(a|s, \mathbf{\theta}) \sqrt{2\pi} &= \frac{1}{\sigma ^2} \left (- \exp{(f(\mu, \sigma))} \nabla \sigma + \sigma \exp{(f(\mu, \sigma))}\nabla f(\mu, \sigma) \right ) \\
&= \frac{1}{\sigma ^2} \left ( -\exp{(f(\mu, \sigma))} \sigma\mathbf{x}_\sigma + \sigma \exp{(f(\mu, \sigma))}\nabla f(\mu, \sigma) \right ) \\
&=\frac{\exp{(f(\mu, \sigma))}}{\sigma} \left (-\mathbf{x}_\sigma + \nabla f(\mu, \sigma) \right ) \\
\end{flalign}$

Now we need only calculate the gradient of $f$:

$\begin{flalign}
\nabla f(\mu, \sigma) &= \frac{-1}{2} \nabla \left [ \frac{(a-\mu)^2}{\sigma^2} \right ] \\
& = \frac{-1}{2\sigma^4} \left [-2 \sigma^2 (a - \mu) \nabla \mu - (a - \mu)^2 2\sigma \nabla \sigma \right ] \\
& = \frac{-1}{\sigma^3} \left [ -\sigma (a - \mu) \nabla \mu - (a - \mu)^2 \nabla \sigma \right ] \\
& = \frac{-1}{\sigma^3} \left [ -\sigma (a - \mu) \mathbf{x}_\mu (s) - (a - \mu)^2 \sigma \mathbf{x}_\sigma \right ] \tag{substituting gradients}\\
& = \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu) \tag{simplifying}\\
\end{flalign}$

Now substitute this back into the policy gradient:

$\nabla \pi(a|s, \mathbf{\theta}) \sqrt{2\pi} = \frac{\exp{(f(\mu, \sigma))}}{\sigma} \left (-\mathbf{x}_\sigma + \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu) \right )$

Furthermore, observe that $\pi(a|s, \mathbf{\theta}) = \frac{1}{\sigma\sqrt{2\pi}} \exp(f(\mu, \sigma))$

So our expression for the policy gradient is:

$\nabla \pi(a|s, \mathbf{\theta}) = \pi(a|s, \mathbf{\theta}) \left (-\mathbf{x}_\sigma + \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu) \right )$

To get the eligibility vector we must divide this by the policy which is conveniently already in the expression:

$\begin{flalign}
\frac{\nabla \pi(a|s, \mathbf{\theta})}{\pi(a|s, \mathbf{\theta})} &= -\mathbf{x}_\sigma + \frac{(a - \mu)}{\sigma^2} ((a - \mu) \mathbf{x}_\sigma + \mathbf{x}_\mu)\\
&= \mathbf{x}_\mu \left [ \frac{(a - \mu)}{\sigma^2} \right ] + \mathbf{x}_\sigma \left [\frac{(a-\mu)^2}{\sigma^2} -1 \right ] \\
\end{flalign}$

There are two components to the sum, one for $\mu$ and one for $\sigma$.  If we think of the paramters and feature vectors as concatenated, then this sum would be an element by element sum where $\mathbf{x}_\mu$ has a zero value for all the feature indices corresponding to $\sigma$ and vice-versa.  This way doing the sum will form one complete vector that has gradient components for all the parameters $\mathbf{\theta}_\mu$ and $\mathbf{\theta}_\sigma$.  Alternatively, the sum can be separated and each gradient can be treated separately with only those components keeping them separated throughout the calculation.
"""

# ╔═╡ 68e6f17e-8c87-40f0-a673-1115ecd1b71d
md"""
### Exercise 13.5
> *Exercise 13.5* A *Bernoulli-logistic unit* is a stochastic neuron-like unit used in some ANNs.  Its input at time *t* is a feature vector $\mathbf{x}(S_t)$; its output, $A_t$, is a random variable having two values, 0 and 1, with $/Pr{A_t=1}=P_t$ and $/Pr{A_t=0}=1-P_t$ (the Bernoulli distribution).  Let $h(s, 0, \mathbf{\theta})$ and $h(s, 1, \mathbf{\theta})$ be the preferences in state $s$ for the unit's two actions given by policy parameter $\mathbf{\theta}$.  Assume that the difference between the action preferences is given by a weights sum of teh unit's input vector, that is, assume that $h(s, 1, \mathbf{\theta})-h(s,0, \mathbf{\theta}) = \mathbf{\theta}^\top \mathbf{x}(s)$, where $\mathbf{\theta}$ is the unit's weight vector.
> 1. Show that if the exponential soft-max distribution (13.2) is used to convert action preferences to policies, then ${P_t = \pi(1|S_t, \theta_t)=1/(1+\exp(-\theta_t^\top\mathbf{x}(S_t)))}$ (the logistic function). 
> 2. What is the Monte-Carlo REINFORCE update of $\theta_t$ to $\theta_{t+1}$ upon receipt of return $G_t$?
> 3. Express the eligility $\nabla \ln \pi(a|s, \theta)$ for a Bernoulli-logistic unit, in terms of $a$, $\mathbf{x}(s)$, and $\pi(a|s, \theta)$ by calculating the gradient.
> Hint for part (c): Define $P=\pi(1|s,\theta)$ and compute the derivative of the logarithm, for each action, using the chain rule on $P$.  Combine the two results into one expression that depends on $a$ and $P$, and then use the chain rule again, this time on $\theta^\top\mathbf{x}(s)$, noting that the derivative of the logistic function $f(x)=1/(1+e^{-x})$ is $f(x)(1-f(x))$.
"""

# ╔═╡ 692c1043-4eaf-491e-b8fe-368618867f99
md"""
1. The soft-max distribution is: 
$\sigma(a|s, \theta) = \frac{e^{h(s, a, \theta)}}{\sum_b e^{h(s, b, \theta)}}$ 

We only have two possible actions in each state so the policy for action 1 would be given by: 

$\pi(1|S_t, \theta_t) = \frac{e^{h(s, 1, \theta_t)}}{e^{h(S_t, 0, \theta_t)} + e^{h(S_t, 1, \theta)}}$ 

Simplify this expression by dividing by $e^{h(s, 1, \theta_t)}$ which results in: 

$\pi(1|S_t, \theta_t) = \frac{1}{e^{h(S_t, 0, \theta_t) - h(S_t, 1, \theta_t)} + 1}$ 

Given the assumption that $h(s, 1, \theta)-h(s, 0, \theta) = \theta^\top\mathbf{x}(s)$, we replace the expression in the exponent resulting in the final expression of: 

$\pi(1|S_t, \theta_t) = \frac{1}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1}$

Using the notation $f(x) = 1/(1+e^{-x})$ we can write $\pi(1|S_t, \theta_t) = f(\theta_t^\top \mathbf{x}(S_t))$ where $f$ is the logistic function.  Consider this notation for the rest of the exercises.

2. The REINFORCE update is given by: $\theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla\pi(A_t|S_t, \theta_t)}{\pi(A_t|S_t, \theta_t)}$, so we need to compute the gradient of the policy in terms of the parameters for this action selection: $\nabla \pi(1|S_t, \theta_t)$.  Luckily, the derivative of the logistic function is simply given by: $f(x)(1-f(x))$ where $f(x)$ is the logistic function itself.  In our case $x = \theta_t^\top \mathbf{x}_t$  so after applying the chain rule we have: 

$\nabla\pi(1|S_t, \theta_t) = f(x)(1-f(x))\nabla x = f(x)(1-f(x)) \mathbf{x_t}$ since $x$ is just a linear function of the parameters.  So for the parameter update step we have: 

$\frac{\nabla\pi(1|S_t, \theta_t)}{\pi(1|S_t, \theta_t)} = \frac{f(x)(1-f(x))\mathbf{x}_t}{f(x)} = (1 - f(x))\mathbf{x}_t$

Also note that:

$1 - f(x) = 1 - \frac{1}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1} = \frac{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1 - 1}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1} = \frac{e^{-\theta_t^\top\mathbf{x}(S_t)}}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1}$

The REINFORCE update will then be: 

$\theta_{t+1} = \theta_t + \alpha G_t \left ( \frac{e^{-\theta_t^\top\mathbf{x}(S_t)}}{e^{-\theta_t^\top\mathbf{x}(S_t)} + 1} \right ) \mathbf{x}_t$

3. For the general case, we want to calculate $\frac{\nabla\pi(a|s, \theta)}{\pi(a|s, \theta)}$.  We already know this expression for $a = 1$.   

$\nabla {\pi(1|s, \mathbf{\theta})} = f(x)(1 - f(x))\mathbf{x}(s) = \pi(1|s, \mathbf{\theta})(1 - \pi(1|s, \mathbf{\theta})\mathbf{x}(s)$

Since $\pi(a|s, \theta)$ is a probability distribution across actions, we also know that 

$\pi(0|s, \theta) = 1 - \pi(1|s, \theta)$ 

which implies that 

$\nabla \pi(0|s, \theta) = -\nabla \pi(1|s, \theta) = -\pi(1|s, \mathbf{\theta})(1 - \pi(1|s, \mathbf{\theta}))\mathbf{x}(s)$ 

We can express this in terms of $\pi(0|s, \theta)$ completely:

$\nabla \pi(0|s, \theta) = (\pi(0|s, \mathbf{\theta}) - 1)\pi(0|s, \theta)\mathbf{x}(s) = -\pi(0|s, \theta)(1 - \pi(0|s, \mathbf{\theta}))\mathbf{x}(s)$ 

Let's now compare the two expressions for the policy gradient at each action:

$\begin{align}
\nabla {\pi(1|s, \mathbf{\theta})} &= \pi(1|s, \mathbf{\theta})(1 - \pi(1|s, \mathbf{\theta})\mathbf{x}(s) \\
\nabla \pi(0|s, \theta) &= -\pi(0|s, \theta)(1 - \pi(0|s, \mathbf{\theta}))\mathbf{x}(s) \\
\therefore \\
\nabla \pi(a|s, \theta) &= \chi (a) \pi(a|s, \theta)(1 - \pi(a|s, \mathbf{\theta}))\mathbf{x}(s) \\
\end{align}$

Where $\chi (a)$ is a function that returns 1 for $a=1$ and -1 for $a=0$.  There are many ways to achieve this but the following expression is simple and works: $\chi(a) = 2a - 1$.  Dividing by the gradient yields a unified expression for the eligibility vector:

$\nabla \ln{\pi(a|s,\theta)} = (2a - 1) (1 - \pi(a|s, \mathbf{\theta}))\mathbf{x}(s)$ 
"""

# ╔═╡ 4c34640f-efa2-4e1d-8a70-0acd2ce45428
md"""
# Bonus Problems: Comparing Techniques
Consider the case of applying the techniques in this chapter to problems where we choose feature vectors and parameters to effectively compute the tabular case.  That is we enumerate every state and state/action pair.  Our parameters for each function will store a single value for each case.  Let's consider the gradients for both the state-value estimate and the policy.  We will use two sets of parameters: $\mathbf{w}$ and $\mathbf{\theta}$.  $\mathbf{w}_s$ is the parameter for state s and $\mathbf{\theta}_{s, a}$ is the parameter for state/action pair $(s, a)$.  Using this notation $\mathbf{w}$ is a vector and $\theta$ is a matrix.

Starting with the state-value function:

$\begin{align}
\hat v(s, \mathbf{w}) &= \mathbf{w}_s \\
\nabla v(s, \mathbf{w}) &= \nabla \mathbf{w}_s \\
&= \mathbf{e}_s
\end{align}$

where $\mathbf{e}_s$ is the one-hot vector for index s and length equal to the number of states.

Now moving on to the policy, we will use a soft-max function to convert action preferences into probabilities.

$\begin{align}
\pi(a|s, \theta) &= \frac{\exp{\theta_{s, a}}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \\
\nabla \pi(a|s, \theta) &=  \nabla \frac{\exp{\theta_{s, a}}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \\
\end{align}$

This is a matrix of partial derivatives with respect to every component of $\theta$, so let's consider each component indexed by $(m, n)$. 

$\nabla \pi(a|s, \theta)_{m, n} = \frac{\partial}{\partial \theta_{m, n}} \left ( \frac{\exp{\theta_{s, a}}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \right )$

This can be solved applying the quotient rule with $u(\theta) = \exp{\theta_{s, a}}$ and $v(\theta) = \sum_{i=1}^{n_A} \exp{\theta_{s, i}}$

Also notice that $\frac{\partial{\theta_{i, j}}}{\partial{\theta_{m, n}}} = \delta_{i, m}\delta_{j, n}$ which eliminates all values from the sum except one.  So the resulting partial derivatives are: 

$\frac{\partial{u}}{\partial \theta_{m, n}} = (\exp{\theta_{s, a}}) \delta_{s, m}\delta_{a, n} \text{ and } \frac{\partial{v}}{\partial \theta_{m, n}} = (\exp{\theta_{s, n}})\delta_{s, m}$


$\begin{align}
\nabla \pi(a|s, \theta)_{m, n} &= \left ( \frac{1}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \right )^2 \left ( (\exp{\theta_{s, a}}) \delta_{s, m}\delta_{a, n} \sum_{i=1}^{n_A} \exp{\theta_{s, i}} - \exp{\theta_{s, a}} (\exp{\theta_{s, n}})\delta_{s, m} \right ) \\
&= \left ( \frac{\exp{\theta_{s, a}}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \right ) \left ( \frac{\delta_{s, m}}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}} \right ) \left (\delta_{a, n}\sum_{i=1}^{n_A} \exp{\theta_{s, i}} - (\exp{\theta_{s, n}}) \right ) \\
&=  \frac{ \delta_{s, m}\pi(a|s, \theta)}{\sum_{i = 1}^{n_A}{\exp{\theta_{s, i}}}}  \left (\delta_{a, n}\sum_{i=1}^{n_A} \exp{\theta_{s, i}} - (\exp{\theta_{s, n}}) \right ) \\
&=  \delta_{s, m}\pi(a|s, \theta) \left (\delta_{a, n} - \pi(n|s, \theta) \right ) \\
\therefore \\
\nabla \ln{\pi(a|s, \theta)_{m, n}} &= \delta_{s, m} \left (\delta_{a, n} - \pi(n|s, \theta) \right ) \\
\end{align}$

$\begin{equation}
\nabla \ln{\pi(a|s, \theta)_{m, n}} =
\begin{cases}
0 & s \neq m \\
-\pi(n|s, \theta) & s=m,a\neq n \\
1 - \pi(n|s, \theta) & s = m, a = n
\end{cases}
\end{equation}$

## Racetrack Environment
"""

# ╔═╡ ff60f48e-2055-4bb6-8cf4-fac1da45200b
const racetrack_velocities = [(vx, vy) for vx in 0:4 for vy in 0:4]

# ╔═╡ a79ed238-a6d3-40e6-9bf3-351b7494b446
const racetrack_actions = [(dx, dy) for dx in -1:1 for dy in -1:1]

# ╔═╡ 76af787d-7a3d-4c65-ab6a-898fba148705
#track is defined as a set of points for each of the start, body, and finish
const track1 = (  start = Set((x, 0) for x in 0:5), 
            finish = Set((13, y) for y in 26:31), 
            body = union(   Set((x, y) for x in 0:5 for y in 1:2),
                            Set((x, y) for x in -1:5 for y in 3:9),
                            Set((x, y) for x in -2:5 for y in 10:17),
                            Set((x, y) for x in -3:5 for y in 18:24),
                            Set((x, 25) for x in -3:6),
                            Set((x, y) for x in -3:12 for y in 26:27),
                            Set((x, 28) for x in -2:12),
                            Set((x, y) for x in -1:12 for y in 29:30),
                            Set((x, 31) for x in 0:12))
)

# ╔═╡ 0538aaf4-716b-4f3c-aa7e-dcb1dd456172
#given a position, velocity, and action takes a forward step in time and returns the new position, new velocity, and a set of points that represent the space covered in between
function project_path(p, v, a)
    (vx, vy) = v
    (dx, dy) = a

    vxnew = clamp(vx + dx, 0, 4)
    vynew = clamp(vy + dy, 0, 4)

    #ensure that the updated velocities are not 0
    if vxnew + vynew == 0
        if iseven(p[1] + p[2])
            vxnew += 1
        else
            vynew += 1
        end
    end

    #position the car ends up at
    pnew = (p[1] + vxnew, p[2] + vynew)

    #how to check if the path intersects the finish line or the boundary?  Form a square from vxnew and vynew and see if the off-track area or finish line is contained in that square
    pathsquares = Set((x, y) for x in p[1]:pnew[1] for y in p[2]:pnew[2])

    (pnew, (vxnew, vynew), pathsquares)
end

# ╔═╡ d37e21ac-b82a-423f-8719-b513bddf433d
function make_racetrack(track)
	positions = mapreduce(a -> collect(a), vcat, track)
	states = [(position = p, velocity = v) for p in positions for v in racetrack_velocities]

	sterm = (position = (-1, -1), velocity = (0, 0))

	#take a forward step from current state returning new state and reward of -1
	function step(s, a)
		s == sterm && return (sterm, 0.0)
		pnew, vnew, psquare = project_path(s.position, s.velocity, a)
		fsquares = intersect(psquare, track.finish)
		outsquares = setdiff(psquare, track.body, track.start)
		s′ = if !isempty(fsquares) #car finished race
			# println("Finished race")
			sterm
		elseif !isempty(outsquares) #car path went outside of track
			# println("car reset")
			(position = rand(track.start), velocity = (0, 0))
		else
			(position = pnew, velocity = vnew)
		end
		# println("starting state: $s, ending state: $s′")
		(s′, -1.0)
	end	

	s0 = (position = first(track.start), velocity = (0, 0))

	function runepisode(π; s0 = s0)
		traj = [s0]
		rewards = Vector{Float64}()
		s = s0
		while true
			(s, r) = step(s, π(s))
			push!(rewards, r)
			(s == sterm) && break
			push!(traj, s)
		end
		return traj, rewards
	end

	(states, sterm, racetrack_actions, step, s0, runepisode)
end

# ╔═╡ e5a0a3fc-2eb3-4f31-8ab6-4a3130c70932
function execute_racetrack_actor_critic(track, αθ, αw; kwargs...)
	(states, sterm, actions, step) = make_racetrack(track)
	s0 = (position = first(track.start), velocity = (0, 0))

	state_action_pairs = [(s, a) for s in states for a in actions]

	#convert states to index
	statelookup = Dict(zip(states, eachindex(states)))
	statelookup[sterm] = lastindex(states) + 1

	#create state feature vectors, leave the terminal state at all zeros
	xs = [zeros(lastindex(states)+1) for i in 1:(lastindex(states)+1)]
	for i in eachindex(states)
		xs[i][i] = 1.0
	end
	
	#parameters
	θ = zeros(lastindex(states)+1, lastindex(actions))
	w = zeros(lastindex(states)+1)

	#allocations for outputs
	πoutput = zeros(lastindex(actions))
	gradoutput = similar(θ)

	#value function and gradient
	v̂(s, w) = w[statelookup[s]]
	∇v̂(s, w) = xs[statelookup[s]]

	function clean_output!(v::AbstractVector{T}) where T <: AbstractFloat
		for (i, x) in enumerate(v)
			if isnan(x) || isinf(x)
				v[i] = zero(T)
			end
		end
		return v
	end

	#policy function and gradient
	function π!(s, θ) 
		soft_max!(θ[statelookup[s], :], πoutput)
		clean_output!(πoutput)
	end

	function ∇lnπ!(a, s, θ)
		#ensure πoutput contains the current softmax output for this state
		# π!(s, θ)
		i = statelookup[s]
		 for n in eachindex(actions)
			@inbounds @simd for m in eachindex(states)
				gradoutput[m, n] = (i == m) * ((n == a) - πoutput[n])
				# if i == m
				# 	println("At state $i Updated gradient of $(gradoutput[m, n])")
				# end
			end
		end
		return gradoutput
	end
	
	# reinforce_monte_carlo_control(π!, ∇lnπ!, length(θ), s0, αθ, step, sterm, actions; θ = θ, kwargs...)

	actor_critic_eligibility(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)

	# one_step_actor_critic(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)
end

# ╔═╡ 85fc29c9-e5ca-4bc8-b607-51d75906a1f2
# ╠═╡ show_logs = false
function eval_racetrack(track; nruns = nthreads(), αlist = 2. .^(-3:-1), λlist = [0.0, 0.1, 0.2, 0.4, 0.8, 1.0], kwargs...)
	traces = [begin
		out = average_runs((;kwargs...) -> execute_racetrack_actor_critic(track, α, α; kwargs...), nruns; λθ = λ, λw = λ, kwargs...) 
		scatter(x = eachindex(out), y = out, name = "α = $α, λ = $λ")
	end
	for α in αlist for λ in λlist]
	plot(traces)
end

# ╔═╡ ab29ec76-0b89-4eaa-81a0-1b31c901f97d
eval_racetrack(track1; max_episodes = 100, maxsteps = 5_000, termination_threshold = (episode = 30, reward = -1000))

# ╔═╡ 4b96e0b4-eca4-46ba-beba-40bcaefdb30a
plot(execute_racetrack_actor_critic(track1, 0.3, 0.3; λθ = 0.7, λw = 0.6, max_episodes = 500, maxsteps = 10000)[1])

# ╔═╡ b50282ed-e599-4687-bfbc-0ac9c4f30c84
function racetrack_optimize_λ(track, αθlist, αwlist; nruns = nthreads(), λlist = [0.0, 0.1, 0.2, 0.4, 0.8, .9], kwargs...)
	function maketrace(αθ, αw) 
		rewards = [begin
			out = average_runs((;kwargs...) -> execute_racetrack_actor_critic(track, αθ, αw; kwargs...), nruns; λθ = λ, λw = λ, kwargs...) 
			mean(out[end-100:end])
		end
		for λ in λlist]
		scatter(x = λlist, y = rewards, name = "αθ = $αθ, αw = $αw")
	end

	traces = [maketrace(a, b) for a in αθlist for b in αwlist]
	plot(traces, Layout(xaxis_title = "λ", yaxis_title = "Average Reward Last 100 Episodes"))
end

# ╔═╡ 801a2dbd-b663-4bfa-b763-092579a8599c
racetrack_optimize_λ(track1, [0.3, 0.5, 0.8], [0.3, 0.5]; max_episodes = 1000, maxsteps = 5000, termination_threshold = (episode = 100, reward = -500), λlist = [0.2, 0.4, 0.5, 0.6, 0.7, 0.8])

# ╔═╡ 80e40d2b-a67b-46eb-86fd-294c0a87a80f
md"""
## Blackjack Environment
"""

# ╔═╡ 8edb3337-0902-45fa-a5b0-c7cc3d40f97f
const cards = (2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, :A)

# ╔═╡ 37dc5518-d378-41fd-b0ef-bc5e3b1b3687
const blackjackactions = (:hit, :stick)

# ╔═╡ 0ac08421-20d2-4e56-bce8-1bc47b36fe2e
#deal a card from an infinite deck and return either the value of that card or an ace
deal() = rand(cards)

# ╔═╡ 064b06ae-903b-4430-b925-534925bca733
const blackjackstates = [(s, c, ua) for s in 12:21 for c in 1:10 for ua in (true, false)]

# ╔═╡ 9be279fa-9325-4eb1-8c73-7742c066664d
const blackjack_sterm = (100, 100, false)

# ╔═╡ 6353a374-9eba-4184-a528-f8ca9f32dfe5
const blackjack_s0 = (0, 0, false)

# ╔═╡ b265b8e6-994a-4be2-a7c9-05adef570fda
makestatelookup(states) = Dict(zip(states, eachindex(states)))

# ╔═╡ 460d9e76-9841-4fb8-8e35-0efbbf6f9f08
const blackjackstatelookup = makestatelookup([blackjackstates; blackjack_s0; blackjack_sterm])

# ╔═╡ 8ea91577-57eb-4afc-8919-95bd16ae6865
#takes a previous sum, usable ace indicator, and a card to be added to the sum.  Returns the updated sum and whether an ace is still usable
function addsum(s::Int64, ua::Bool, c::Symbol)
	if !ua
		s >= 11 ? (s+1, false) : (s+11, true)
	else
		(s+1, true)
	end
end

# ╔═╡ 6324046e-c766-444f-8a74-f6e3569154fa
function addsum(s::Int64, ua::Bool, c::Int64)
	if !ua
		(s + c, false)
	else
		if (s + c) > 21
			(s + c - 10, false)
		else
			(s + c, true)
		end
	end
end

# ╔═╡ 7f8ea283-8b42-4bb7-8d49-a54855a98c5d
function playerstep(s, ua, a)
	a == :stick && return (s, ua)
	addsum(s, ua, deal())
end

# ╔═╡ 8f133852-12da-41b3-8071-51a12211f432
function dealer_sim(s::Int64, ua::Bool)
	(s >= 17) && return s
	(s, ua) = addsum(s, ua, deal())
	dealer_sim(s, ua)
end

# ╔═╡ ebeabff8-4779-49e6-a04f-16a76e0b9b04
function blackjack_step(state, action)
	#score a game in which the player didn't go bust
	function scoregame(playersum, dealersum)
		#if the dealer goes bust, the player wins
		dealersum > 21 && return 1.0

		#if the player is closer to 21 the player wins
		playersum > dealersum && return 1.0

		#if the dealer sum is closer to 21 the player loses
		playersum < dealersum && return -1.0

		#otherwise the outcome is a draw
		return 0.0
	end
	
	(s, c, ua) = state

	#initial state
	if s == 0 
		initstate = true
		#deal two cards and check for player natural
		(s, ua) = addsum(s, ua, deal())
		(s, ua) = addsum(s, ua, deal())
		playernatural = (s == 21)

		#if sum is less than 12 then keep dealing since these are not states with any action choice
		while s < 12
			(s, ua) = addsum(s, ua, deal())
		end

		#generate dealer card
		c = rand(1:10)
	else
		initstate = false
		playernatural = false
	end
	
	#generate hidden dealer card and final state
	hc = deal()
	(ds, dua) = if c == 1
		addsum(11, true, hc)
	else 
		addsum(c, false, hc)
	end

	dealernatural = ds == 21

	sdealer = dealer_sim(ds, dua)

	#calculate score in case of player natural
	playernatural && return (blackjack_sterm, Float64(!dealernatural))

	#if there is no playernatural and we are in the initial state, then return the new initial state ignoring the action selection and giving no reward
	initstate && return ((s, c, ua), 0.0)

	#sticking always ends the game 
	action == :stick && return (blackjack_sterm, scoregame(s, sdealer))

	#deal player new card if hitting
	(s, ua) = addsum(s, ua, deal())

	#player always looses if busts
	s > 21 && return (blackjack_sterm, -1.0)

	#if player has 21 game also ends
	s == 21 && return (blackjack_sterm, scoregame(s, sdealer))

	#otherwise return new state and 0 reward
	return ((s, c, ua), 0.0)
end			

# ╔═╡ 097b8fc1-b4a4-4b93-bc08-2ceebd5d759a
function execute_blackjack_actor_critic(αθ, αw, statelookup; kwargs...)
	nstates = length(statelookup)
	
	#create state feature one hot vectors
	xs = [zeros(nstates) for i in 1:nstates]
	for i in values(statelookup)
		xs[i][i] = 1.0
	end
	
	#parameters
	θ = zeros(nstates, lastindex(blackjackactions))
	w = zeros(nstates)

	#allocations for outputs
	πoutput = zeros(lastindex(blackjackactions))
	gradoutput = similar(θ)

	#value function and gradient
	v̂(s, w) = w[statelookup[s]]
	∇v̂(s, w) = xs[statelookup[s]]

	function clean_output!(v::AbstractVector{T}) where T <: AbstractFloat
		for (i, x) in enumerate(v)
			if isnan(x) || isinf(x)
				v[i] = zero(T)
			end
		end
		return v
	end

	#policy function and gradient
	function π!(s, θ) 
		soft_max!(θ[statelookup[s], :], πoutput)
		clean_output!(πoutput)
	end

	function ∇lnπ!(a, s, θ)
		#ensure πoutput contains the current softmax output for this state
		# π!(s, θ)
		i = statelookup[s]
		 for n in eachindex(blackjackactions)
			@inbounds @simd for m in 1:nstates
				gradoutput[m, n] = (i == m) * ((n == a) - πoutput[n])
				# if i == m
				# 	println("At state $i Updated gradient of $(gradoutput[m, n])")
				# end
			end
		end
		return gradoutput
	end
	
	# reinforce_monte_carlo_control(π!, ∇lnπ!, length(θ), s0, αθ, step, sterm, actions; θ = θ, kwargs...)

	actor_critic_eligibility(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), blackjack_s0, αθ, αw, blackjack_step, blackjack_sterm, blackjackactions; θ = θ, w = w, kwargs...)

	# one_step_actor_critic(π!, ∇lnπ!, v̂, ∇v̂, length(θ), length(w), s0, αθ, αw, step, sterm, actions; θ = θ, w = w, kwargs...)
end

# ╔═╡ 519e6da0-efbf-4b0a-a61c-5849ba403389
function plotblackjackwinrate(αθ, αw, max_episodes; kwargs...)
	y = cumsum(execute_blackjack_actor_critic(αθ, αw, blackjackstatelookup; max_episodes = max_episodes, kwargs...)[1])[100:end]
	x = (100:max_episodes)

	l = length(y)

	i = ceil(Int64, l / 10_000)
	plot(y[1:i:l] ./ x[1:i:l])
end

# ╔═╡ 4c4ba58e-e3b7-4d02-81ae-b8d753487caa
plotblackjackwinrate(0.3, 0.3, 1_000_000; λθ = 0.5, λw = 0.5)

# ╔═╡ 8cb58177-cc29-4bf0-af2f-704bebb9871f
_, blackjackθ, _ = execute_blackjack_actor_critic(0.1, 0.1, blackjackstatelookup; max_episodes = 1_000_000, λθ = 0.5, λw = 0.5)

# ╔═╡ 0b6fb5bf-c21e-4727-aafb-65fc3f7b76fb
function plot_blackjack_policy(θ)
	πstargridua = zeros(10, 10)
	πstargridnua = zeros(10, 10)
	for state in blackjackstates
		(s, c, ua) = state
		n = blackjackstatelookup[state]
		a = blackjackactions[argmax(θ[n, :])]
		if ua
			(πstargridua[s-11, c] = soft_max(θ[n, :])[1])
		else
			(πstargridnua[s-11, c] = soft_max(θ[n, :])[1])
		end
	end

	# vstar = eval_blackjack_policy(Dict(s => π[s] == :hit ? [1.0, 0.0] : [0.0, 1.0] for s in blackjackstates), 500_000)
	p1 = plot(heatmap(z = πstargridua, x = ["A"; string.([2, 3, 4, 5, 6, 7, 8, 9, 10])], y = 12:21), Layout(legend = false, title = "Usable Ace Policy, Stick Probability"))
	p2 = plot(heatmap(z = πstargridnua, x = ["A"; string.([2, 3, 4, 5, 6, 7, 8, 9, 10])], y = 12:21), Layout(legend = false, title = "No usable Ace Policy", x_label = "Dealer Showing", y_label = "Player sum"))
	[p1, p2]
	# p3 = heatmap(vstar[1], legend = false, yticks = (1:10, 12:21), title = "V*")
	# p4 = heatmap(vstar[2], yticks = (1:10, 12:21))
	# plot(p1, p3, p2, p4, layout = (2,2))
end
	

# ╔═╡ 8a909bf5-55fe-4b0a-b3e6-e862678e62b4
plot_blackjack_policy(blackjackθ)

# ╔═╡ 0ab70fc3-6188-42eb-aba2-d808f319be9f
md"""
# Dependencies and Settings
"""

# ╔═╡ c75b36a3-41d6-4ad8-83d6-1cf83734e1fc
html"""
<style>
	main {
		margin: 0 auto;
		max-width: 2000px;
    	padding-left: max(160px, 10%);
    	padding-right: max(160px, 10%);
	}
</style>
"""

# ╔═╡ ea8cdebd-7a25-49ae-9695-48dda2a880b4
TableOfContents()

# ╔═╡ 00000000-0000-0000-0000-000000000001
PLUTO_PROJECT_TOML_CONTENTS = """
[deps]
Distributions = "31c24e10-a181-5473-b8eb-7969acd0382f"
LaTeXStrings = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
LinearAlgebra = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"
PlutoPlotly = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
PlutoUI = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
ProfileCanvas = "efd6af41-a80b-495e-886c-e51b0c7d77a3"
Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"
StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"

[compat]
Distributions = "~0.25.87"
LaTeXStrings = "~1.3.0"
PlutoPlotly = "~0.3.6"
PlutoUI = "~0.7.50"
ProfileCanvas = "~0.1.6"
StatsBase = "~0.33.21"
"""

# ╔═╡ 00000000-0000-0000-0000-000000000002
PLUTO_MANIFEST_TOML_CONTENTS = """
# This file is machine-generated - editing it directly is not advised

julia_version = "1.9.0-rc2"
manifest_format = "2.0"
project_hash = "98067cea35f86f7dfcfaf1734e9def984d12141d"

[[deps.AbstractPlutoDingetjes]]
deps = ["Pkg"]
git-tree-sha1 = "8eaf9f1b4921132a4cff3f36a1d9ba923b14a481"
uuid = "6e696c72-6542-2067-7265-42206c756150"
version = "1.1.4"

[[deps.ArgTools]]
uuid = "0dad84c5-d112-42e6-8d28-ef12dabb789f"
version = "1.1.1"

[[deps.Artifacts]]
uuid = "56f22d72-fd6d-98f1-02f0-08ddc0907c33"

[[deps.Base64]]
uuid = "2a0f44e3-6c83-55bd-87e4-b1978d98bd5f"

[[deps.Calculus]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "f641eb0a4f00c343bbc32346e1217b86f3ce9dad"
uuid = "49dc2e85-a5d0-5ad3-a950-438e2897f1b9"
version = "0.5.1"

[[deps.ColorSchemes]]
deps = ["ColorTypes", "ColorVectorSpace", "Colors", "FixedPointNumbers", "Random", "SnoopPrecompile"]
git-tree-sha1 = "aa3edc8f8dea6cbfa176ee12f7c2fc82f0608ed3"
uuid = "35d6a980-a343-548e-a6ea-1d62b119f2f4"
version = "3.20.0"

[[deps.ColorTypes]]
deps = ["FixedPointNumbers", "Random"]
git-tree-sha1 = "eb7f0f8307f71fac7c606984ea5fb2817275d6e4"
uuid = "3da002f7-5984-5a60-b8a6-cbb66c0b333f"
version = "0.11.4"

[[deps.ColorVectorSpace]]
deps = ["ColorTypes", "FixedPointNumbers", "LinearAlgebra", "SpecialFunctions", "Statistics", "TensorCore"]
git-tree-sha1 = "600cc5508d66b78aae350f7accdb58763ac18589"
uuid = "c3611d14-8923-5661-9e6a-0046d554d3a4"
version = "0.9.10"

[[deps.Colors]]
deps = ["ColorTypes", "FixedPointNumbers", "Reexport"]
git-tree-sha1 = "fc08e5930ee9a4e03f84bfb5211cb54e7769758a"
uuid = "5ae59095-9a9b-59fe-a467-6f913c188581"
version = "0.12.10"

[[deps.Compat]]
deps = ["UUIDs"]
git-tree-sha1 = "7a60c856b9fa189eb34f5f8a6f6b5529b7942957"
uuid = "34da2185-b29b-5c13-b0c7-acf172513d20"
version = "4.6.1"
weakdeps = ["Dates", "LinearAlgebra"]

    [deps.Compat.extensions]
    CompatLinearAlgebraExt = "LinearAlgebra"

[[deps.CompilerSupportLibraries_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "e66e0078-7015-5450-92f7-15fbd957f2ae"
version = "1.0.2+0"

[[deps.DataAPI]]
git-tree-sha1 = "e8119c1a33d267e16108be441a287a6981ba1630"
uuid = "9a962f9c-6df0-11e9-0e5d-c546b8b5ee8a"
version = "1.14.0"

[[deps.DataStructures]]
deps = ["Compat", "InteractiveUtils", "OrderedCollections"]
git-tree-sha1 = "d1fff3a548102f48987a52a2e0d114fa97d730f0"
uuid = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
version = "0.18.13"

[[deps.Dates]]
deps = ["Printf"]
uuid = "ade2ca70-3891-5945-98fb-dc099432e06a"

[[deps.DelimitedFiles]]
deps = ["Mmap"]
git-tree-sha1 = "9e2f36d3c96a820c678f2f1f1782582fcf685bae"
uuid = "8bb1440f-4735-579b-a4ab-409b98df4dab"
version = "1.9.1"

[[deps.Distributions]]
deps = ["FillArrays", "LinearAlgebra", "PDMats", "Printf", "QuadGK", "Random", "SparseArrays", "SpecialFunctions", "Statistics", "StatsBase", "StatsFuns", "Test"]
git-tree-sha1 = "13027f188d26206b9e7b863036f87d2f2e7d013a"
uuid = "31c24e10-a181-5473-b8eb-7969acd0382f"
version = "0.25.87"

    [deps.Distributions.extensions]
    DistributionsChainRulesCoreExt = "ChainRulesCore"
    DistributionsDensityInterfaceExt = "DensityInterface"

    [deps.Distributions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    DensityInterface = "b429d917-457f-4dbc-8f4c-0cc954292b1d"

[[deps.DocStringExtensions]]
deps = ["LibGit2"]
git-tree-sha1 = "2fb1e02f2b635d0845df5d7c167fec4dd739b00d"
uuid = "ffbed154-4ef7-542d-bbb7-c09d3a79fcae"
version = "0.9.3"

[[deps.Downloads]]
deps = ["ArgTools", "FileWatching", "LibCURL", "NetworkOptions"]
uuid = "f43a241f-c20a-4ad4-852c-f6b1247861c6"
version = "1.6.0"

[[deps.DualNumbers]]
deps = ["Calculus", "NaNMath", "SpecialFunctions"]
git-tree-sha1 = "5837a837389fccf076445fce071c8ddaea35a566"
uuid = "fa6b7ba4-c1ee-5f82-b5fc-ecf0adba8f74"
version = "0.6.8"

[[deps.FileWatching]]
uuid = "7b1f6079-737a-58dc-b8bc-7a2ca5c1b5ee"

[[deps.FillArrays]]
deps = ["LinearAlgebra", "Random", "SparseArrays", "Statistics"]
git-tree-sha1 = "fc86b4fd3eff76c3ce4f5e96e2fdfa6282722885"
uuid = "1a297f60-69ca-5386-bcde-b61e274b549b"
version = "1.0.0"

[[deps.FixedPointNumbers]]
deps = ["Statistics"]
git-tree-sha1 = "335bfdceacc84c5cdf16aadc768aa5ddfc5383cc"
uuid = "53c48c17-4a7d-5ca2-90c5-79b7896eea93"
version = "0.8.4"

[[deps.HypergeometricFunctions]]
deps = ["DualNumbers", "LinearAlgebra", "OpenLibm_jll", "SpecialFunctions"]
git-tree-sha1 = "432b5b03176f8182bd6841fbfc42c718506a2d5f"
uuid = "34004b35-14d8-5ef3-9330-4cdb6864b03a"
version = "0.3.15"

[[deps.Hyperscript]]
deps = ["Test"]
git-tree-sha1 = "8d511d5b81240fc8e6802386302675bdf47737b9"
uuid = "47d2ed2b-36de-50cf-bf87-49c2cf4b8b91"
version = "0.0.4"

[[deps.HypertextLiteral]]
deps = ["Tricks"]
git-tree-sha1 = "c47c5fa4c5308f27ccaac35504858d8914e102f9"
uuid = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
version = "0.9.4"

[[deps.IOCapture]]
deps = ["Logging", "Random"]
git-tree-sha1 = "f7be53659ab06ddc986428d3a9dcc95f6fa6705a"
uuid = "b5f81e59-6552-4d32-b1f0-c071b021bf89"
version = "0.2.2"

[[deps.InteractiveUtils]]
deps = ["Markdown"]
uuid = "b77e0a4c-d291-57a0-90e8-8db25a27a240"

[[deps.IrrationalConstants]]
git-tree-sha1 = "630b497eafcc20001bba38a4651b327dcfc491d2"
uuid = "92d709cd-6900-40b7-9082-c6be49f344b6"
version = "0.2.2"

[[deps.JLLWrappers]]
deps = ["Preferences"]
git-tree-sha1 = "abc9885a7ca2052a736a600f7fa66209f96506e1"
uuid = "692b3bcd-3c85-4b1f-b108-f13ce0eb3210"
version = "1.4.1"

[[deps.JSON]]
deps = ["Dates", "Mmap", "Parsers", "Unicode"]
git-tree-sha1 = "3c837543ddb02250ef42f4738347454f95079d4e"
uuid = "682c06a0-de6a-54ab-a142-c8b1cf79cde6"
version = "0.21.3"

[[deps.LaTeXStrings]]
git-tree-sha1 = "f2355693d6778a178ade15952b7ac47a4ff97996"
uuid = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
version = "1.3.0"

[[deps.LibCURL]]
deps = ["LibCURL_jll", "MozillaCACerts_jll"]
uuid = "b27032c2-a3e7-50c8-80cd-2d36dbcbfd21"
version = "0.6.3"

[[deps.LibCURL_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll", "Zlib_jll", "nghttp2_jll"]
uuid = "deac9b47-8bc7-5906-a0fe-35ac56dc84c0"
version = "7.84.0+0"

[[deps.LibGit2]]
deps = ["Base64", "NetworkOptions", "Printf", "SHA"]
uuid = "76f85450-5226-5b5a-8eaa-529ad045b433"

[[deps.LibSSH2_jll]]
deps = ["Artifacts", "Libdl", "MbedTLS_jll"]
uuid = "29816b5a-b9ab-546f-933c-edad1886dfa8"
version = "1.10.2+0"

[[deps.Libdl]]
uuid = "8f399da3-3557-5675-b5ff-fb832c97cbdb"

[[deps.LinearAlgebra]]
deps = ["Libdl", "OpenBLAS_jll", "libblastrampoline_jll"]
uuid = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"

[[deps.LogExpFunctions]]
deps = ["DocStringExtensions", "IrrationalConstants", "LinearAlgebra"]
git-tree-sha1 = "0a1b7c2863e44523180fdb3146534e265a91870b"
uuid = "2ab3a3ac-af41-5b50-aa03-7779005ae688"
version = "0.3.23"

    [deps.LogExpFunctions.extensions]
    LogExpFunctionsChainRulesCoreExt = "ChainRulesCore"
    LogExpFunctionsChangesOfVariablesExt = "ChangesOfVariables"
    LogExpFunctionsInverseFunctionsExt = "InverseFunctions"

    [deps.LogExpFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    ChangesOfVariables = "9e997f8a-9a97-42d5-a9f1-ce6bfc15e2c0"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.Logging]]
uuid = "56ddb016-857b-54e1-b83d-db4d58db5568"

[[deps.MIMEs]]
git-tree-sha1 = "65f28ad4b594aebe22157d6fac869786a255b7eb"
uuid = "6c6e2e6c-3030-632d-7369-2d6c69616d65"
version = "0.1.4"

[[deps.Markdown]]
deps = ["Base64"]
uuid = "d6f4376e-aef5-505a-96c1-9c027394607a"

[[deps.MbedTLS_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "c8ffd9c3-330d-5841-b78e-0817d7145fa1"
version = "2.28.2+0"

[[deps.Missings]]
deps = ["DataAPI"]
git-tree-sha1 = "f66bdc5de519e8f8ae43bdc598782d35a25b1272"
uuid = "e1d29d7a-bbdc-5cf2-9ac0-f12de2c33e28"
version = "1.1.0"

[[deps.Mmap]]
uuid = "a63ad114-7e13-5084-954f-fe012c677804"

[[deps.MozillaCACerts_jll]]
uuid = "14a3606d-f60d-562e-9121-12d972cd8159"
version = "2022.10.11"

[[deps.NaNMath]]
deps = ["OpenLibm_jll"]
git-tree-sha1 = "0877504529a3e5c3343c6f8b4c0381e57e4387e4"
uuid = "77ba4419-2d1f-58cd-9bb1-8ffee604a2e3"
version = "1.0.2"

[[deps.NetworkOptions]]
uuid = "ca575930-c2e3-43a9-ace4-1e988b2c1908"
version = "1.2.0"

[[deps.OpenBLAS_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "Libdl"]
uuid = "4536629a-c528-5b80-bd46-f80d51c5b363"
version = "0.3.21+4"

[[deps.OpenLibm_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "05823500-19ac-5b8b-9628-191a04bc5112"
version = "0.8.1+0"

[[deps.OpenSpecFun_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "JLLWrappers", "Libdl", "Pkg"]
git-tree-sha1 = "13652491f6856acfd2db29360e1bbcd4565d04f1"
uuid = "efe28fd5-8261-553b-a9e1-b2916fc3738e"
version = "0.5.5+0"

[[deps.OrderedCollections]]
git-tree-sha1 = "d321bf2de576bf25ec4d3e4360faca399afca282"
uuid = "bac558e1-5e72-5ebc-8fee-abe8a469f55d"
version = "1.6.0"

[[deps.PDMats]]
deps = ["LinearAlgebra", "SparseArrays", "SuiteSparse"]
git-tree-sha1 = "67eae2738d63117a196f497d7db789821bce61d1"
uuid = "90014a1f-27ba-587c-ab20-58faa44d9150"
version = "0.11.17"

[[deps.Parameters]]
deps = ["OrderedCollections", "UnPack"]
git-tree-sha1 = "34c0e9ad262e5f7fc75b10a9952ca7692cfc5fbe"
uuid = "d96e819e-fc66-5662-9728-84c9c7592b0a"
version = "0.12.3"

[[deps.Parsers]]
deps = ["Dates", "SnoopPrecompile"]
git-tree-sha1 = "478ac6c952fddd4399e71d4779797c538d0ff2bf"
uuid = "69de0a69-1ddd-5017-9359-2bf0b02dc9f0"
version = "2.5.8"

[[deps.Pkg]]
deps = ["Artifacts", "Dates", "Downloads", "FileWatching", "LibGit2", "Libdl", "Logging", "Markdown", "Printf", "REPL", "Random", "SHA", "Serialization", "TOML", "Tar", "UUIDs", "p7zip_jll"]
uuid = "44cfe95a-1eb2-52ea-b672-e2afdf69b78f"
version = "1.9.0"

[[deps.PlotlyBase]]
deps = ["ColorSchemes", "Dates", "DelimitedFiles", "DocStringExtensions", "JSON", "LaTeXStrings", "Logging", "Parameters", "Pkg", "REPL", "Requires", "Statistics", "UUIDs"]
git-tree-sha1 = "56baf69781fc5e61607c3e46227ab17f7040ffa2"
uuid = "a03496cd-edff-5a9b-9e67-9cda94a718b5"
version = "0.8.19"

[[deps.PlutoPlotly]]
deps = ["AbstractPlutoDingetjes", "Colors", "Dates", "HypertextLiteral", "InteractiveUtils", "LaTeXStrings", "Markdown", "PlotlyBase", "PlutoUI", "Reexport"]
git-tree-sha1 = "dec81dcd52748ffc59ce3582e709414ff78d947f"
uuid = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
version = "0.3.6"

[[deps.PlutoUI]]
deps = ["AbstractPlutoDingetjes", "Base64", "ColorTypes", "Dates", "FixedPointNumbers", "Hyperscript", "HypertextLiteral", "IOCapture", "InteractiveUtils", "JSON", "Logging", "MIMEs", "Markdown", "Random", "Reexport", "URIs", "UUIDs"]
git-tree-sha1 = "5bb5129fdd62a2bbbe17c2756932259acf467386"
uuid = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
version = "0.7.50"

[[deps.Preferences]]
deps = ["TOML"]
git-tree-sha1 = "47e5f437cc0e7ef2ce8406ce1e7e24d44915f88d"
uuid = "21216c6a-2e73-6563-6e65-726566657250"
version = "1.3.0"

[[deps.Printf]]
deps = ["Unicode"]
uuid = "de0858da-6303-5e67-8744-51eddeeeb8d7"

[[deps.Profile]]
deps = ["Printf"]
uuid = "9abbd945-dff8-562f-b5e8-e1ebf5ef1b79"

[[deps.ProfileCanvas]]
deps = ["Base64", "JSON", "Pkg", "Profile", "REPL"]
git-tree-sha1 = "e42571ce9a614c2fbebcaa8aab23bbf8865c624e"
uuid = "efd6af41-a80b-495e-886c-e51b0c7d77a3"
version = "0.1.6"

[[deps.QuadGK]]
deps = ["DataStructures", "LinearAlgebra"]
git-tree-sha1 = "6ec7ac8412e83d57e313393220879ede1740f9ee"
uuid = "1fd47b50-473d-5c70-9696-f719f8f3bcdc"
version = "2.8.2"

[[deps.REPL]]
deps = ["InteractiveUtils", "Markdown", "Sockets", "Unicode"]
uuid = "3fa0cd96-eef1-5676-8a61-b3b8758bbffb"

[[deps.Random]]
deps = ["SHA", "Serialization"]
uuid = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"

[[deps.Reexport]]
git-tree-sha1 = "45e428421666073eab6f2da5c9d310d99bb12f9b"
uuid = "189a3867-3050-52da-a836-e630ba90ab69"
version = "1.2.2"

[[deps.Requires]]
deps = ["UUIDs"]
git-tree-sha1 = "838a3a4188e2ded87a4f9f184b4b0d78a1e91cb7"
uuid = "ae029012-a4dd-5104-9daa-d747884805df"
version = "1.3.0"

[[deps.Rmath]]
deps = ["Random", "Rmath_jll"]
git-tree-sha1 = "f65dcb5fa46aee0cf9ed6274ccbd597adc49aa7b"
uuid = "79098fc4-a85e-5d69-aa6a-4863f24498fa"
version = "0.7.1"

[[deps.Rmath_jll]]
deps = ["Artifacts", "JLLWrappers", "Libdl", "Pkg"]
git-tree-sha1 = "6ed52fdd3382cf21947b15e8870ac0ddbff736da"
uuid = "f50d1b31-88e8-58de-be2c-1cc44531875f"
version = "0.4.0+0"

[[deps.SHA]]
uuid = "ea8e919c-243c-51af-8825-aaa63cd721ce"
version = "0.7.0"

[[deps.Serialization]]
uuid = "9e88b42a-f829-5b0c-bbe9-9e923198166b"

[[deps.SnoopPrecompile]]
deps = ["Preferences"]
git-tree-sha1 = "e760a70afdcd461cf01a575947738d359234665c"
uuid = "66db9d55-30c0-4569-8b51-7e840670fc0c"
version = "1.0.3"

[[deps.Sockets]]
uuid = "6462fe0b-24de-5631-8697-dd941f90decc"

[[deps.SortingAlgorithms]]
deps = ["DataStructures"]
git-tree-sha1 = "a4ada03f999bd01b3a25dcaa30b2d929fe537e00"
uuid = "a2af1166-a08f-5f64-846c-94a0d3cef48c"
version = "1.1.0"

[[deps.SparseArrays]]
deps = ["Libdl", "LinearAlgebra", "Random", "Serialization", "SuiteSparse_jll"]
uuid = "2f01184e-e22b-5df5-ae63-d93ebab69eaf"

[[deps.SpecialFunctions]]
deps = ["IrrationalConstants", "LogExpFunctions", "OpenLibm_jll", "OpenSpecFun_jll"]
git-tree-sha1 = "ef28127915f4229c971eb43f3fc075dd3fe91880"
uuid = "276daf66-3868-5448-9aa4-cd146d93841b"
version = "2.2.0"

    [deps.SpecialFunctions.extensions]
    SpecialFunctionsChainRulesCoreExt = "ChainRulesCore"

    [deps.SpecialFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"

[[deps.Statistics]]
deps = ["LinearAlgebra", "SparseArrays"]
uuid = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
version = "1.9.0"

[[deps.StatsAPI]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "45a7769a04a3cf80da1c1c7c60caf932e6f4c9f7"
uuid = "82ae8749-77ed-4fe6-ae5f-f523153014b0"
version = "1.6.0"

[[deps.StatsBase]]
deps = ["DataAPI", "DataStructures", "LinearAlgebra", "LogExpFunctions", "Missings", "Printf", "Random", "SortingAlgorithms", "SparseArrays", "Statistics", "StatsAPI"]
git-tree-sha1 = "d1bf48bfcc554a3761a133fe3a9bb01488e06916"
uuid = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
version = "0.33.21"

[[deps.StatsFuns]]
deps = ["HypergeometricFunctions", "IrrationalConstants", "LogExpFunctions", "Reexport", "Rmath", "SpecialFunctions"]
git-tree-sha1 = "f625d686d5a88bcd2b15cd81f18f98186fdc0c9a"
uuid = "4c63d2b9-4356-54db-8cca-17b64c39e42c"
version = "1.3.0"

    [deps.StatsFuns.extensions]
    StatsFunsChainRulesCoreExt = "ChainRulesCore"
    StatsFunsInverseFunctionsExt = "InverseFunctions"

    [deps.StatsFuns.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.SuiteSparse]]
deps = ["Libdl", "LinearAlgebra", "Serialization", "SparseArrays"]
uuid = "4607b0f0-06f3-5cda-b6b1-a6196a1729e9"

[[deps.SuiteSparse_jll]]
deps = ["Artifacts", "Libdl", "Pkg", "libblastrampoline_jll"]
uuid = "bea87d4a-7f5b-5778-9afe-8cc45184846c"
version = "5.10.1+6"

[[deps.TOML]]
deps = ["Dates"]
uuid = "fa267f1f-6049-4f14-aa54-33bafae1ed76"
version = "1.0.3"

[[deps.Tar]]
deps = ["ArgTools", "SHA"]
uuid = "a4e569a6-e804-4fa4-b0f3-eef7a1d5b13e"
version = "1.10.0"

[[deps.TensorCore]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1feb45f88d133a655e001435632f019a9a1bcdb6"
uuid = "62fd8b95-f654-4bbd-a8a5-9c27f68ccd50"
version = "0.1.1"

[[deps.Test]]
deps = ["InteractiveUtils", "Logging", "Random", "Serialization"]
uuid = "8dfed614-e22c-5e08-85e1-65c5234f0b40"

[[deps.Tricks]]
git-tree-sha1 = "aadb748be58b492045b4f56166b5188aa63ce549"
uuid = "410a4b4d-49e4-4fbc-ab6d-cb71b17b3775"
version = "0.1.7"

[[deps.URIs]]
git-tree-sha1 = "074f993b0ca030848b897beff716d93aca60f06a"
uuid = "5c2747f8-b7ea-4ff2-ba2e-563bfd36b1d4"
version = "1.4.2"

[[deps.UUIDs]]
deps = ["Random", "SHA"]
uuid = "cf7118a7-6976-5b1a-9a39-7adc72f591a4"

[[deps.UnPack]]
git-tree-sha1 = "387c1f73762231e86e0c9c5443ce3b4a0a9a0c2b"
uuid = "3a884ed6-31ef-47d7-9d2a-63182c4928ed"
version = "1.0.2"

[[deps.Unicode]]
uuid = "4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5"

[[deps.Zlib_jll]]
deps = ["Libdl"]
uuid = "83775a58-1f1d-513f-b197-d71354ab007a"
version = "1.2.13+0"

[[deps.libblastrampoline_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850b90-86db-534c-a0d3-1478176c7d93"
version = "5.4.0+0"

[[deps.nghttp2_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850ede-7688-5339-a07c-302acd2aaf8d"
version = "1.48.0+0"

[[deps.p7zip_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "3f19e933-33d8-53b3-aaab-bd5110c3b7a0"
version = "17.4.0+0"
"""

# ╔═╡ Cell order:
# ╟─36a6e43f-6bcf-4c27-bfbb-047760e77ada
# ╟─2501cbc0-9772-4b2f-ab01-ef7903e62950
# ╟─7a6fb1f0-fc3c-4c29-a6d9-769d32ca98a9
# ╠═1b89a5be-d4f6-43b6-b778-0895d77d0962
# ╠═759afa53-2b01-4d9b-b398-80120626634f
# ╠═97046258-7753-4edb-b0c9-0981d587ad35
# ╠═423321cc-1c8c-44a0-bd8e-a4d3cb68962b
# ╠═980af3e7-2f1c-49be-8f6b-fc61271dff52
# ╠═edb145d7-95e0-44c9-a60f-57d517edb0c7
# ╟─9d815d9c-6e5a-473e-a395-6f92d504dbf3
# ╟─e5faaa1b-88cb-43e2-8d04-8972b58b4bda
# ╟─406638af-1e08-44d2-9ee4-97aa9294a94b
# ╟─aa450da4-fe84-4eea-b6c4-9820b7982437
# ╟─f924eb30-d1cc-4941-8fb5-ff70ad425ab9
# ╠═b406577a-5478-42fd-8ed0-e36b5574cfc6
# ╠═2b11ef08-288f-4110-b741-ba580782b6a7
# ╟─71973c41-5fbb-40bf-8cc9-e063c7372a1c
# ╟─49a1d508-b491-4d3a-8415-f5def06884e9
# ╟─c6b61679-8a06-47ae-abab-6997ad5cbfea
# ╠═c2d8a622-b8f9-454b-9fd1-dc940280624c
# ╠═5f91ce14-c9d4-4818-8955-8e7381b4943b
# ╠═a45c1930-ad70-44f4-a6bc-10ccb03f65ab
# ╠═71c8d422-8177-4324-b048-98dd39198fee
# ╟─a206c759-3f6e-4003-8cba-5f6ce6742646
# ╟─2e6d0374-1c93-48c8-b8ba-dd1a0c682d01
# ╟─cc45091e-b889-4d5a-9eef-84d80f792046
# ╠═5bebef34-e266-4c18-95c3-28e1f1cb4b64
# ╠═d26cd4cb-9a62-4a03-8b71-b415c9be79f6
# ╠═5f042c7e-45ad-4f8f-94d8-9133e67dd0f6
# ╟─b72e030f-7d52-481f-b4f7-2b16b227e547
# ╟─87876de4-ca40-4736-81a8-bb26bc273d89
# ╟─ce33f710-fd9d-4dfa-acda-40204e54d518
# ╟─f4b6f10b-4cd0-4be6-98ec-4d4ffb696392
# ╠═bf656d55-19cb-4052-baaa-0896ab6d23a0
# ╟─4cbdb082-22ba-49e9-a6ed-4380917625ac
# ╠═58ad84b0-f9c9-424e-8c05-0b15fbe7b349
# ╠═8f11b8dc-2c3e-41a5-8dbb-9af06235fe85
# ╟─70d4e199-2941-46dd-99c0-0f0520bf976b
# ╠═72900e88-98f4-4879-b005-d79ef6c7ee7f
# ╟─225ab967-7c7a-44ea-925b-5fa786382d62
# ╟─511a847f-234c-465e-8f4a-688e79d9b975
# ╠═533cbf4b-ac14-47eb-98cf-e569f32cc215
# ╟─735b548a-88f5-4a30-ab8f-dfb3d6401b2b
# ╠═79c85707-ea09-4f6b-ad51-a2683c3923c0
# ╟─7ccadf01-fbba-4dfd-a5ad-770dab9946f9
# ╟─beb01fb8-c77d-4b5c-a66d-3812415e04a3
# ╟─68e6f17e-8c87-40f0-a673-1115ecd1b71d
# ╟─692c1043-4eaf-491e-b8fe-368618867f99
# ╟─4c34640f-efa2-4e1d-8a70-0acd2ce45428
# ╠═ff60f48e-2055-4bb6-8cf4-fac1da45200b
# ╠═a79ed238-a6d3-40e6-9bf3-351b7494b446
# ╠═76af787d-7a3d-4c65-ab6a-898fba148705
# ╠═0538aaf4-716b-4f3c-aa7e-dcb1dd456172
# ╠═d37e21ac-b82a-423f-8719-b513bddf433d
# ╠═e5a0a3fc-2eb3-4f31-8ab6-4a3130c70932
# ╠═85fc29c9-e5ca-4bc8-b607-51d75906a1f2
# ╠═ab29ec76-0b89-4eaa-81a0-1b31c901f97d
# ╠═4b96e0b4-eca4-46ba-beba-40bcaefdb30a
# ╠═b50282ed-e599-4687-bfbc-0ac9c4f30c84
# ╠═801a2dbd-b663-4bfa-b763-092579a8599c
# ╟─80e40d2b-a67b-46eb-86fd-294c0a87a80f
# ╠═8edb3337-0902-45fa-a5b0-c7cc3d40f97f
# ╠═37dc5518-d378-41fd-b0ef-bc5e3b1b3687
# ╠═0ac08421-20d2-4e56-bce8-1bc47b36fe2e
# ╠═064b06ae-903b-4430-b925-534925bca733
# ╠═9be279fa-9325-4eb1-8c73-7742c066664d
# ╠═6353a374-9eba-4184-a528-f8ca9f32dfe5
# ╠═b265b8e6-994a-4be2-a7c9-05adef570fda
# ╠═460d9e76-9841-4fb8-8e35-0efbbf6f9f08
# ╠═8ea91577-57eb-4afc-8919-95bd16ae6865
# ╠═6324046e-c766-444f-8a74-f6e3569154fa
# ╠═7f8ea283-8b42-4bb7-8d49-a54855a98c5d
# ╠═8f133852-12da-41b3-8071-51a12211f432
# ╠═ebeabff8-4779-49e6-a04f-16a76e0b9b04
# ╠═097b8fc1-b4a4-4b93-bc08-2ceebd5d759a
# ╠═519e6da0-efbf-4b0a-a61c-5849ba403389
# ╠═4c4ba58e-e3b7-4d02-81ae-b8d753487caa
# ╠═8cb58177-cc29-4bf0-af2f-704bebb9871f
# ╠═8a909bf5-55fe-4b0a-b3e6-e862678e62b4
# ╠═0b6fb5bf-c21e-4727-aafb-65fc3f7b76fb
# ╟─0ab70fc3-6188-42eb-aba2-d808f319be9f
# ╠═d04d4234-d97f-11ed-2ea3-85ee0fc3bd70
# ╠═c75b36a3-41d6-4ad8-83d6-1cf83734e1fc
# ╠═ea8cdebd-7a25-49ae-9695-48dda2a880b4
# ╟─00000000-0000-0000-0000-000000000001
# ╟─00000000-0000-0000-0000-000000000002
